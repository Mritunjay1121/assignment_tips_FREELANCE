{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59eaaf35-6400-49a8-bc73-79589b887a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from statistics import mode\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d9fd71-f54e-45f9-8729-5357a74de171",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445820</td>\n",
       "      <td>1.00770</td>\n",
       "      <td>0.16655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085480</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.225900</td>\n",
       "      <td>1.65000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>0.037986</td>\n",
       "      <td>0.029998</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.26019</td>\n",
       "      <td>1.234300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030867</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.058814</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.021649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.070290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.031747</td>\n",
       "      <td>0.036008</td>\n",
       "      <td>0.004851</td>\n",
       "      <td>0.033030</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.050037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011666</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.018619</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.032792</td>\n",
       "      <td>0.030836</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.022252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13067</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.207740</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134970</td>\n",
       "      <td>1.82980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>0.028282</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.030918</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.032241</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.034105</td>\n",
       "      <td>0.030931</td>\n",
       "      <td>0.028016</td>\n",
       "      <td>0.037107</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.050524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25423</td>\n",
       "      <td>0.536650</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>0.15445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058705</td>\n",
       "      <td>0.059477</td>\n",
       "      <td>0.040081</td>\n",
       "      <td>0.039660</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.039089</td>\n",
       "      <td>0.024199</td>\n",
       "      <td>0.039740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.380670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039576</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.037379</td>\n",
       "      <td>0.022508</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.023862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.033937</td>\n",
       "      <td>0.015698</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>0.046850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.324620</td>\n",
       "      <td>0.047755</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.22718</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.104700</td>\n",
       "      <td>0.24705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031290</td>\n",
       "      <td>0.033076</td>\n",
       "      <td>0.015958</td>\n",
       "      <td>0.037091</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>0.032208</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>0.020791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CNNs    CNNs.1   CNNs.2   CNNs.3    CNNs.4    CNNs.5   CNNs.6  \\\n",
       "0    0.000000  0.445820  1.00770  0.16655  0.000000  0.085480  0.00000   \n",
       "1    0.000000  0.000000  0.00000  0.26019  1.234300  0.000000  0.00000   \n",
       "2    0.943630  0.000000  0.00000  0.14180  0.070290  0.000000  0.00000   \n",
       "3    0.019543  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000   \n",
       "4    0.000000  0.000000  0.13067  0.00000  0.006485  0.207740  0.00000   \n",
       "..        ...       ...      ...      ...       ...       ...      ...   \n",
       "395  0.000000  0.139840  0.00000  0.00000  0.000000  0.000000  0.00000   \n",
       "396  0.000000  0.000000  0.00000  0.25423  0.536650  0.052813  0.00000   \n",
       "397  0.000000  0.000000  0.00000  0.00000  0.380670  0.000000  0.00000   \n",
       "398  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000   \n",
       "399  0.324620  0.047755  0.00000  0.00000  0.000000  0.000000  0.22718   \n",
       "\n",
       "     CNNs.7    CNNs.8   CNNs.9  ...  GIST.376  GIST.377  GIST.378  GIST.379  \\\n",
       "0     0.000  1.225900  1.65000  ...  0.032443  0.037986  0.029998  0.008660   \n",
       "1     0.000  0.000000  0.25666  ...  0.030867  0.050679  0.021178  0.016048   \n",
       "2     0.000  0.390320  0.00000  ...  0.010313  0.031747  0.036008  0.004851   \n",
       "3     0.000  0.000000  0.00000  ...  0.011666  0.015680  0.018619  0.005827   \n",
       "4     0.000  0.134970  1.82980  ...  0.027669  0.028282  0.004130  0.030918   \n",
       "..      ...       ...      ...  ...       ...       ...       ...       ...   \n",
       "395   0.000  0.000000  0.00000  ...  0.011641  0.034105  0.030931  0.028016   \n",
       "396   0.000  0.016321  0.15445  ...  0.058705  0.059477  0.040081  0.039660   \n",
       "397   0.000  0.000000  0.99250  ...  0.039576  0.020201  0.019792  0.031384   \n",
       "398   1.075  0.000000  0.00000  ...  0.010942  0.023830  0.033937  0.015698   \n",
       "399   0.000  1.104700  0.24705  ...  0.031290  0.033076  0.015958  0.037091   \n",
       "\n",
       "     GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0    0.034269  0.010365  0.011738  0.030153    1.0        0.66  \n",
       "1    0.047683  0.058814  0.014283  0.021649    0.0        1.00  \n",
       "2    0.033030  0.020830  0.034741  0.050037    0.0        1.00  \n",
       "3    0.032792  0.030836  0.023793  0.022252    0.0        0.66  \n",
       "4    0.035200  0.032241  0.009805  0.006492    0.0        0.66  \n",
       "..        ...       ...       ...       ...    ...         ...  \n",
       "395  0.037107  0.015217  0.005206  0.050524    1.0        0.66  \n",
       "396  0.037121  0.039089  0.024199  0.039740    0.0        0.66  \n",
       "397  0.037379  0.022508  0.010190  0.023862    0.0        0.66  \n",
       "398  0.051196  0.004867  0.005816  0.046850    1.0        0.66  \n",
       "399  0.024476  0.032208  0.022142  0.020791    0.0        0.66  \n",
       "\n",
       "[400 rows x 3458 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"training1.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95791b44-eb5d-4e71-89bf-768fdd6f5be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CNNs', 'CNNs.1', 'CNNs.2', 'CNNs.3', 'CNNs.4', 'CNNs.5', 'CNNs.6',\n",
       "       'CNNs.7', 'CNNs.8', 'CNNs.9',\n",
       "       ...\n",
       "       'GIST.376', 'GIST.377', 'GIST.378', 'GIST.379', 'GIST.380', 'GIST.381',\n",
       "       'GIST.382', 'GIST.383', 'label', 'confidence'],\n",
       "      dtype='object', length=3458)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe66078e-b2e8-4659-b07d-ec51752d6e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0.0    213\n",
       "1.0    187\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61609fce-d6e5-40b2-a9b2-d256e9618417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label  confidence\n",
       "0      1.0        0.66\n",
       "1      0.0        1.00\n",
       "2      0.0        1.00\n",
       "3      0.0        0.66\n",
       "4      0.0        0.66\n",
       "..     ...         ...\n",
       "395    1.0        0.66\n",
       "396    0.0        0.66\n",
       "397    0.0        0.66\n",
       "398    1.0        0.66\n",
       "399    0.0        0.66\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[:,-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9c4345-7ef6-4c68-8942-7f49a1170b29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df1.isnull().sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ef1c4d-eb99-4a92-9ac2-5994b0f06c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.374</th>\n",
       "      <th>GIST.375</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445820</td>\n",
       "      <td>1.00770</td>\n",
       "      <td>0.16655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085480</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.225900</td>\n",
       "      <td>1.65000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>0.041512</td>\n",
       "      <td>0.032443</td>\n",
       "      <td>0.037986</td>\n",
       "      <td>0.029998</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.034269</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.030153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.26019</td>\n",
       "      <td>1.234300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015814</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>0.030867</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.047683</td>\n",
       "      <td>0.058814</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.021649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14180</td>\n",
       "      <td>0.070290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.390320</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031940</td>\n",
       "      <td>0.022160</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.031747</td>\n",
       "      <td>0.036008</td>\n",
       "      <td>0.004851</td>\n",
       "      <td>0.033030</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.050037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015834</td>\n",
       "      <td>0.030633</td>\n",
       "      <td>0.011666</td>\n",
       "      <td>0.015680</td>\n",
       "      <td>0.018619</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.032792</td>\n",
       "      <td>0.030836</td>\n",
       "      <td>0.023793</td>\n",
       "      <td>0.022252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13067</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.207740</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.134970</td>\n",
       "      <td>1.82980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037997</td>\n",
       "      <td>0.007409</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>0.028282</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.030918</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.032241</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.006492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028802</td>\n",
       "      <td>0.027278</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.034105</td>\n",
       "      <td>0.030931</td>\n",
       "      <td>0.028016</td>\n",
       "      <td>0.037107</td>\n",
       "      <td>0.015217</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.050524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.25423</td>\n",
       "      <td>0.536650</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016321</td>\n",
       "      <td>0.15445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029782</td>\n",
       "      <td>0.034993</td>\n",
       "      <td>0.058705</td>\n",
       "      <td>0.059477</td>\n",
       "      <td>0.040081</td>\n",
       "      <td>0.039660</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.039089</td>\n",
       "      <td>0.024199</td>\n",
       "      <td>0.039740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.380670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.99250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026287</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>0.039576</td>\n",
       "      <td>0.020201</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.031384</td>\n",
       "      <td>0.037379</td>\n",
       "      <td>0.022508</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.023862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026681</td>\n",
       "      <td>0.023454</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.033937</td>\n",
       "      <td>0.015698</td>\n",
       "      <td>0.051196</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>0.005816</td>\n",
       "      <td>0.046850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.324620</td>\n",
       "      <td>0.047755</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.22718</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.104700</td>\n",
       "      <td>0.24705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037796</td>\n",
       "      <td>0.037610</td>\n",
       "      <td>0.031290</td>\n",
       "      <td>0.033076</td>\n",
       "      <td>0.015958</td>\n",
       "      <td>0.037091</td>\n",
       "      <td>0.024476</td>\n",
       "      <td>0.032208</td>\n",
       "      <td>0.022142</td>\n",
       "      <td>0.020791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CNNs    CNNs.1   CNNs.2   CNNs.3    CNNs.4    CNNs.5   CNNs.6  \\\n",
       "0    0.000000  0.445820  1.00770  0.16655  0.000000  0.085480  0.00000   \n",
       "1    0.000000  0.000000  0.00000  0.26019  1.234300  0.000000  0.00000   \n",
       "2    0.943630  0.000000  0.00000  0.14180  0.070290  0.000000  0.00000   \n",
       "3    0.019543  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000   \n",
       "4    0.000000  0.000000  0.13067  0.00000  0.006485  0.207740  0.00000   \n",
       "..        ...       ...      ...      ...       ...       ...      ...   \n",
       "395  0.000000  0.139840  0.00000  0.00000  0.000000  0.000000  0.00000   \n",
       "396  0.000000  0.000000  0.00000  0.25423  0.536650  0.052813  0.00000   \n",
       "397  0.000000  0.000000  0.00000  0.00000  0.380670  0.000000  0.00000   \n",
       "398  0.000000  0.000000  0.00000  0.00000  0.000000  0.000000  0.00000   \n",
       "399  0.324620  0.047755  0.00000  0.00000  0.000000  0.000000  0.22718   \n",
       "\n",
       "     CNNs.7    CNNs.8   CNNs.9  ...  GIST.374  GIST.375  GIST.376  GIST.377  \\\n",
       "0     0.000  1.225900  1.65000  ...  0.030754  0.041512  0.032443  0.037986   \n",
       "1     0.000  0.000000  0.25666  ...  0.015814  0.019815  0.030867  0.050679   \n",
       "2     0.000  0.390320  0.00000  ...  0.031940  0.022160  0.010313  0.031747   \n",
       "3     0.000  0.000000  0.00000  ...  0.015834  0.030633  0.011666  0.015680   \n",
       "4     0.000  0.134970  1.82980  ...  0.037997  0.007409  0.027669  0.028282   \n",
       "..      ...       ...      ...  ...       ...       ...       ...       ...   \n",
       "395   0.000  0.000000  0.00000  ...  0.028802  0.027278  0.011641  0.034105   \n",
       "396   0.000  0.016321  0.15445  ...  0.029782  0.034993  0.058705  0.059477   \n",
       "397   0.000  0.000000  0.99250  ...  0.026287  0.003462  0.039576  0.020201   \n",
       "398   1.075  0.000000  0.00000  ...  0.026681  0.023454  0.010942  0.023830   \n",
       "399   0.000  1.104700  0.24705  ...  0.037796  0.037610  0.031290  0.033076   \n",
       "\n",
       "     GIST.378  GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  \n",
       "0    0.029998  0.008660  0.034269  0.010365  0.011738  0.030153  \n",
       "1    0.021178  0.016048  0.047683  0.058814  0.014283  0.021649  \n",
       "2    0.036008  0.004851  0.033030  0.020830  0.034741  0.050037  \n",
       "3    0.018619  0.005827  0.032792  0.030836  0.023793  0.022252  \n",
       "4    0.004130  0.030918  0.035200  0.032241  0.009805  0.006492  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "395  0.030931  0.028016  0.037107  0.015217  0.005206  0.050524  \n",
       "396  0.040081  0.039660  0.037121  0.039089  0.024199  0.039740  \n",
       "397  0.019792  0.031384  0.037379  0.022508  0.010190  0.023862  \n",
       "398  0.033937  0.015698  0.051196  0.004867  0.005816  0.046850  \n",
       "399  0.015958  0.037091  0.024476  0.032208  0.022142  0.020791  \n",
       "\n",
       "[400 rows x 3456 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1152ac-d416-461b-9182-0e2cc441fc30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33607</td>\n",
       "      <td>1.588400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036742</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053308</td>\n",
       "      <td>0.026501</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.255400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043143</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.023059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.080498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.027310</td>\n",
       "      <td>0.038010</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.034397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.044585</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>0.021064</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.90437</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.40552</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.042025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12083</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.027050</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>0.033778</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.098692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.14190</td>\n",
       "      <td>0.821120</td>\n",
       "      <td>0.83365</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31507</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.02660</td>\n",
       "      <td>0.083007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042222</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>0.039760</td>\n",
       "      <td>0.029942</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>0.048776</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.590700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.032747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23020</td>\n",
       "      <td>1.09890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.056895</td>\n",
       "      <td>0.033361</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42193</td>\n",
       "      <td>0.51987</td>\n",
       "      <td>0.425480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.051770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNNs  CNNs.1   CNNs.2    CNNs.3   CNNs.4   CNNs.5   CNNs.6   CNNs.7  \\\n",
       "0     0.000000     0.0  0.00000       NaN  0.00000  0.00000  0.00000  0.00000   \n",
       "1     0.000000     0.0  0.00000  0.000000  0.00000      NaN  0.00000  0.00000   \n",
       "2     0.000000     NaN  0.00000  0.080498      NaN  0.00000  0.00000      NaN   \n",
       "3     0.000000     0.0  0.39567  0.000000  0.00000  0.00000  0.00000      NaN   \n",
       "4          NaN     NaN      NaN  0.037334  0.00000  0.90437  1.17000  0.40552   \n",
       "...        ...     ...      ...       ...      ...      ...      ...      ...   \n",
       "2745  0.000000     NaN  0.12083  0.493970      NaN  0.00000  0.00000  0.00000   \n",
       "2746  0.098692     0.0  1.14190  0.821120  0.83365  0.00000  0.31507  0.00000   \n",
       "2747  0.000000     0.0  0.00000  2.590700  0.00000  0.00000  0.00000  0.00000   \n",
       "2748  0.549600     0.0      NaN  0.000000  0.00000      NaN  1.23020  1.09890   \n",
       "2749       NaN     0.0  0.51118  0.000000  0.00000  0.00000  0.00000  0.42193   \n",
       "\n",
       "       CNNs.8    CNNs.9  ...  GIST.376  GIST.377  GIST.378  GIST.379  \\\n",
       "0     0.33607  1.588400  ...  0.036742  0.012381       NaN  0.053308   \n",
       "1     0.00000  2.255400  ...  0.043143  0.019345  0.016736  0.008209   \n",
       "2     0.00000  0.000000  ...  0.039189  0.027310  0.038010  0.003747   \n",
       "3     0.00000  0.000000  ...  0.020592  0.044585  0.032217  0.054913   \n",
       "4     0.00000  0.212560  ...  0.003779  0.006411       NaN  0.003991   \n",
       "...       ...       ...  ...       ...       ...       ...       ...   \n",
       "2745  0.00000       NaN  ...  0.008865  0.027050  0.027571  0.022909   \n",
       "2746  1.02660  0.083007  ...  0.042222  0.042643  0.039760  0.029942   \n",
       "2747  0.00000  0.000000  ...  0.015427       NaN  0.021563  0.028100   \n",
       "2748  0.00000  0.026666  ...  0.002101  0.056895  0.033361  0.007273   \n",
       "2749  0.51987  0.425480  ...  0.020336  0.051770       NaN  0.024108   \n",
       "\n",
       "      GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0     0.026501  0.005391  0.001272  0.001446    0.0        0.66  \n",
       "1     0.023059       NaN       NaN  0.022575    0.0        0.66  \n",
       "2     0.016547       NaN  0.017964  0.034397    1.0        1.00  \n",
       "3     0.035068  0.021064  0.020542  0.033792    1.0        1.00  \n",
       "4     0.012906  0.008374  0.002190  0.042025    1.0        0.66  \n",
       "...        ...       ...       ...       ...    ...         ...  \n",
       "2745  0.033778  0.003859  0.003978  0.038523    0.0        0.66  \n",
       "2746  0.023028  0.013646  0.009340  0.048776    1.0        0.66  \n",
       "2747  0.032747       NaN  0.012894  0.026136    1.0        0.66  \n",
       "2748       NaN       NaN       NaN  0.042235    1.0        1.00  \n",
       "2749  0.009441       NaN  0.007373  0.025179    1.0        0.66  \n",
       "\n",
       "[2750 rows x 3458 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv(\"training2.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2862fe86-bd7c-4805-b125-246288f5756a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Splitting the dataframe into 2 dataframes so that CNN features and GIST featurtes can be imputed difefrently accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "713d366d-daf6-4b7a-8547-6358f8feb78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2_cnn=df2.iloc[:,:3072]\n",
    "df2_gist=df2.iloc[:,3072:-2]\n",
    "df2_labels=df2.iloc[:,-2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516befe1-d355-4d62-a7e0-3e7f6a976494",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>CNNs.3062</th>\n",
       "      <th>CNNs.3063</th>\n",
       "      <th>CNNs.3064</th>\n",
       "      <th>CNNs.3065</th>\n",
       "      <th>CNNs.3066</th>\n",
       "      <th>CNNs.3067</th>\n",
       "      <th>CNNs.3068</th>\n",
       "      <th>CNNs.3069</th>\n",
       "      <th>CNNs.3070</th>\n",
       "      <th>CNNs.3071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33607</td>\n",
       "      <td>1.588400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.30750</td>\n",
       "      <td>0.41131</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.255400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.523580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.33080</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.080498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.80140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184880</td>\n",
       "      <td>0.99247</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.102500</td>\n",
       "      <td>0.76338</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.90437</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.40552</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.304140</td>\n",
       "      <td>0.249960</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.11260</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.12083</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009762</td>\n",
       "      <td>0.052281</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.098692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.14190</td>\n",
       "      <td>0.821120</td>\n",
       "      <td>0.83365</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31507</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.02660</td>\n",
       "      <td>0.083007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.16659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.590700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.68780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.414850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.23020</td>\n",
       "      <td>1.09890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31271</td>\n",
       "      <td>0.272660</td>\n",
       "      <td>0.623160</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33232</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42193</td>\n",
       "      <td>0.51987</td>\n",
       "      <td>0.425480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.090369</td>\n",
       "      <td>0.068786</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 3072 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNNs  CNNs.1   CNNs.2    CNNs.3   CNNs.4   CNNs.5   CNNs.6   CNNs.7  \\\n",
       "0     0.000000     0.0  0.00000       NaN  0.00000  0.00000  0.00000  0.00000   \n",
       "1     0.000000     0.0  0.00000  0.000000  0.00000      NaN  0.00000  0.00000   \n",
       "2     0.000000     NaN  0.00000  0.080498      NaN  0.00000  0.00000      NaN   \n",
       "3     0.000000     0.0  0.39567  0.000000  0.00000  0.00000  0.00000      NaN   \n",
       "4          NaN     NaN      NaN  0.037334  0.00000  0.90437  1.17000  0.40552   \n",
       "...        ...     ...      ...       ...      ...      ...      ...      ...   \n",
       "2745  0.000000     NaN  0.12083  0.493970      NaN  0.00000  0.00000  0.00000   \n",
       "2746  0.098692     0.0  1.14190  0.821120  0.83365  0.00000  0.31507  0.00000   \n",
       "2747  0.000000     0.0  0.00000  2.590700  0.00000  0.00000  0.00000  0.00000   \n",
       "2748  0.549600     0.0      NaN  0.000000  0.00000      NaN  1.23020  1.09890   \n",
       "2749       NaN     0.0  0.51118  0.000000  0.00000  0.00000  0.00000  0.42193   \n",
       "\n",
       "       CNNs.8    CNNs.9  ...  CNNs.3062  CNNs.3063  CNNs.3064  CNNs.3065  \\\n",
       "0     0.33607  1.588400  ...    0.00000        NaN   0.000000    0.00000   \n",
       "1     0.00000  2.255400  ...    0.00000   0.000000        NaN    0.00000   \n",
       "2     0.00000  0.000000  ...    0.00000        NaN   0.000000        NaN   \n",
       "3     0.00000  0.000000  ...    0.49147   0.000000   1.102500    0.76338   \n",
       "4     0.00000  0.212560  ...    0.00000   0.000000   0.000000        NaN   \n",
       "...       ...       ...  ...        ...        ...        ...        ...   \n",
       "2745  0.00000       NaN  ...        NaN   0.009762   0.052281    0.00000   \n",
       "2746  1.02660  0.083007  ...    0.00000   0.000000   0.068887        NaN   \n",
       "2747  0.00000  0.000000  ...    0.38582   0.000000   0.000000    0.00000   \n",
       "2748  0.00000  0.026666  ...        NaN   0.000000        NaN    0.00000   \n",
       "2749  0.51987  0.425480  ...    0.21972        NaN        NaN    0.92351   \n",
       "\n",
       "      CNNs.3066  CNNs.3067  CNNs.3068  CNNs.3069  CNNs.3070  CNNs.3071  \n",
       "0       0.00000   0.000000   0.000000    1.30750    0.41131    0.00000  \n",
       "1       0.00000   0.523580   0.000000    0.33080    0.00000    0.00000  \n",
       "2       1.80140   0.000000   0.184880    0.99247    0.00000    0.00000  \n",
       "3       0.00000        NaN   0.000000    0.00000    0.00000    0.00000  \n",
       "4           NaN   0.304140   0.249960    0.00000    1.11260        NaN  \n",
       "...         ...        ...        ...        ...        ...        ...  \n",
       "2745    0.00000   0.000000   0.000000        NaN    0.00000    0.00000  \n",
       "2746    0.00000   0.017237   0.000000    0.16659        NaN    0.85018  \n",
       "2747    1.68780        NaN   0.414850    0.00000    0.00000    0.00000  \n",
       "2748    0.31271   0.272660   0.623160    0.00000    0.33232    0.00000  \n",
       "2749        NaN   0.090369   0.068786    0.00000        NaN    0.01276  \n",
       "\n",
       "[2750 rows x 3072 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71bebea1-3a2e-4624-9e17-d6309b70e9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GIST</th>\n",
       "      <th>GIST.1</th>\n",
       "      <th>GIST.2</th>\n",
       "      <th>GIST.3</th>\n",
       "      <th>GIST.4</th>\n",
       "      <th>GIST.5</th>\n",
       "      <th>GIST.6</th>\n",
       "      <th>GIST.7</th>\n",
       "      <th>GIST.8</th>\n",
       "      <th>GIST.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.374</th>\n",
       "      <th>GIST.375</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016265</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.041718</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.017142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005336</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036742</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.053308</td>\n",
       "      <td>0.026501</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012948</td>\n",
       "      <td>0.026028</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.045909</td>\n",
       "      <td>0.019803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009318</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040871</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>0.043143</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.023059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.054798</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>0.028166</td>\n",
       "      <td>0.049056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051623</td>\n",
       "      <td>0.023276</td>\n",
       "      <td>0.027843</td>\n",
       "      <td>0.033263</td>\n",
       "      <td>0.040112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035165</td>\n",
       "      <td>0.027588</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.027310</td>\n",
       "      <td>0.038010</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.034397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.028527</td>\n",
       "      <td>0.036634</td>\n",
       "      <td>0.042374</td>\n",
       "      <td>0.026994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049510</td>\n",
       "      <td>0.027773</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.044585</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>0.021064</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.033792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.035821</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>0.006080</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.013653</td>\n",
       "      <td>0.003287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.021205</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.042025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051623</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.059316</td>\n",
       "      <td>0.043091</td>\n",
       "      <td>0.010035</td>\n",
       "      <td>0.026302</td>\n",
       "      <td>0.025503</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013827</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.027050</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>0.033778</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.038523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.042698</td>\n",
       "      <td>0.019879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023117</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>0.038491</td>\n",
       "      <td>0.015234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030253</td>\n",
       "      <td>0.032428</td>\n",
       "      <td>0.042222</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>0.039760</td>\n",
       "      <td>0.029942</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>0.048776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.047701</td>\n",
       "      <td>0.034001</td>\n",
       "      <td>0.030565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.023263</td>\n",
       "      <td>0.028943</td>\n",
       "      <td>0.028276</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014122</td>\n",
       "      <td>0.035093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.032747</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.026136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.016756</td>\n",
       "      <td>0.067551</td>\n",
       "      <td>0.027872</td>\n",
       "      <td>0.014548</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.050812</td>\n",
       "      <td>0.023515</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.021684</td>\n",
       "      <td>0.018237</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018742</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.056895</td>\n",
       "      <td>0.033361</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>0.045552</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022588</td>\n",
       "      <td>0.005558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018289</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.015134</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.051770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.025179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GIST    GIST.1    GIST.2    GIST.3    GIST.4    GIST.5    GIST.6  \\\n",
       "0     0.016265       NaN  0.001430  0.041718  0.003476  0.017142       NaN   \n",
       "1     0.012948  0.026028  0.004586  0.045909  0.019803       NaN  0.009318   \n",
       "2     0.054798  0.040541  0.028166  0.049056       NaN  0.051623  0.023276   \n",
       "3          NaN  0.040860       NaN  0.028527  0.036634  0.042374  0.026994   \n",
       "4     0.007711       NaN  0.002845  0.035821  0.005590  0.006080  0.008861   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2745       NaN  0.051623  0.002341  0.059316  0.043091  0.010035  0.026302   \n",
       "2746  0.042698  0.019879       NaN       NaN  0.023117  0.031304  0.038491   \n",
       "2747  0.047701  0.034001  0.030565       NaN  0.023263  0.028943  0.028276   \n",
       "2748  0.016756  0.067551  0.027872  0.014548  0.009484  0.050812  0.023515   \n",
       "2749  0.045552  0.004186       NaN  0.022588  0.005558       NaN  0.018289   \n",
       "\n",
       "        GIST.7    GIST.8    GIST.9  ...  GIST.374  GIST.375  GIST.376  \\\n",
       "0          NaN  0.005336  0.007241  ...  0.007640       NaN  0.036742   \n",
       "1     0.016345  0.022464  0.072051  ...  0.040871  0.020330  0.043143   \n",
       "2     0.027843  0.033263  0.040112  ...  0.035165  0.027588  0.039189   \n",
       "3          NaN  0.022641       NaN  ...  0.049510  0.027773  0.020592   \n",
       "4     0.002636  0.013653  0.003287  ...  0.003357  0.021205  0.003779   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2745  0.025503       NaN  0.013827  ...       NaN       NaN  0.008865   \n",
       "2746  0.015234       NaN  0.017417  ...  0.030253  0.032428  0.042222   \n",
       "2747       NaN  0.014122  0.035093  ...  0.031710  0.021337  0.015427   \n",
       "2748  0.009091  0.021684  0.018237  ...       NaN  0.018742  0.002101   \n",
       "2749  0.002997  0.015134  0.022733  ...       NaN       NaN  0.020336   \n",
       "\n",
       "      GIST.377  GIST.378  GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  \n",
       "0     0.012381       NaN  0.053308  0.026501  0.005391  0.001272  0.001446  \n",
       "1     0.019345  0.016736  0.008209  0.023059       NaN       NaN  0.022575  \n",
       "2     0.027310  0.038010  0.003747  0.016547       NaN  0.017964  0.034397  \n",
       "3     0.044585  0.032217  0.054913  0.035068  0.021064  0.020542  0.033792  \n",
       "4     0.006411       NaN  0.003991  0.012906  0.008374  0.002190  0.042025  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2745  0.027050  0.027571  0.022909  0.033778  0.003859  0.003978  0.038523  \n",
       "2746  0.042643  0.039760  0.029942  0.023028  0.013646  0.009340  0.048776  \n",
       "2747       NaN  0.021563  0.028100  0.032747       NaN  0.012894  0.026136  \n",
       "2748  0.056895  0.033361  0.007273       NaN       NaN       NaN  0.042235  \n",
       "2749  0.051770       NaN  0.024108  0.009441       NaN  0.007373  0.025179  \n",
       "\n",
       "[2750 rows x 384 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_gist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfacbb39-9df4-48b3-92c0-bdcd68431fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  confidence\n",
       "0       0.0        0.66\n",
       "1       0.0        0.66\n",
       "2       1.0        1.00\n",
       "3       1.0        1.00\n",
       "4       1.0        0.66\n",
       "...     ...         ...\n",
       "2745    0.0        0.66\n",
       "2746    1.0        0.66\n",
       "2747    1.0        0.66\n",
       "2748    1.0        1.00\n",
       "2749    1.0        0.66\n",
       "\n",
       "[2750 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f821462-ff90-4f32-8c0a-335f8806ec60",
   "metadata": {},
   "source": [
    "## Impute NaN values with most frequent values. For both the CNN and Gist features their respective  most frequent values will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52fa2d5a-f006-40c9-b79f-0288b6813d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>CNNs.3062</th>\n",
       "      <th>CNNs.3063</th>\n",
       "      <th>CNNs.3064</th>\n",
       "      <th>CNNs.3065</th>\n",
       "      <th>CNNs.3066</th>\n",
       "      <th>CNNs.3067</th>\n",
       "      <th>CNNs.3068</th>\n",
       "      <th>CNNs.3069</th>\n",
       "      <th>CNNs.3070</th>\n",
       "      <th>CNNs.3071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33607</td>\n",
       "      <td>1.588400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.30750</td>\n",
       "      <td>0.41131</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.255400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.523580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.33080</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.080498</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.80140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.184880</td>\n",
       "      <td>0.99247</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.49147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.102500</td>\n",
       "      <td>0.76338</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.90437</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.40552</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.304140</td>\n",
       "      <td>0.249960</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.11260</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12083</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.009762</td>\n",
       "      <td>0.052281</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.098692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.14190</td>\n",
       "      <td>0.821120</td>\n",
       "      <td>0.83365</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31507</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.02660</td>\n",
       "      <td>0.083007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068887</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.16659</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.85018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.590700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.38582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.68780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.23020</td>\n",
       "      <td>1.09890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31271</td>\n",
       "      <td>0.272660</td>\n",
       "      <td>0.623160</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33232</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42193</td>\n",
       "      <td>0.51987</td>\n",
       "      <td>0.425480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.92351</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.090369</td>\n",
       "      <td>0.068786</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.01276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 3072 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNNs  CNNs.1   CNNs.2    CNNs.3   CNNs.4   CNNs.5   CNNs.6   CNNs.7  \\\n",
       "0     0.000000     0.0  0.00000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "1     0.000000     0.0  0.00000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "2     0.000000     0.0  0.00000  0.080498  0.00000  0.00000  0.00000  0.00000   \n",
       "3     0.000000     0.0  0.39567  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "4     0.000000     0.0  0.00000  0.037334  0.00000  0.90437  1.17000  0.40552   \n",
       "...        ...     ...      ...       ...      ...      ...      ...      ...   \n",
       "2745  0.000000     0.0  0.12083  0.493970  0.00000  0.00000  0.00000  0.00000   \n",
       "2746  0.098692     0.0  1.14190  0.821120  0.83365  0.00000  0.31507  0.00000   \n",
       "2747  0.000000     0.0  0.00000  2.590700  0.00000  0.00000  0.00000  0.00000   \n",
       "2748  0.549600     0.0  0.00000  0.000000  0.00000  0.00000  1.23020  1.09890   \n",
       "2749  0.000000     0.0  0.51118  0.000000  0.00000  0.00000  0.00000  0.42193   \n",
       "\n",
       "       CNNs.8    CNNs.9  ...  CNNs.3062  CNNs.3063  CNNs.3064  CNNs.3065  \\\n",
       "0     0.33607  1.588400  ...    0.00000   0.000000   0.000000    0.00000   \n",
       "1     0.00000  2.255400  ...    0.00000   0.000000   0.000000    0.00000   \n",
       "2     0.00000  0.000000  ...    0.00000   0.000000   0.000000    0.00000   \n",
       "3     0.00000  0.000000  ...    0.49147   0.000000   1.102500    0.76338   \n",
       "4     0.00000  0.212560  ...    0.00000   0.000000   0.000000    0.00000   \n",
       "...       ...       ...  ...        ...        ...        ...        ...   \n",
       "2745  0.00000  0.000000  ...    0.00000   0.009762   0.052281    0.00000   \n",
       "2746  1.02660  0.083007  ...    0.00000   0.000000   0.068887    0.00000   \n",
       "2747  0.00000  0.000000  ...    0.38582   0.000000   0.000000    0.00000   \n",
       "2748  0.00000  0.026666  ...    0.00000   0.000000   0.000000    0.00000   \n",
       "2749  0.51987  0.425480  ...    0.21972   0.000000   0.000000    0.92351   \n",
       "\n",
       "      CNNs.3066  CNNs.3067  CNNs.3068  CNNs.3069  CNNs.3070  CNNs.3071  \n",
       "0       0.00000   0.000000   0.000000    1.30750    0.41131    0.00000  \n",
       "1       0.00000   0.523580   0.000000    0.33080    0.00000    0.00000  \n",
       "2       1.80140   0.000000   0.184880    0.99247    0.00000    0.00000  \n",
       "3       0.00000   0.000000   0.000000    0.00000    0.00000    0.00000  \n",
       "4       0.00000   0.304140   0.249960    0.00000    1.11260    0.00000  \n",
       "...         ...        ...        ...        ...        ...        ...  \n",
       "2745    0.00000   0.000000   0.000000    0.00000    0.00000    0.00000  \n",
       "2746    0.00000   0.017237   0.000000    0.16659    0.00000    0.85018  \n",
       "2747    1.68780   0.000000   0.414850    0.00000    0.00000    0.00000  \n",
       "2748    0.31271   0.272660   0.623160    0.00000    0.33232    0.00000  \n",
       "2749    0.00000   0.090369   0.068786    0.00000    0.00000    0.01276  \n",
       "\n",
       "[2750 rows x 3072 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For CNN Features\n",
    "final_df2_cnn=[]\n",
    "for row in range(len(df2_cnn)):\n",
    "    nparray=np.array(df2_cnn.iloc[row,:])\n",
    "    modeofrow=mode(df2_cnn.iloc[row,:])\n",
    "    nparray=np.where(np.isnan(nparray),modeofrow,nparray)\n",
    "    final_df2_cnn.append(nparray)\n",
    "    \n",
    "    \n",
    "    nan_mask = np.isnan(nparray)\n",
    "\n",
    "    # Check if any NaN value is present in the entire array\n",
    "    is_nan_present = np.any(nan_mask)\n",
    "    \n",
    "final_df2_cnn=pd.DataFrame(final_df2_cnn,columns=df2_cnn.columns)\n",
    "final_df2_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23409121-f6c1-44aa-a294-1f9945e30a05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final_df2_cnn.isnull().sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45ead433-eb23-4015-8d3e-152c73018f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "s=set()\n",
    "i=0\n",
    "for val in np.array(df2_gist.iloc[0,:]):\n",
    "    # print(type(val))\n",
    "    if math.isnan(val)==True:\n",
    "        # print(val,i)\n",
    "        i+=1\n",
    "    s.add(val)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86dcb6f8-9ff0-424b-8a29-3bbbccb4b4ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016265"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode(df2_gist.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e36450da-5158-4e9f-9153-a5fec7beccec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_gist['GIST'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4e94a8b-6505-48a8-b4e7-3fae963c3d03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/1591067508.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df2_gist['GIST'].fillna(modeofrow,inplace=True)\n",
      "/tmp/ipykernel_40/1591067508.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_gist['GIST'].fillna(modeofrow,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode(df2_gist['GIST'])\n",
    "df2_gist['GIST'].fillna(modeofrow,inplace=True)\n",
    "df2_gist['GIST'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e18c8ca4-2fc5-449a-8377-a246ddf6c775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/1435441666.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df2_gist[col].fillna(modeofrow,inplace=True)\n",
      "/tmp/ipykernel_40/1435441666.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_gist[col].fillna(modeofrow,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for col in df2_gist.columns:\n",
    "   \n",
    "   \n",
    "    modeofrow=mode(df2_gist[col])\n",
    "    df2_gist[col].fillna(modeofrow,inplace=True)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2d5de3f-e89e-4f8f-9254-42bb90c83dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIST        0\n",
       "GIST.1      0\n",
       "GIST.2      0\n",
       "GIST.3      0\n",
       "GIST.4      0\n",
       "           ..\n",
       "GIST.379    0\n",
       "GIST.380    0\n",
       "GIST.381    0\n",
       "GIST.382    0\n",
       "GIST.383    0\n",
       "Length: 384, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_gist.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d48fa94b-ff7e-420f-9ad4-b4da4ad574fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33607</td>\n",
       "      <td>1.588400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036742</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>0.011663</td>\n",
       "      <td>0.053308</td>\n",
       "      <td>0.026501</td>\n",
       "      <td>0.005391</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.255400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043143</td>\n",
       "      <td>0.019345</td>\n",
       "      <td>0.016736</td>\n",
       "      <td>0.008209</td>\n",
       "      <td>0.023059</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.022575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.080498</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.027310</td>\n",
       "      <td>0.038010</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.034397</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020592</td>\n",
       "      <td>0.044585</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.054913</td>\n",
       "      <td>0.035068</td>\n",
       "      <td>0.021064</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.037334</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.90437</td>\n",
       "      <td>1.17000</td>\n",
       "      <td>0.40552</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.212560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.011663</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.008374</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.042025</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12083</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008865</td>\n",
       "      <td>0.027050</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>0.033778</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.038523</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.098692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.14190</td>\n",
       "      <td>0.821120</td>\n",
       "      <td>0.83365</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31507</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.02660</td>\n",
       "      <td>0.083007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042222</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>0.039760</td>\n",
       "      <td>0.029942</td>\n",
       "      <td>0.023028</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>0.048776</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.590700</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>0.050419</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.032747</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.012894</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.549600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.23020</td>\n",
       "      <td>1.09890</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.026666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.056895</td>\n",
       "      <td>0.033361</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.017723</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.042235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.51118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42193</td>\n",
       "      <td>0.51987</td>\n",
       "      <td>0.425480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.051770</td>\n",
       "      <td>0.011663</td>\n",
       "      <td>0.024108</td>\n",
       "      <td>0.009441</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNNs  CNNs.1   CNNs.2    CNNs.3   CNNs.4   CNNs.5   CNNs.6   CNNs.7  \\\n",
       "0     0.000000     0.0  0.00000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "1     0.000000     0.0  0.00000  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "2     0.000000     0.0  0.00000  0.080498  0.00000  0.00000  0.00000  0.00000   \n",
       "3     0.000000     0.0  0.39567  0.000000  0.00000  0.00000  0.00000  0.00000   \n",
       "4     0.000000     0.0  0.00000  0.037334  0.00000  0.90437  1.17000  0.40552   \n",
       "...        ...     ...      ...       ...      ...      ...      ...      ...   \n",
       "2745  0.000000     0.0  0.12083  0.493970  0.00000  0.00000  0.00000  0.00000   \n",
       "2746  0.098692     0.0  1.14190  0.821120  0.83365  0.00000  0.31507  0.00000   \n",
       "2747  0.000000     0.0  0.00000  2.590700  0.00000  0.00000  0.00000  0.00000   \n",
       "2748  0.549600     0.0  0.00000  0.000000  0.00000  0.00000  1.23020  1.09890   \n",
       "2749  0.000000     0.0  0.51118  0.000000  0.00000  0.00000  0.00000  0.42193   \n",
       "\n",
       "       CNNs.8    CNNs.9  ...  GIST.376  GIST.377  GIST.378  GIST.379  \\\n",
       "0     0.33607  1.588400  ...  0.036742  0.012381  0.011663  0.053308   \n",
       "1     0.00000  2.255400  ...  0.043143  0.019345  0.016736  0.008209   \n",
       "2     0.00000  0.000000  ...  0.039189  0.027310  0.038010  0.003747   \n",
       "3     0.00000  0.000000  ...  0.020592  0.044585  0.032217  0.054913   \n",
       "4     0.00000  0.212560  ...  0.003779  0.006411  0.011663  0.003991   \n",
       "...       ...       ...  ...       ...       ...       ...       ...   \n",
       "2745  0.00000  0.000000  ...  0.008865  0.027050  0.027571  0.022909   \n",
       "2746  1.02660  0.083007  ...  0.042222  0.042643  0.039760  0.029942   \n",
       "2747  0.00000  0.000000  ...  0.015427  0.050419  0.021563  0.028100   \n",
       "2748  0.00000  0.026666  ...  0.002101  0.056895  0.033361  0.007273   \n",
       "2749  0.51987  0.425480  ...  0.020336  0.051770  0.011663  0.024108   \n",
       "\n",
       "      GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0     0.026501  0.005391  0.001272  0.001446    0.0        0.66  \n",
       "1     0.023059  0.015309  0.010528  0.022575    0.0        0.66  \n",
       "2     0.016547  0.015309  0.017964  0.034397    1.0        1.00  \n",
       "3     0.035068  0.021064  0.020542  0.033792    1.0        1.00  \n",
       "4     0.012906  0.008374  0.002190  0.042025    1.0        0.66  \n",
       "...        ...       ...       ...       ...    ...         ...  \n",
       "2745  0.033778  0.003859  0.003978  0.038523    0.0        0.66  \n",
       "2746  0.023028  0.013646  0.009340  0.048776    1.0        0.66  \n",
       "2747  0.032747  0.015309  0.012894  0.026136    1.0        0.66  \n",
       "2748  0.017723  0.015309  0.010528  0.042235    1.0        1.00  \n",
       "2749  0.009441  0.015309  0.007373  0.025179    1.0        0.66  \n",
       "\n",
       "[2750 rows x 3458 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_new=pd.concat([final_df2_cnn,df2_gist,df2_labels],axis=1)\n",
    "df2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f1e446c-3886-478d-942d-07476114969a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Normalized\n",
    "\n",
    "df2_normalized = (df2_new - df2_new.min()) / (df2_new.max() - df2_new.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5d23298-26ec-43b6-b0a5-eb713827e83f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090094</td>\n",
       "      <td>0.461235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253763</td>\n",
       "      <td>0.078853</td>\n",
       "      <td>0.128822</td>\n",
       "      <td>0.348250</td>\n",
       "      <td>0.278519</td>\n",
       "      <td>0.036071</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.010796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298776</td>\n",
       "      <td>0.127733</td>\n",
       "      <td>0.187819</td>\n",
       "      <td>0.050996</td>\n",
       "      <td>0.241325</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.158589</td>\n",
       "      <td>0.251122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270971</td>\n",
       "      <td>0.183638</td>\n",
       "      <td>0.435229</td>\n",
       "      <td>0.021584</td>\n",
       "      <td>0.170955</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.284358</td>\n",
       "      <td>0.385590</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140194</td>\n",
       "      <td>0.304888</td>\n",
       "      <td>0.367858</td>\n",
       "      <td>0.358829</td>\n",
       "      <td>0.371095</td>\n",
       "      <td>0.198026</td>\n",
       "      <td>0.327960</td>\n",
       "      <td>0.378708</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251893</td>\n",
       "      <td>0.416326</td>\n",
       "      <td>0.172415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021960</td>\n",
       "      <td>0.036947</td>\n",
       "      <td>0.128822</td>\n",
       "      <td>0.023193</td>\n",
       "      <td>0.131610</td>\n",
       "      <td>0.066899</td>\n",
       "      <td>0.017571</td>\n",
       "      <td>0.472354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036412</td>\n",
       "      <td>0.159397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.181813</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.147884</td>\n",
       "      <td>0.357155</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>0.432521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.055576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344112</td>\n",
       "      <td>0.264963</td>\n",
       "      <td>0.233091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275213</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292299</td>\n",
       "      <td>0.291258</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>0.194240</td>\n",
       "      <td>0.240990</td>\n",
       "      <td>0.121372</td>\n",
       "      <td>0.138489</td>\n",
       "      <td>0.549142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103873</td>\n",
       "      <td>0.345836</td>\n",
       "      <td>0.243956</td>\n",
       "      <td>0.182099</td>\n",
       "      <td>0.346014</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.198606</td>\n",
       "      <td>0.291626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.309494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437747</td>\n",
       "      <td>0.467219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>0.391290</td>\n",
       "      <td>0.381162</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>0.183663</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.158589</td>\n",
       "      <td>0.474742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179392</td>\n",
       "      <td>0.139368</td>\n",
       "      <td>0.123550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138394</td>\n",
       "      <td>0.355319</td>\n",
       "      <td>0.128822</td>\n",
       "      <td>0.155787</td>\n",
       "      <td>0.094168</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.105224</td>\n",
       "      <td>0.280741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2750 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNNs  CNNs.1    CNNs.2    CNNs.3    CNNs.4    CNNs.5    CNNs.6  \\\n",
       "0     0.000000     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.000000     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.000000     0.0  0.000000  0.025975  0.000000  0.000000  0.000000   \n",
       "3     0.000000     0.0  0.119235  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000     0.0  0.000000  0.012047  0.000000  0.251893  0.416326   \n",
       "...        ...     ...       ...       ...       ...       ...       ...   \n",
       "2745  0.000000     0.0  0.036412  0.159397  0.000000  0.000000  0.000000   \n",
       "2746  0.055576     0.0  0.344112  0.264963  0.233091  0.000000  0.112113   \n",
       "2747  0.000000     0.0  0.000000  0.835979  0.000000  0.000000  0.000000   \n",
       "2748  0.309494     0.0  0.000000  0.000000  0.000000  0.000000  0.437747   \n",
       "2749  0.000000     0.0  0.154044  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        CNNs.7    CNNs.8    CNNs.9  ...  GIST.376  GIST.377  GIST.378  \\\n",
       "0     0.000000  0.090094  0.461235  ...  0.253763  0.078853  0.128822   \n",
       "1     0.000000  0.000000  0.654916  ...  0.298776  0.127733  0.187819   \n",
       "2     0.000000  0.000000  0.000000  ...  0.270971  0.183638  0.435229   \n",
       "3     0.000000  0.000000  0.000000  ...  0.140194  0.304888  0.367858   \n",
       "4     0.172415  0.000000  0.061723  ...  0.021960  0.036947  0.128822   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2745  0.000000  0.000000  0.000000  ...  0.057726  0.181813  0.313827   \n",
       "2746  0.000000  0.275213  0.024103  ...  0.292299  0.291258  0.455581   \n",
       "2747  0.000000  0.000000  0.000000  ...  0.103873  0.345836  0.243956   \n",
       "2748  0.467219  0.000000  0.007743  ...  0.010161  0.391290  0.381162   \n",
       "2749  0.179392  0.139368  0.123550  ...  0.138394  0.355319  0.128822   \n",
       "\n",
       "      GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0     0.348250  0.278519  0.036071  0.002045  0.010796    0.0         0.0  \n",
       "1     0.050996  0.241325  0.138557  0.158589  0.251122    0.0         0.0  \n",
       "2     0.021584  0.170955  0.138557  0.284358  0.385590    1.0         1.0  \n",
       "3     0.358829  0.371095  0.198026  0.327960  0.378708    1.0         1.0  \n",
       "4     0.023193  0.131610  0.066899  0.017571  0.472354    1.0         0.0  \n",
       "...        ...       ...       ...       ...       ...    ...         ...  \n",
       "2745  0.147884  0.357155  0.020240  0.047797  0.432521    0.0         0.0  \n",
       "2746  0.194240  0.240990  0.121372  0.138489  0.549142    1.0         0.0  \n",
       "2747  0.182099  0.346014  0.138557  0.198606  0.291626    1.0         0.0  \n",
       "2748  0.044828  0.183663  0.138557  0.158589  0.474742    1.0         1.0  \n",
       "2749  0.155787  0.094168  0.138557  0.105224  0.280741    1.0         0.0  \n",
       "\n",
       "[2750 rows x 3458 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46bf4fb9-d2e0-4279-a3e6-82eeebb44cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1_normalized = (df1 - df1.min()) / (df1.max() - df1.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43423d33-8918-4972-a668-c4e60d91290a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134441</td>\n",
       "      <td>0.290898</td>\n",
       "      <td>0.052439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537558</td>\n",
       "      <td>0.634006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337514</td>\n",
       "      <td>0.335107</td>\n",
       "      <td>0.356106</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.309591</td>\n",
       "      <td>0.097111</td>\n",
       "      <td>0.181742</td>\n",
       "      <td>0.259996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.478059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320866</td>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.248476</td>\n",
       "      <td>0.089248</td>\n",
       "      <td>0.432596</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.185148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103742</td>\n",
       "      <td>0.277181</td>\n",
       "      <td>0.429447</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.298230</td>\n",
       "      <td>0.214834</td>\n",
       "      <td>0.560897</td>\n",
       "      <td>0.435005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118034</td>\n",
       "      <td>0.128009</td>\n",
       "      <td>0.217248</td>\n",
       "      <td>0.028289</td>\n",
       "      <td>0.296047</td>\n",
       "      <td>0.327394</td>\n",
       "      <td>0.380443</td>\n",
       "      <td>0.190455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.066785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059184</td>\n",
       "      <td>0.703093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287083</td>\n",
       "      <td>0.245011</td>\n",
       "      <td>0.040443</td>\n",
       "      <td>0.177931</td>\n",
       "      <td>0.318128</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.149874</td>\n",
       "      <td>0.051746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117770</td>\n",
       "      <td>0.299074</td>\n",
       "      <td>0.367492</td>\n",
       "      <td>0.160624</td>\n",
       "      <td>0.335615</td>\n",
       "      <td>0.151692</td>\n",
       "      <td>0.074083</td>\n",
       "      <td>0.439291</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080045</td>\n",
       "      <td>0.207851</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>0.059347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614935</td>\n",
       "      <td>0.534638</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.230067</td>\n",
       "      <td>0.335743</td>\n",
       "      <td>0.420235</td>\n",
       "      <td>0.387135</td>\n",
       "      <td>0.344376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412864</td>\n",
       "      <td>0.169984</td>\n",
       "      <td>0.231562</td>\n",
       "      <td>0.180710</td>\n",
       "      <td>0.338109</td>\n",
       "      <td>0.233710</td>\n",
       "      <td>0.156227</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110386</td>\n",
       "      <td>0.203677</td>\n",
       "      <td>0.404174</td>\n",
       "      <td>0.087161</td>\n",
       "      <td>0.464809</td>\n",
       "      <td>0.035256</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.406955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.225728</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484411</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325334</td>\n",
       "      <td>0.289520</td>\n",
       "      <td>0.184776</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.219790</td>\n",
       "      <td>0.342828</td>\n",
       "      <td>0.353230</td>\n",
       "      <td>0.177596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CNNs    CNNs.1    CNNs.2    CNNs.3    CNNs.4    CNNs.5    CNNs.6  \\\n",
       "0    0.000000  0.134441  0.290898  0.052439  0.000000  0.027480  0.000000   \n",
       "1    0.000000  0.000000  0.000000  0.081921  0.478059  0.000000  0.000000   \n",
       "2    0.656164  0.000000  0.000000  0.044646  0.027224  0.000000  0.000000   \n",
       "3    0.013589  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.000000  0.000000  0.037721  0.000000  0.002512  0.066785  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  0.000000  0.042170  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "396  0.000000  0.000000  0.000000  0.080045  0.207851  0.016978  0.000000   \n",
       "397  0.000000  0.000000  0.000000  0.000000  0.147438  0.000000  0.000000   \n",
       "398  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "399  0.225728  0.014401  0.000000  0.000000  0.000000  0.000000  0.077075   \n",
       "\n",
       "       CNNs.7    CNNs.8    CNNs.9  ...  GIST.376  GIST.377  GIST.378  \\\n",
       "0    0.000000  0.537558  0.634006  ...  0.337514  0.335107  0.356106   \n",
       "1    0.000000  0.000000  0.098621  ...  0.320866  0.452954  0.248476   \n",
       "2    0.000000  0.171155  0.000000  ...  0.103742  0.277181  0.429447   \n",
       "3    0.000000  0.000000  0.000000  ...  0.118034  0.128009  0.217248   \n",
       "4    0.000000  0.059184  0.703093  ...  0.287083  0.245011  0.040443   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395  0.000000  0.000000  0.000000  ...  0.117770  0.299074  0.367492   \n",
       "396  0.000000  0.007157  0.059347  ...  0.614935  0.534638  0.479150   \n",
       "397  0.000000  0.000000  0.381364  ...  0.412864  0.169984  0.231562   \n",
       "398  0.308624  0.000000  0.000000  ...  0.110386  0.203677  0.404174   \n",
       "399  0.000000  0.484411  0.094928  ...  0.325334  0.289520  0.184776   \n",
       "\n",
       "     GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0    0.045190  0.309591  0.097111  0.181742  0.259996    1.0         0.0  \n",
       "1    0.089248  0.432596  0.642127  0.223691  0.185148    0.0         1.0  \n",
       "2    0.022471  0.298230  0.214834  0.560897  0.435005    0.0         1.0  \n",
       "3    0.028289  0.296047  0.327394  0.380443  0.190455    0.0         0.0  \n",
       "4    0.177931  0.318128  0.343200  0.149874  0.051746    0.0         0.0  \n",
       "..        ...       ...       ...       ...       ...    ...         ...  \n",
       "395  0.160624  0.335615  0.151692  0.074083  0.439291    1.0         0.0  \n",
       "396  0.230067  0.335743  0.420235  0.387135  0.344376    0.0         0.0  \n",
       "397  0.180710  0.338109  0.233710  0.156227  0.204626    0.0         0.0  \n",
       "398  0.087161  0.464809  0.035256  0.084123  0.406955    1.0         0.0  \n",
       "399  0.214746  0.219790  0.342828  0.353230  0.177596    0.0         0.0  \n",
       "\n",
       "[400 rows x 3458 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b65445e6-e430-478b-99d8-403f5326e665",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134441</td>\n",
       "      <td>0.290898</td>\n",
       "      <td>0.052439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537558</td>\n",
       "      <td>0.634006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337514</td>\n",
       "      <td>0.335107</td>\n",
       "      <td>0.356106</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.309591</td>\n",
       "      <td>0.097111</td>\n",
       "      <td>0.181742</td>\n",
       "      <td>0.259996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.478059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320866</td>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.248476</td>\n",
       "      <td>0.089248</td>\n",
       "      <td>0.432596</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.185148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103742</td>\n",
       "      <td>0.277181</td>\n",
       "      <td>0.429447</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.298230</td>\n",
       "      <td>0.214834</td>\n",
       "      <td>0.560897</td>\n",
       "      <td>0.435005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118034</td>\n",
       "      <td>0.128009</td>\n",
       "      <td>0.217248</td>\n",
       "      <td>0.028289</td>\n",
       "      <td>0.296047</td>\n",
       "      <td>0.327394</td>\n",
       "      <td>0.380443</td>\n",
       "      <td>0.190455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.066785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059184</td>\n",
       "      <td>0.703093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287083</td>\n",
       "      <td>0.245011</td>\n",
       "      <td>0.040443</td>\n",
       "      <td>0.177931</td>\n",
       "      <td>0.318128</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.149874</td>\n",
       "      <td>0.051746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117770</td>\n",
       "      <td>0.299074</td>\n",
       "      <td>0.367492</td>\n",
       "      <td>0.160624</td>\n",
       "      <td>0.335615</td>\n",
       "      <td>0.151692</td>\n",
       "      <td>0.074083</td>\n",
       "      <td>0.439291</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080045</td>\n",
       "      <td>0.207851</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>0.059347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.614935</td>\n",
       "      <td>0.534638</td>\n",
       "      <td>0.479150</td>\n",
       "      <td>0.230067</td>\n",
       "      <td>0.335743</td>\n",
       "      <td>0.420235</td>\n",
       "      <td>0.387135</td>\n",
       "      <td>0.344376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.381364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412864</td>\n",
       "      <td>0.169984</td>\n",
       "      <td>0.231562</td>\n",
       "      <td>0.180710</td>\n",
       "      <td>0.338109</td>\n",
       "      <td>0.233710</td>\n",
       "      <td>0.156227</td>\n",
       "      <td>0.204626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110386</td>\n",
       "      <td>0.203677</td>\n",
       "      <td>0.404174</td>\n",
       "      <td>0.087161</td>\n",
       "      <td>0.464809</td>\n",
       "      <td>0.035256</td>\n",
       "      <td>0.084123</td>\n",
       "      <td>0.406955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.225728</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484411</td>\n",
       "      <td>0.094928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325334</td>\n",
       "      <td>0.289520</td>\n",
       "      <td>0.184776</td>\n",
       "      <td>0.214746</td>\n",
       "      <td>0.219790</td>\n",
       "      <td>0.342828</td>\n",
       "      <td>0.353230</td>\n",
       "      <td>0.177596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CNNs    CNNs.1    CNNs.2    CNNs.3    CNNs.4    CNNs.5    CNNs.6  \\\n",
       "0    0.000000  0.134441  0.290898  0.052439  0.000000  0.027480  0.000000   \n",
       "1    0.000000  0.000000  0.000000  0.081921  0.478059  0.000000  0.000000   \n",
       "2    0.656164  0.000000  0.000000  0.044646  0.027224  0.000000  0.000000   \n",
       "3    0.013589  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    0.000000  0.000000  0.037721  0.000000  0.002512  0.066785  0.000000   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "395  0.000000  0.042170  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "396  0.000000  0.000000  0.000000  0.080045  0.207851  0.016978  0.000000   \n",
       "397  0.000000  0.000000  0.000000  0.000000  0.147438  0.000000  0.000000   \n",
       "398  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "399  0.225728  0.014401  0.000000  0.000000  0.000000  0.000000  0.077075   \n",
       "\n",
       "       CNNs.7    CNNs.8    CNNs.9  ...  GIST.376  GIST.377  GIST.378  \\\n",
       "0    0.000000  0.537558  0.634006  ...  0.337514  0.335107  0.356106   \n",
       "1    0.000000  0.000000  0.098621  ...  0.320866  0.452954  0.248476   \n",
       "2    0.000000  0.171155  0.000000  ...  0.103742  0.277181  0.429447   \n",
       "3    0.000000  0.000000  0.000000  ...  0.118034  0.128009  0.217248   \n",
       "4    0.000000  0.059184  0.703093  ...  0.287083  0.245011  0.040443   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "395  0.000000  0.000000  0.000000  ...  0.117770  0.299074  0.367492   \n",
       "396  0.000000  0.007157  0.059347  ...  0.614935  0.534638  0.479150   \n",
       "397  0.000000  0.000000  0.381364  ...  0.412864  0.169984  0.231562   \n",
       "398  0.308624  0.000000  0.000000  ...  0.110386  0.203677  0.404174   \n",
       "399  0.000000  0.484411  0.094928  ...  0.325334  0.289520  0.184776   \n",
       "\n",
       "     GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0    0.045190  0.309591  0.097111  0.181742  0.259996    1.0         0.0  \n",
       "1    0.089248  0.432596  0.642127  0.223691  0.185148    0.0         1.0  \n",
       "2    0.022471  0.298230  0.214834  0.560897  0.435005    0.0         1.0  \n",
       "3    0.028289  0.296047  0.327394  0.380443  0.190455    0.0         0.0  \n",
       "4    0.177931  0.318128  0.343200  0.149874  0.051746    0.0         0.0  \n",
       "..        ...       ...       ...       ...       ...    ...         ...  \n",
       "395  0.160624  0.335615  0.151692  0.074083  0.439291    1.0         0.0  \n",
       "396  0.230067  0.335743  0.420235  0.387135  0.344376    0.0         0.0  \n",
       "397  0.180710  0.338109  0.233710  0.156227  0.204626    0.0         0.0  \n",
       "398  0.087161  0.464809  0.035256  0.084123  0.406955    1.0         0.0  \n",
       "399  0.214746  0.219790  0.342828  0.353230  0.177596    0.0         0.0  \n",
       "\n",
       "[400 rows x 3458 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8b07536-bf62-46ea-a1b2-8c0b338a1bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134441</td>\n",
       "      <td>0.290898</td>\n",
       "      <td>0.052439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537558</td>\n",
       "      <td>0.634006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337514</td>\n",
       "      <td>0.335107</td>\n",
       "      <td>0.356106</td>\n",
       "      <td>0.045190</td>\n",
       "      <td>0.309591</td>\n",
       "      <td>0.097111</td>\n",
       "      <td>0.181742</td>\n",
       "      <td>0.259996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>0.478059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320866</td>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.248476</td>\n",
       "      <td>0.089248</td>\n",
       "      <td>0.432596</td>\n",
       "      <td>0.642127</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.185148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.656164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044646</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103742</td>\n",
       "      <td>0.277181</td>\n",
       "      <td>0.429447</td>\n",
       "      <td>0.022471</td>\n",
       "      <td>0.298230</td>\n",
       "      <td>0.214834</td>\n",
       "      <td>0.560897</td>\n",
       "      <td>0.435005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118034</td>\n",
       "      <td>0.128009</td>\n",
       "      <td>0.217248</td>\n",
       "      <td>0.028289</td>\n",
       "      <td>0.296047</td>\n",
       "      <td>0.327394</td>\n",
       "      <td>0.380443</td>\n",
       "      <td>0.190455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037721</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.066785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059184</td>\n",
       "      <td>0.703093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287083</td>\n",
       "      <td>0.245011</td>\n",
       "      <td>0.040443</td>\n",
       "      <td>0.177931</td>\n",
       "      <td>0.318128</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.149874</td>\n",
       "      <td>0.051746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2745</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036412</td>\n",
       "      <td>0.159397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.181813</td>\n",
       "      <td>0.313827</td>\n",
       "      <td>0.147884</td>\n",
       "      <td>0.357155</td>\n",
       "      <td>0.020240</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>0.432521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2746</th>\n",
       "      <td>0.055576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344112</td>\n",
       "      <td>0.264963</td>\n",
       "      <td>0.233091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275213</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292299</td>\n",
       "      <td>0.291258</td>\n",
       "      <td>0.455581</td>\n",
       "      <td>0.194240</td>\n",
       "      <td>0.240990</td>\n",
       "      <td>0.121372</td>\n",
       "      <td>0.138489</td>\n",
       "      <td>0.549142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.835979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103873</td>\n",
       "      <td>0.345836</td>\n",
       "      <td>0.243956</td>\n",
       "      <td>0.182099</td>\n",
       "      <td>0.346014</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.198606</td>\n",
       "      <td>0.291626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>0.309494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437747</td>\n",
       "      <td>0.467219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>0.391290</td>\n",
       "      <td>0.381162</td>\n",
       "      <td>0.044828</td>\n",
       "      <td>0.183663</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.158589</td>\n",
       "      <td>0.474742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179392</td>\n",
       "      <td>0.139368</td>\n",
       "      <td>0.123550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138394</td>\n",
       "      <td>0.355319</td>\n",
       "      <td>0.128822</td>\n",
       "      <td>0.155787</td>\n",
       "      <td>0.094168</td>\n",
       "      <td>0.138557</td>\n",
       "      <td>0.105224</td>\n",
       "      <td>0.280741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3150 rows × 3458 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CNNs    CNNs.1    CNNs.2    CNNs.3    CNNs.4    CNNs.5    CNNs.6  \\\n",
       "0     0.000000  0.134441  0.290898  0.052439  0.000000  0.027480  0.000000   \n",
       "1     0.000000  0.000000  0.000000  0.081921  0.478059  0.000000  0.000000   \n",
       "2     0.656164  0.000000  0.000000  0.044646  0.027224  0.000000  0.000000   \n",
       "3     0.013589  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.037721  0.000000  0.002512  0.066785  0.000000   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2745  0.000000  0.000000  0.036412  0.159397  0.000000  0.000000  0.000000   \n",
       "2746  0.055576  0.000000  0.344112  0.264963  0.233091  0.000000  0.112113   \n",
       "2747  0.000000  0.000000  0.000000  0.835979  0.000000  0.000000  0.000000   \n",
       "2748  0.309494  0.000000  0.000000  0.000000  0.000000  0.000000  0.437747   \n",
       "2749  0.000000  0.000000  0.154044  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        CNNs.7    CNNs.8    CNNs.9  ...  GIST.376  GIST.377  GIST.378  \\\n",
       "0     0.000000  0.537558  0.634006  ...  0.337514  0.335107  0.356106   \n",
       "1     0.000000  0.000000  0.098621  ...  0.320866  0.452954  0.248476   \n",
       "2     0.000000  0.171155  0.000000  ...  0.103742  0.277181  0.429447   \n",
       "3     0.000000  0.000000  0.000000  ...  0.118034  0.128009  0.217248   \n",
       "4     0.000000  0.059184  0.703093  ...  0.287083  0.245011  0.040443   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2745  0.000000  0.000000  0.000000  ...  0.057726  0.181813  0.313827   \n",
       "2746  0.000000  0.275213  0.024103  ...  0.292299  0.291258  0.455581   \n",
       "2747  0.000000  0.000000  0.000000  ...  0.103873  0.345836  0.243956   \n",
       "2748  0.467219  0.000000  0.007743  ...  0.010161  0.391290  0.381162   \n",
       "2749  0.179392  0.139368  0.123550  ...  0.138394  0.355319  0.128822   \n",
       "\n",
       "      GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  label  confidence  \n",
       "0     0.045190  0.309591  0.097111  0.181742  0.259996    1.0         0.0  \n",
       "1     0.089248  0.432596  0.642127  0.223691  0.185148    0.0         1.0  \n",
       "2     0.022471  0.298230  0.214834  0.560897  0.435005    0.0         1.0  \n",
       "3     0.028289  0.296047  0.327394  0.380443  0.190455    0.0         0.0  \n",
       "4     0.177931  0.318128  0.343200  0.149874  0.051746    0.0         0.0  \n",
       "...        ...       ...       ...       ...       ...    ...         ...  \n",
       "2745  0.147884  0.357155  0.020240  0.047797  0.432521    0.0         0.0  \n",
       "2746  0.194240  0.240990  0.121372  0.138489  0.549142    1.0         0.0  \n",
       "2747  0.182099  0.346014  0.138557  0.198606  0.291626    1.0         0.0  \n",
       "2748  0.044828  0.183663  0.138557  0.158589  0.474742    1.0         1.0  \n",
       "2749  0.155787  0.094168  0.138557  0.105224  0.280741    1.0         0.0  \n",
       "\n",
       "[3150 rows x 3458 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new=pd.concat([df1_normalized,df2_normalized],axis=0)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74902175-3e2f-4aae-ba66-6d88ba5a15cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNs          0\n",
       "CNNs.1        0\n",
       "CNNs.2        0\n",
       "CNNs.3        0\n",
       "CNNs.4        0\n",
       "             ..\n",
       "GIST.381      0\n",
       "GIST.382      0\n",
       "GIST.383      0\n",
       "label         0\n",
       "confidence    0\n",
       "Length: 3458, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c4e7f3a-87a5-4f63-98b6-235e6f563474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3150, 3456), (3150, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array(df_new.iloc[:,:-2])\n",
    "Y=np.array(df_new.loc[:,['label']])\n",
    "X.shape,Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06e9b859-aa10-40f0-835e-0922813031fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer=adam, init=glorot_uniform, batch_size=40, epochs=50\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_480 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_481 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_482 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_483 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_484 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_485 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_486 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_487 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_488 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_489 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_490 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_491 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_492 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_493 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_494 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_495 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 7s 22ms/step - loss: 0.6602 - accuracy: 0.5539 - val_loss: 0.6159 - val_accuracy: 0.7308\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.5654 - accuracy: 0.7405 - val_loss: 0.6039 - val_accuracy: 0.7260\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5333 - accuracy: 0.7654 - val_loss: 0.5442 - val_accuracy: 0.7476\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5082 - accuracy: 0.7749 - val_loss: 0.5733 - val_accuracy: 0.7344\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4691 - accuracy: 0.7879 - val_loss: 0.5888 - val_accuracy: 0.7175\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4273 - accuracy: 0.8110 - val_loss: 0.5848 - val_accuracy: 0.7356\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.3793 - accuracy: 0.8146 - val_loss: 0.6881 - val_accuracy: 0.7236\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.3505 - accuracy: 0.8294 - val_loss: 0.7748 - val_accuracy: 0.6803\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.2888 - accuracy: 0.8507 - val_loss: 1.0234 - val_accuracy: 0.7067\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.1788 - accuracy: 0.9165 - val_loss: 1.4541 - val_accuracy: 0.7031\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1563 - accuracy: 0.9425 - val_loss: 1.2147 - val_accuracy: 0.6635\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1523 - accuracy: 0.9408 - val_loss: 1.2500 - val_accuracy: 0.7079\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0694 - accuracy: 0.9751 - val_loss: 2.1618 - val_accuracy: 0.6755\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0622 - accuracy: 0.9763 - val_loss: 1.9072 - val_accuracy: 0.6947\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.1001 - accuracy: 0.9573 - val_loss: 1.6277 - val_accuracy: 0.7067\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0587 - accuracy: 0.9799 - val_loss: 1.7286 - val_accuracy: 0.7284\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0619 - accuracy: 0.9781 - val_loss: 3.5394 - val_accuracy: 0.6983\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.0800 - accuracy: 0.9745 - val_loss: 1.5631 - val_accuracy: 0.7175\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0364 - accuracy: 0.9899 - val_loss: 2.5069 - val_accuracy: 0.7452\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.0133 - accuracy: 0.9964 - val_loss: 3.3968 - val_accuracy: 0.6863\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 3.7819 - val_accuracy: 0.7127\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 2.1825e-04 - accuracy: 1.0000 - val_loss: 4.4650 - val_accuracy: 0.6935\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.6453e-05 - accuracy: 1.0000 - val_loss: 4.6494 - val_accuracy: 0.6935\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 6.7604e-06 - accuracy: 1.0000 - val_loss: 4.8194 - val_accuracy: 0.6971\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 4.3162e-06 - accuracy: 1.0000 - val_loss: 4.9735 - val_accuracy: 0.6983\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 3.1136e-06 - accuracy: 1.0000 - val_loss: 5.1236 - val_accuracy: 0.6959\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.2552e-06 - accuracy: 1.0000 - val_loss: 5.2199 - val_accuracy: 0.6959\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.8011e-06 - accuracy: 1.0000 - val_loss: 5.3167 - val_accuracy: 0.6959\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.4958e-06 - accuracy: 1.0000 - val_loss: 5.4175 - val_accuracy: 0.6947\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.2402e-06 - accuracy: 1.0000 - val_loss: 5.5085 - val_accuracy: 0.6959\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.0376e-06 - accuracy: 1.0000 - val_loss: 5.5965 - val_accuracy: 0.6971\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 8.7622e-07 - accuracy: 1.0000 - val_loss: 5.6990 - val_accuracy: 0.6971\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 7.2217e-07 - accuracy: 1.0000 - val_loss: 5.8052 - val_accuracy: 0.6971\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 5.9786e-07 - accuracy: 1.0000 - val_loss: 5.9285 - val_accuracy: 0.6971\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 4.8215e-07 - accuracy: 1.0000 - val_loss: 6.0393 - val_accuracy: 0.6971\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 3.9800e-07 - accuracy: 1.0000 - val_loss: 6.1515 - val_accuracy: 0.6971\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 3.2947e-07 - accuracy: 1.0000 - val_loss: 6.2631 - val_accuracy: 0.6971\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.7779e-07 - accuracy: 1.0000 - val_loss: 6.3726 - val_accuracy: 0.6971\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.3756e-07 - accuracy: 1.0000 - val_loss: 6.4855 - val_accuracy: 0.6971\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 2.0438e-07 - accuracy: 1.0000 - val_loss: 6.5796 - val_accuracy: 0.6971\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.7782e-07 - accuracy: 1.0000 - val_loss: 6.6702 - val_accuracy: 0.6971\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.5505e-07 - accuracy: 1.0000 - val_loss: 6.7375 - val_accuracy: 0.6971\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.3768e-07 - accuracy: 1.0000 - val_loss: 6.8406 - val_accuracy: 0.6971\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.2044e-07 - accuracy: 1.0000 - val_loss: 6.9106 - val_accuracy: 0.6971\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.0850e-07 - accuracy: 1.0000 - val_loss: 6.9852 - val_accuracy: 0.6983\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 9.7543e-08 - accuracy: 1.0000 - val_loss: 7.0481 - val_accuracy: 0.6983\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 8.8451e-08 - accuracy: 1.0000 - val_loss: 7.1096 - val_accuracy: 0.6983\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 7.9882e-08 - accuracy: 1.0000 - val_loss: 7.1722 - val_accuracy: 0.6983\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 7.2635e-08 - accuracy: 1.0000 - val_loss: 7.2346 - val_accuracy: 0.6983\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 6.6411e-08 - accuracy: 1.0000 - val_loss: 7.2904 - val_accuracy: 0.6983\n",
      "Validation accuracy: 0.6809523701667786\n",
      "\n",
      "Training with optimizer=adam, init=glorot_uniform, batch_size=40, epochs=100\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_496 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_497 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_498 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_499 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_500 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_501 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_502 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_503 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_504 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_505 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_506 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_507 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_508 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_509 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_510 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_511 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "43/43 [==============================] - 7s 19ms/step - loss: 0.6667 - accuracy: 0.5308 - val_loss: 0.6577 - val_accuracy: 0.6803\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5743 - accuracy: 0.7447 - val_loss: 0.5531 - val_accuracy: 0.7416\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5251 - accuracy: 0.7618 - val_loss: 0.5374 - val_accuracy: 0.7476\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4789 - accuracy: 0.7956 - val_loss: 0.5480 - val_accuracy: 0.7512\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.4390 - accuracy: 0.8098 - val_loss: 0.6948 - val_accuracy: 0.7380\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.2073 - accuracy: 0.9224 - val_loss: 0.9389 - val_accuracy: 0.6851\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.2269 - accuracy: 0.9040 - val_loss: 0.7975 - val_accuracy: 0.7115\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.2352 - accuracy: 0.9141 - val_loss: 1.2819 - val_accuracy: 0.6839\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.1864 - accuracy: 0.9277 - val_loss: 1.1656 - val_accuracy: 0.6803\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.1373 - accuracy: 0.9449 - val_loss: 1.2639 - val_accuracy: 0.6647\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.1061 - accuracy: 0.9579 - val_loss: 1.6498 - val_accuracy: 0.6478\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0758 - accuracy: 0.9692 - val_loss: 2.3227 - val_accuracy: 0.7019\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.1304 - accuracy: 0.9585 - val_loss: 1.0244 - val_accuracy: 0.7127\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0834 - accuracy: 0.9727 - val_loss: 1.5555 - val_accuracy: 0.6767\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0265 - accuracy: 0.9911 - val_loss: 3.1662 - val_accuracy: 0.6514\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.1366 - accuracy: 0.9627 - val_loss: 2.6401 - val_accuracy: 0.6839\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.2372 - accuracy: 0.9259 - val_loss: 1.3372 - val_accuracy: 0.6779\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0664 - accuracy: 0.9769 - val_loss: 1.9137 - val_accuracy: 0.6755\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0185 - accuracy: 0.9953 - val_loss: 3.6919 - val_accuracy: 0.7248\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 4.0801 - val_accuracy: 0.7067\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.0120 - accuracy: 0.9953 - val_loss: 3.2729 - val_accuracy: 0.6995\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 2.7101 - val_accuracy: 0.7031\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 6.1792 - val_accuracy: 0.6935\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 7.8117e-05 - accuracy: 1.0000 - val_loss: 6.8184 - val_accuracy: 0.6935\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.5912e-05 - accuracy: 1.0000 - val_loss: 7.1362 - val_accuracy: 0.6911\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 9.9815e-06 - accuracy: 1.0000 - val_loss: 7.1364 - val_accuracy: 0.6911\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 7.4561e-06 - accuracy: 1.0000 - val_loss: 7.1799 - val_accuracy: 0.6911\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 5.7803e-06 - accuracy: 1.0000 - val_loss: 7.2099 - val_accuracy: 0.6923\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 4.6199e-06 - accuracy: 1.0000 - val_loss: 7.2350 - val_accuracy: 0.6935\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 3.8498e-06 - accuracy: 1.0000 - val_loss: 7.2615 - val_accuracy: 0.6923\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 3.3080e-06 - accuracy: 1.0000 - val_loss: 7.3077 - val_accuracy: 0.6935\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 2.8712e-06 - accuracy: 1.0000 - val_loss: 7.3462 - val_accuracy: 0.6935\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.5082e-06 - accuracy: 1.0000 - val_loss: 7.3865 - val_accuracy: 0.6935\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 2.2016e-06 - accuracy: 1.0000 - val_loss: 7.4098 - val_accuracy: 0.6923\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.9671e-06 - accuracy: 1.0000 - val_loss: 7.4573 - val_accuracy: 0.6923\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.7813e-06 - accuracy: 1.0000 - val_loss: 7.5065 - val_accuracy: 0.6923\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.6229e-06 - accuracy: 1.0000 - val_loss: 7.5397 - val_accuracy: 0.6923\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 1.4770e-06 - accuracy: 1.0000 - val_loss: 7.5797 - val_accuracy: 0.6923\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.3568e-06 - accuracy: 1.0000 - val_loss: 7.6284 - val_accuracy: 0.6911\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.2404e-06 - accuracy: 1.0000 - val_loss: 7.6638 - val_accuracy: 0.6923\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.1373e-06 - accuracy: 1.0000 - val_loss: 7.7108 - val_accuracy: 0.6923\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.0506e-06 - accuracy: 1.0000 - val_loss: 7.7516 - val_accuracy: 0.6911\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 9.7633e-07 - accuracy: 1.0000 - val_loss: 7.7917 - val_accuracy: 0.6935\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 9.0786e-07 - accuracy: 1.0000 - val_loss: 7.8314 - val_accuracy: 0.6935\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 8.4777e-07 - accuracy: 1.0000 - val_loss: 7.8719 - val_accuracy: 0.6911\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 7.9487e-07 - accuracy: 1.0000 - val_loss: 7.9100 - val_accuracy: 0.6911\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 7.4673e-07 - accuracy: 1.0000 - val_loss: 7.9496 - val_accuracy: 0.6899\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 7.0106e-07 - accuracy: 1.0000 - val_loss: 7.9843 - val_accuracy: 0.6899\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 6.6184e-07 - accuracy: 1.0000 - val_loss: 8.0233 - val_accuracy: 0.6899\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 6.2471e-07 - accuracy: 1.0000 - val_loss: 8.0632 - val_accuracy: 0.6911\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 5.8942e-07 - accuracy: 1.0000 - val_loss: 8.1058 - val_accuracy: 0.6923\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 5.5742e-07 - accuracy: 1.0000 - val_loss: 8.1411 - val_accuracy: 0.6935\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 5.2819e-07 - accuracy: 1.0000 - val_loss: 8.1727 - val_accuracy: 0.6935\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 5.0085e-07 - accuracy: 1.0000 - val_loss: 8.2119 - val_accuracy: 0.6935\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 4.7534e-07 - accuracy: 1.0000 - val_loss: 8.2413 - val_accuracy: 0.6935\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 4.5224e-07 - accuracy: 1.0000 - val_loss: 8.2799 - val_accuracy: 0.6935\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 4.2925e-07 - accuracy: 1.0000 - val_loss: 8.3167 - val_accuracy: 0.6923\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 4.0934e-07 - accuracy: 1.0000 - val_loss: 8.3371 - val_accuracy: 0.6923\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 3.9035e-07 - accuracy: 1.0000 - val_loss: 8.3807 - val_accuracy: 0.6935\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 3.7165e-07 - accuracy: 1.0000 - val_loss: 8.4068 - val_accuracy: 0.6935\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 3.5516e-07 - accuracy: 1.0000 - val_loss: 8.4552 - val_accuracy: 0.6935\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 3.3922e-07 - accuracy: 1.0000 - val_loss: 8.4880 - val_accuracy: 0.6923\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 3.2367e-07 - accuracy: 1.0000 - val_loss: 8.5166 - val_accuracy: 0.6923\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 3.0975e-07 - accuracy: 1.0000 - val_loss: 8.5507 - val_accuracy: 0.6923\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 2.9662e-07 - accuracy: 1.0000 - val_loss: 8.5866 - val_accuracy: 0.6923\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.8320e-07 - accuracy: 1.0000 - val_loss: 8.6145 - val_accuracy: 0.6923\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.7128e-07 - accuracy: 1.0000 - val_loss: 8.6504 - val_accuracy: 0.6923\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.5996e-07 - accuracy: 1.0000 - val_loss: 8.6824 - val_accuracy: 0.6911\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.4918e-07 - accuracy: 1.0000 - val_loss: 8.7152 - val_accuracy: 0.6911\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 2.3928e-07 - accuracy: 1.0000 - val_loss: 8.7413 - val_accuracy: 0.6923\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.2983e-07 - accuracy: 1.0000 - val_loss: 8.7763 - val_accuracy: 0.6911\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.2074e-07 - accuracy: 1.0000 - val_loss: 8.8019 - val_accuracy: 0.6899\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.1220e-07 - accuracy: 1.0000 - val_loss: 8.8361 - val_accuracy: 0.6875\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.0390e-07 - accuracy: 1.0000 - val_loss: 8.8676 - val_accuracy: 0.6875\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.9607e-07 - accuracy: 1.0000 - val_loss: 8.8923 - val_accuracy: 0.6875\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.8814e-07 - accuracy: 1.0000 - val_loss: 8.9210 - val_accuracy: 0.6875\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.8124e-07 - accuracy: 1.0000 - val_loss: 8.9562 - val_accuracy: 0.6875\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.7439e-07 - accuracy: 1.0000 - val_loss: 8.9835 - val_accuracy: 0.6875\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.6806e-07 - accuracy: 1.0000 - val_loss: 9.0094 - val_accuracy: 0.6875\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.6184e-07 - accuracy: 1.0000 - val_loss: 9.0386 - val_accuracy: 0.6863\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.5574e-07 - accuracy: 1.0000 - val_loss: 9.0636 - val_accuracy: 0.6863\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.4967e-07 - accuracy: 1.0000 - val_loss: 9.0951 - val_accuracy: 0.6863\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.4442e-07 - accuracy: 1.0000 - val_loss: 9.1217 - val_accuracy: 0.6851\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.3956e-07 - accuracy: 1.0000 - val_loss: 9.1470 - val_accuracy: 0.6851\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.3457e-07 - accuracy: 1.0000 - val_loss: 9.1739 - val_accuracy: 0.6839\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 1.3008e-07 - accuracy: 1.0000 - val_loss: 9.2000 - val_accuracy: 0.6839\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.2557e-07 - accuracy: 1.0000 - val_loss: 9.2268 - val_accuracy: 0.6839\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.2140e-07 - accuracy: 1.0000 - val_loss: 9.2504 - val_accuracy: 0.6839\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.1732e-07 - accuracy: 1.0000 - val_loss: 9.2740 - val_accuracy: 0.6839\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.1331e-07 - accuracy: 1.0000 - val_loss: 9.3039 - val_accuracy: 0.6827\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 1.0931e-07 - accuracy: 1.0000 - val_loss: 9.3318 - val_accuracy: 0.6827\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.0574e-07 - accuracy: 1.0000 - val_loss: 9.3559 - val_accuracy: 0.6839\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.0227e-07 - accuracy: 1.0000 - val_loss: 9.3805 - val_accuracy: 0.6839\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 9.8902e-08 - accuracy: 1.0000 - val_loss: 9.4098 - val_accuracy: 0.6851\n",
      "Validation accuracy: 0.6666666865348816\n",
      "\n",
      "Training with optimizer=adam, init=glorot_uniform, batch_size=80, epochs=50\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_512 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_513 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_514 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_515 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_516 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_517 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_518 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_519 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_520 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_521 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_522 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_523 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_524 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_525 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_526 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_527 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 7s 27ms/step - loss: 0.6920 - accuracy: 0.5255 - val_loss: 0.6880 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6568 - accuracy: 0.5563 - val_loss: 0.5969 - val_accuracy: 0.7332\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.5753 - accuracy: 0.7435 - val_loss: 0.5554 - val_accuracy: 0.7452\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.5491 - accuracy: 0.7518 - val_loss: 0.5597 - val_accuracy: 0.7392\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.5164 - accuracy: 0.7695 - val_loss: 0.5588 - val_accuracy: 0.7284\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 1s 26ms/step - loss: 0.4978 - accuracy: 0.7743 - val_loss: 0.6150 - val_accuracy: 0.7476\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.4904 - accuracy: 0.7844 - val_loss: 0.5814 - val_accuracy: 0.7356\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4471 - accuracy: 0.7897 - val_loss: 0.6607 - val_accuracy: 0.7452\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4153 - accuracy: 0.7980 - val_loss: 0.8752 - val_accuracy: 0.7392\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3348 - accuracy: 0.8412 - val_loss: 1.1226 - val_accuracy: 0.7524\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.2660 - accuracy: 0.8667 - val_loss: 1.1479 - val_accuracy: 0.7248\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.1685 - accuracy: 0.9289 - val_loss: 1.7025 - val_accuracy: 0.7224\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.1534 - accuracy: 0.9396 - val_loss: 1.3703 - val_accuracy: 0.7224\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.2135 - accuracy: 0.9212 - val_loss: 0.9891 - val_accuracy: 0.6839\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.1344 - accuracy: 0.9508 - val_loss: 1.8444 - val_accuracy: 0.7175\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0761 - accuracy: 0.9763 - val_loss: 2.0959 - val_accuracy: 0.6875\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0495 - accuracy: 0.9822 - val_loss: 2.0167 - val_accuracy: 0.6575\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0550 - accuracy: 0.9787 - val_loss: 1.9431 - val_accuracy: 0.6719\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0230 - accuracy: 0.9905 - val_loss: 3.2152 - val_accuracy: 0.7031\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.0584 - accuracy: 0.9852 - val_loss: 2.8806 - val_accuracy: 0.7019\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0521 - accuracy: 0.9810 - val_loss: 1.7862 - val_accuracy: 0.6683\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0125 - accuracy: 0.9953 - val_loss: 2.9767 - val_accuracy: 0.7091\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0339 - accuracy: 0.9923 - val_loss: 4.2410 - val_accuracy: 0.6647\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.2036 - accuracy: 0.9390 - val_loss: 2.2035 - val_accuracy: 0.7212\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0479 - accuracy: 0.9822 - val_loss: 2.3541 - val_accuracy: 0.7067\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 3.2941 - val_accuracy: 0.6899\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 7.2535e-04 - accuracy: 1.0000 - val_loss: 4.6156 - val_accuracy: 0.6995\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.7042e-05 - accuracy: 1.0000 - val_loss: 4.9782 - val_accuracy: 0.6851\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 9.5411e-06 - accuracy: 1.0000 - val_loss: 5.2912 - val_accuracy: 0.6827\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 4.8225e-06 - accuracy: 1.0000 - val_loss: 5.5735 - val_accuracy: 0.6851\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.9545e-06 - accuracy: 1.0000 - val_loss: 5.8548 - val_accuracy: 0.6851\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.7549e-06 - accuracy: 1.0000 - val_loss: 6.1200 - val_accuracy: 0.6863\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.1993e-06 - accuracy: 1.0000 - val_loss: 6.3343 - val_accuracy: 0.6863\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 9.3680e-07 - accuracy: 1.0000 - val_loss: 6.5414 - val_accuracy: 0.6863\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 6.5833e-07 - accuracy: 1.0000 - val_loss: 6.6551 - val_accuracy: 0.6863\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 5.4567e-07 - accuracy: 1.0000 - val_loss: 6.7920 - val_accuracy: 0.6875\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 4.4286e-07 - accuracy: 1.0000 - val_loss: 6.9053 - val_accuracy: 0.6875\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.7882e-07 - accuracy: 1.0000 - val_loss: 7.0245 - val_accuracy: 0.6875\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.1780e-07 - accuracy: 1.0000 - val_loss: 7.1058 - val_accuracy: 0.6875\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.7585e-07 - accuracy: 1.0000 - val_loss: 7.1743 - val_accuracy: 0.6875\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.5147e-07 - accuracy: 1.0000 - val_loss: 7.2836 - val_accuracy: 0.6875\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.1172e-07 - accuracy: 1.0000 - val_loss: 7.3481 - val_accuracy: 0.6875\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.5890e-07 - accuracy: 1.0000 - val_loss: 7.5457 - val_accuracy: 0.6875\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.3021e-07 - accuracy: 1.0000 - val_loss: 7.6120 - val_accuracy: 0.6875\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.1939e-07 - accuracy: 1.0000 - val_loss: 7.6661 - val_accuracy: 0.6875\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.0989e-07 - accuracy: 1.0000 - val_loss: 7.7066 - val_accuracy: 0.6875\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.0298e-07 - accuracy: 1.0000 - val_loss: 7.7610 - val_accuracy: 0.6875\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 9.6384e-08 - accuracy: 1.0000 - val_loss: 7.8158 - val_accuracy: 0.6875\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 8.9240e-08 - accuracy: 1.0000 - val_loss: 7.8565 - val_accuracy: 0.6875\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 8.3724e-08 - accuracy: 1.0000 - val_loss: 7.8974 - val_accuracy: 0.6875\n",
      "Validation accuracy: 0.6682539582252502\n",
      "\n",
      "Training with optimizer=adam, init=glorot_uniform, batch_size=80, epochs=100\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_528 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_529 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_530 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_531 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_532 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_533 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_534 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_535 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_536 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_537 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_538 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_539 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_540 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_541 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_542 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_543 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 9s 85ms/step - loss: 0.6622 - accuracy: 0.5373 - val_loss: 0.7008 - val_accuracy: 0.6923\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.5988 - accuracy: 0.7334 - val_loss: 0.6292 - val_accuracy: 0.7356\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.5290 - accuracy: 0.7601 - val_loss: 0.5428 - val_accuracy: 0.7452\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.4952 - accuracy: 0.7767 - val_loss: 0.5580 - val_accuracy: 0.7488\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.4217 - accuracy: 0.8116 - val_loss: 0.5973 - val_accuracy: 0.7368\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.3715 - accuracy: 0.8389 - val_loss: 0.7277 - val_accuracy: 0.6695\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.3374 - accuracy: 0.8608 - val_loss: 0.9222 - val_accuracy: 0.7200\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.3123 - accuracy: 0.8709 - val_loss: 0.7289 - val_accuracy: 0.7175\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.2857 - accuracy: 0.9023 - val_loss: 1.0014 - val_accuracy: 0.7212\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.2854 - accuracy: 0.8934 - val_loss: 0.8252 - val_accuracy: 0.7212\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.2375 - accuracy: 0.9100 - val_loss: 1.1514 - val_accuracy: 0.7236\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.1880 - accuracy: 0.9242 - val_loss: 1.0554 - val_accuracy: 0.6815\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 1s 24ms/step - loss: 0.1548 - accuracy: 0.9372 - val_loss: 0.9956 - val_accuracy: 0.6887\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.1420 - accuracy: 0.9443 - val_loss: 1.5364 - val_accuracy: 0.6719\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.1365 - accuracy: 0.9485 - val_loss: 1.2202 - val_accuracy: 0.7067\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0887 - accuracy: 0.9662 - val_loss: 2.2679 - val_accuracy: 0.6755\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0578 - accuracy: 0.9793 - val_loss: 1.8508 - val_accuracy: 0.7151\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.0379 - accuracy: 0.9828 - val_loss: 2.2416 - val_accuracy: 0.6827\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0577 - accuracy: 0.9805 - val_loss: 1.8027 - val_accuracy: 0.6575\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0135 - accuracy: 0.9947 - val_loss: 2.8339 - val_accuracy: 0.6707\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0186 - accuracy: 0.9947 - val_loss: 2.8230 - val_accuracy: 0.6779\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0169 - accuracy: 0.9953 - val_loss: 2.3133 - val_accuracy: 0.6454\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 3.0638 - val_accuracy: 0.6887\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 3.5149 - val_accuracy: 0.7067\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.0094 - accuracy: 0.9964 - val_loss: 3.2523 - val_accuracy: 0.6851\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.0127 - accuracy: 0.9941 - val_loss: 2.6721 - val_accuracy: 0.6767\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.5566 - val_accuracy: 0.6791\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 2.9536e-04 - accuracy: 1.0000 - val_loss: 4.1680 - val_accuracy: 0.6839\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.8964e-04 - accuracy: 1.0000 - val_loss: 4.7447 - val_accuracy: 0.6947\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.8124e-05 - accuracy: 1.0000 - val_loss: 4.7173 - val_accuracy: 0.6851\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.3648e-05 - accuracy: 1.0000 - val_loss: 4.7332 - val_accuracy: 0.6779\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 7.9501e-06 - accuracy: 1.0000 - val_loss: 4.7518 - val_accuracy: 0.6803\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 5.9654e-06 - accuracy: 1.0000 - val_loss: 4.7708 - val_accuracy: 0.6815\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 4.6685e-06 - accuracy: 1.0000 - val_loss: 4.7895 - val_accuracy: 0.6827\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 3.8156e-06 - accuracy: 1.0000 - val_loss: 4.8090 - val_accuracy: 0.6839\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 3.0519e-06 - accuracy: 1.0000 - val_loss: 4.8311 - val_accuracy: 0.6839\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 2.5317e-06 - accuracy: 1.0000 - val_loss: 4.8546 - val_accuracy: 0.6779\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.1324e-06 - accuracy: 1.0000 - val_loss: 4.8749 - val_accuracy: 0.6767\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 1.8439e-06 - accuracy: 1.0000 - val_loss: 4.8937 - val_accuracy: 0.6743\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.6132e-06 - accuracy: 1.0000 - val_loss: 4.9129 - val_accuracy: 0.6755\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.4419e-06 - accuracy: 1.0000 - val_loss: 4.9420 - val_accuracy: 0.6755\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.2842e-06 - accuracy: 1.0000 - val_loss: 4.9633 - val_accuracy: 0.6755\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.1439e-06 - accuracy: 1.0000 - val_loss: 4.9817 - val_accuracy: 0.6755\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.0268e-06 - accuracy: 1.0000 - val_loss: 4.9994 - val_accuracy: 0.6767\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 9.4384e-07 - accuracy: 1.0000 - val_loss: 5.0191 - val_accuracy: 0.6767\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 8.4969e-07 - accuracy: 1.0000 - val_loss: 5.0379 - val_accuracy: 0.6767\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 7.7922e-07 - accuracy: 1.0000 - val_loss: 5.0558 - val_accuracy: 0.6767\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 7.2175e-07 - accuracy: 1.0000 - val_loss: 5.0748 - val_accuracy: 0.6767\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 6.6071e-07 - accuracy: 1.0000 - val_loss: 5.0932 - val_accuracy: 0.6767\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 6.0504e-07 - accuracy: 1.0000 - val_loss: 5.1139 - val_accuracy: 0.6767\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 5.6024e-07 - accuracy: 1.0000 - val_loss: 5.1319 - val_accuracy: 0.6755\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 5.1977e-07 - accuracy: 1.0000 - val_loss: 5.1480 - val_accuracy: 0.6755\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4.8799e-07 - accuracy: 1.0000 - val_loss: 5.1648 - val_accuracy: 0.6755\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4.5629e-07 - accuracy: 1.0000 - val_loss: 5.1821 - val_accuracy: 0.6755\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4.2861e-07 - accuracy: 1.0000 - val_loss: 5.1994 - val_accuracy: 0.6755\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 4.0235e-07 - accuracy: 1.0000 - val_loss: 5.2157 - val_accuracy: 0.6743\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 3.7850e-07 - accuracy: 1.0000 - val_loss: 5.2321 - val_accuracy: 0.6743\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 3.5898e-07 - accuracy: 1.0000 - val_loss: 5.2485 - val_accuracy: 0.6743\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 3.3907e-07 - accuracy: 1.0000 - val_loss: 5.2658 - val_accuracy: 0.6755\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 3.2195e-07 - accuracy: 1.0000 - val_loss: 5.2824 - val_accuracy: 0.6755\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.0404e-07 - accuracy: 1.0000 - val_loss: 5.2998 - val_accuracy: 0.6755\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 2.8980e-07 - accuracy: 1.0000 - val_loss: 5.3164 - val_accuracy: 0.6755\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.7485e-07 - accuracy: 1.0000 - val_loss: 5.3337 - val_accuracy: 0.6743\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.6030e-07 - accuracy: 1.0000 - val_loss: 5.3485 - val_accuracy: 0.6743\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 2.4802e-07 - accuracy: 1.0000 - val_loss: 5.3642 - val_accuracy: 0.6743\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.3741e-07 - accuracy: 1.0000 - val_loss: 5.3821 - val_accuracy: 0.6743\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 2.2602e-07 - accuracy: 1.0000 - val_loss: 5.3979 - val_accuracy: 0.6743\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.1555e-07 - accuracy: 1.0000 - val_loss: 5.4120 - val_accuracy: 0.6743\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.0669e-07 - accuracy: 1.0000 - val_loss: 5.4271 - val_accuracy: 0.6743\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.9793e-07 - accuracy: 1.0000 - val_loss: 5.4498 - val_accuracy: 0.6743\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.8897e-07 - accuracy: 1.0000 - val_loss: 5.4651 - val_accuracy: 0.6743\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.8141e-07 - accuracy: 1.0000 - val_loss: 5.4792 - val_accuracy: 0.6731\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.7412e-07 - accuracy: 1.0000 - val_loss: 5.4966 - val_accuracy: 0.6731\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.6704e-07 - accuracy: 1.0000 - val_loss: 5.5116 - val_accuracy: 0.6731\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.6091e-07 - accuracy: 1.0000 - val_loss: 5.5265 - val_accuracy: 0.6743\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.5444e-07 - accuracy: 1.0000 - val_loss: 5.5392 - val_accuracy: 0.6743\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.4851e-07 - accuracy: 1.0000 - val_loss: 5.5520 - val_accuracy: 0.6743\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.4338e-07 - accuracy: 1.0000 - val_loss: 5.5667 - val_accuracy: 0.6743\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.3798e-07 - accuracy: 1.0000 - val_loss: 5.5796 - val_accuracy: 0.6743\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 1.3310e-07 - accuracy: 1.0000 - val_loss: 5.5923 - val_accuracy: 0.6743\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.2865e-07 - accuracy: 1.0000 - val_loss: 5.6062 - val_accuracy: 0.6743\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.2384e-07 - accuracy: 1.0000 - val_loss: 5.6234 - val_accuracy: 0.6743\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.1947e-07 - accuracy: 1.0000 - val_loss: 5.6368 - val_accuracy: 0.6743\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.1557e-07 - accuracy: 1.0000 - val_loss: 5.6503 - val_accuracy: 0.6743\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.1183e-07 - accuracy: 1.0000 - val_loss: 5.6627 - val_accuracy: 0.6743\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.0779e-07 - accuracy: 1.0000 - val_loss: 5.6758 - val_accuracy: 0.6743\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.0278e-07 - accuracy: 1.0000 - val_loss: 5.6931 - val_accuracy: 0.6743\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 9.7674e-08 - accuracy: 1.0000 - val_loss: 5.7044 - val_accuracy: 0.6743\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 9.4735e-08 - accuracy: 1.0000 - val_loss: 5.7179 - val_accuracy: 0.6755\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 9.1646e-08 - accuracy: 1.0000 - val_loss: 5.7304 - val_accuracy: 0.6755\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 8.8813e-08 - accuracy: 1.0000 - val_loss: 5.7416 - val_accuracy: 0.6755\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 8.6278e-08 - accuracy: 1.0000 - val_loss: 5.7529 - val_accuracy: 0.6755\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 8.3705e-08 - accuracy: 1.0000 - val_loss: 5.7649 - val_accuracy: 0.6755\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 8.1406e-08 - accuracy: 1.0000 - val_loss: 5.7766 - val_accuracy: 0.6755\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 7.8996e-08 - accuracy: 1.0000 - val_loss: 5.7891 - val_accuracy: 0.6755\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 7.6614e-08 - accuracy: 1.0000 - val_loss: 5.8008 - val_accuracy: 0.6755\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 7.4394e-08 - accuracy: 1.0000 - val_loss: 5.8118 - val_accuracy: 0.6755\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 7.2358e-08 - accuracy: 1.0000 - val_loss: 5.8246 - val_accuracy: 0.6755\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 7.0345e-08 - accuracy: 1.0000 - val_loss: 5.8354 - val_accuracy: 0.6755\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 6.8262e-08 - accuracy: 1.0000 - val_loss: 5.8471 - val_accuracy: 0.6755\n",
      "Validation accuracy: 0.682539701461792\n",
      "\n",
      "Training with optimizer=adam, init=normal, batch_size=40, epochs=50\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_544 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_545 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_546 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_547 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_548 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_549 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_550 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_551 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_552 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_553 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_554 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_555 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_556 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_557 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_558 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_559 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 9s 24ms/step - loss: 0.6917 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Training with optimizer=adam, init=normal, batch_size=40, epochs=100\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_560 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_561 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_562 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_563 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_564 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_565 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_566 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_567 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_568 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_569 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_570 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_571 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_572 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_573 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_574 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_575 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "43/43 [==============================] - 8s 21ms/step - loss: 0.6924 - accuracy: 0.5201 - val_loss: 0.6917 - val_accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6927 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6785 - accuracy: 0.5723 - val_loss: 0.5611 - val_accuracy: 0.7356\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5780 - accuracy: 0.7269 - val_loss: 0.5749 - val_accuracy: 0.7248\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5424 - accuracy: 0.7476 - val_loss: 0.5608 - val_accuracy: 0.7428\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5187 - accuracy: 0.7672 - val_loss: 0.5555 - val_accuracy: 0.7308\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4887 - accuracy: 0.7737 - val_loss: 0.5632 - val_accuracy: 0.7404\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4627 - accuracy: 0.7879 - val_loss: 0.5601 - val_accuracy: 0.7308\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4445 - accuracy: 0.8039 - val_loss: 0.5602 - val_accuracy: 0.7452\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.3673 - accuracy: 0.8412 - val_loss: 0.8014 - val_accuracy: 0.7260\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.3260 - accuracy: 0.8655 - val_loss: 1.2580 - val_accuracy: 0.7380\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.2786 - accuracy: 0.8703 - val_loss: 1.0452 - val_accuracy: 0.7308\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.2246 - accuracy: 0.9005 - val_loss: 1.1072 - val_accuracy: 0.7284\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.1808 - accuracy: 0.9188 - val_loss: 1.3210 - val_accuracy: 0.7079\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.2020 - accuracy: 0.9100 - val_loss: 1.6846 - val_accuracy: 0.7127\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.1446 - accuracy: 0.9502 - val_loss: 2.2605 - val_accuracy: 0.7019\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0664 - accuracy: 0.9763 - val_loss: 3.1868 - val_accuracy: 0.6947\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0527 - accuracy: 0.9787 - val_loss: 3.3478 - val_accuracy: 0.7272\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.0589 - accuracy: 0.9733 - val_loss: 2.2458 - val_accuracy: 0.6971\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.0392 - accuracy: 0.9882 - val_loss: 2.9507 - val_accuracy: 0.7019\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 4.1510 - val_accuracy: 0.7175\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 4.7519 - val_accuracy: 0.6755\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0325 - accuracy: 0.9911 - val_loss: 2.2195 - val_accuracy: 0.6959\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0189 - accuracy: 0.9917 - val_loss: 3.1426 - val_accuracy: 0.6947\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 3.5543 - val_accuracy: 0.6911\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.0191 - accuracy: 0.9959 - val_loss: 2.1067 - val_accuracy: 0.6839\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 1s 20ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 3.0542 - val_accuracy: 0.7079\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 2.9073 - val_accuracy: 0.7103\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 5.4624e-04 - accuracy: 1.0000 - val_loss: 3.5764 - val_accuracy: 0.7079\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 7.9090e-05 - accuracy: 1.0000 - val_loss: 3.8346 - val_accuracy: 0.7115\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 4.7094e-05 - accuracy: 1.0000 - val_loss: 3.9950 - val_accuracy: 0.7067\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 3.3652e-05 - accuracy: 1.0000 - val_loss: 4.1055 - val_accuracy: 0.7067\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 2.6396e-05 - accuracy: 1.0000 - val_loss: 4.2096 - val_accuracy: 0.7067\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.1192e-05 - accuracy: 1.0000 - val_loss: 4.2892 - val_accuracy: 0.7079\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.7730e-05 - accuracy: 1.0000 - val_loss: 4.3658 - val_accuracy: 0.7079\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.4850e-05 - accuracy: 1.0000 - val_loss: 4.4380 - val_accuracy: 0.7079\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.2836e-05 - accuracy: 1.0000 - val_loss: 4.4973 - val_accuracy: 0.7079\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.1260e-05 - accuracy: 1.0000 - val_loss: 4.5492 - val_accuracy: 0.7067\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 9.9925e-06 - accuracy: 1.0000 - val_loss: 4.6012 - val_accuracy: 0.7079\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 8.9204e-06 - accuracy: 1.0000 - val_loss: 4.6463 - val_accuracy: 0.7067\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 8.0611e-06 - accuracy: 1.0000 - val_loss: 4.6934 - val_accuracy: 0.7079\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 7.2815e-06 - accuracy: 1.0000 - val_loss: 4.7348 - val_accuracy: 0.7079\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 6.6334e-06 - accuracy: 1.0000 - val_loss: 4.7738 - val_accuracy: 0.7091\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 6.0745e-06 - accuracy: 1.0000 - val_loss: 4.8108 - val_accuracy: 0.7091\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 5.5822e-06 - accuracy: 1.0000 - val_loss: 4.8466 - val_accuracy: 0.7091\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 5.1563e-06 - accuracy: 1.0000 - val_loss: 4.8828 - val_accuracy: 0.7091\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 4.7662e-06 - accuracy: 1.0000 - val_loss: 4.9139 - val_accuracy: 0.7091\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 4.4167e-06 - accuracy: 1.0000 - val_loss: 4.9462 - val_accuracy: 0.7091\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 4.1043e-06 - accuracy: 1.0000 - val_loss: 4.9791 - val_accuracy: 0.7091\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 3.8295e-06 - accuracy: 1.0000 - val_loss: 5.0098 - val_accuracy: 0.7091\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 3.5764e-06 - accuracy: 1.0000 - val_loss: 5.0372 - val_accuracy: 0.7091\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 3.3552e-06 - accuracy: 1.0000 - val_loss: 5.0641 - val_accuracy: 0.7091\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 3.1562e-06 - accuracy: 1.0000 - val_loss: 5.0913 - val_accuracy: 0.7091\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.9539e-06 - accuracy: 1.0000 - val_loss: 5.1200 - val_accuracy: 0.7091\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.7809e-06 - accuracy: 1.0000 - val_loss: 5.1459 - val_accuracy: 0.7091\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 2.6178e-06 - accuracy: 1.0000 - val_loss: 5.1719 - val_accuracy: 0.7091\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 2.4737e-06 - accuracy: 1.0000 - val_loss: 5.1956 - val_accuracy: 0.7091\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.3455e-06 - accuracy: 1.0000 - val_loss: 5.2195 - val_accuracy: 0.7091\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 2.2231e-06 - accuracy: 1.0000 - val_loss: 5.2420 - val_accuracy: 0.7091\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.1040e-06 - accuracy: 1.0000 - val_loss: 5.2648 - val_accuracy: 0.7091\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 2.0010e-06 - accuracy: 1.0000 - val_loss: 5.2874 - val_accuracy: 0.7091\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.9020e-06 - accuracy: 1.0000 - val_loss: 5.3079 - val_accuracy: 0.7091\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.8075e-06 - accuracy: 1.0000 - val_loss: 5.3308 - val_accuracy: 0.7091\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.7221e-06 - accuracy: 1.0000 - val_loss: 5.3505 - val_accuracy: 0.7091\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.6290e-06 - accuracy: 1.0000 - val_loss: 5.3771 - val_accuracy: 0.7091\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 1.5502e-06 - accuracy: 1.0000 - val_loss: 5.3970 - val_accuracy: 0.7091\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.4774e-06 - accuracy: 1.0000 - val_loss: 5.4181 - val_accuracy: 0.7091\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.4072e-06 - accuracy: 1.0000 - val_loss: 5.4375 - val_accuracy: 0.7091\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.3470e-06 - accuracy: 1.0000 - val_loss: 5.4571 - val_accuracy: 0.7091\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.2894e-06 - accuracy: 1.0000 - val_loss: 5.4756 - val_accuracy: 0.7091\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.2361e-06 - accuracy: 1.0000 - val_loss: 5.4930 - val_accuracy: 0.7091\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 1.1866e-06 - accuracy: 1.0000 - val_loss: 5.5123 - val_accuracy: 0.7091\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.1373e-06 - accuracy: 1.0000 - val_loss: 5.5292 - val_accuracy: 0.7091\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.0923e-06 - accuracy: 1.0000 - val_loss: 5.5471 - val_accuracy: 0.7091\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 1.0483e-06 - accuracy: 1.0000 - val_loss: 5.5642 - val_accuracy: 0.7091\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 1.0038e-06 - accuracy: 1.0000 - val_loss: 5.5865 - val_accuracy: 0.7091\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 9.6013e-07 - accuracy: 1.0000 - val_loss: 5.6027 - val_accuracy: 0.7091\n",
      "Validation accuracy: 0.6746031641960144\n",
      "\n",
      "Training with optimizer=adam, init=normal, batch_size=80, epochs=50\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_576 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_577 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_578 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_579 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_580 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_581 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_582 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_583 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_584 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_585 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_586 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_587 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_588 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_589 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_590 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_591 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 7s 32ms/step - loss: 0.6923 - accuracy: 0.5314 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6916 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 22ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Training with optimizer=adam, init=normal, batch_size=80, epochs=100\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_592 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_593 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_594 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_595 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_596 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_597 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_598 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_599 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_600 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_601 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_602 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_603 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_604 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_605 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_606 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_607 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 8s 40ms/step - loss: 0.6918 - accuracy: 0.5344 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6852 - accuracy: 0.5367 - val_loss: 0.6091 - val_accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6882 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6898 - val_accuracy: 0.5325\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6577 - accuracy: 0.6451 - val_loss: 0.6385 - val_accuracy: 0.6743\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6113 - accuracy: 0.7180 - val_loss: 0.5767 - val_accuracy: 0.7260\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.5370 - accuracy: 0.7541 - val_loss: 0.5564 - val_accuracy: 0.7392\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.5286 - accuracy: 0.7690 - val_loss: 0.5931 - val_accuracy: 0.7260\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.5039 - accuracy: 0.7672 - val_loss: 0.5723 - val_accuracy: 0.7416\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.4642 - accuracy: 0.8175 - val_loss: 0.5951 - val_accuracy: 0.7296\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.4230 - accuracy: 0.8276 - val_loss: 0.7964 - val_accuracy: 0.7236\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.4134 - accuracy: 0.8365 - val_loss: 0.6387 - val_accuracy: 0.7188\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3571 - accuracy: 0.8667 - val_loss: 0.6849 - val_accuracy: 0.6779\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3032 - accuracy: 0.8945 - val_loss: 0.9537 - val_accuracy: 0.7103\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.2422 - accuracy: 0.9153 - val_loss: 1.0533 - val_accuracy: 0.6863\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.2023 - accuracy: 0.9325 - val_loss: 1.0775 - val_accuracy: 0.7019\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.1838 - accuracy: 0.9384 - val_loss: 1.1931 - val_accuracy: 0.7043\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.1472 - accuracy: 0.9496 - val_loss: 1.9650 - val_accuracy: 0.6418\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1717 - accuracy: 0.9384 - val_loss: 1.3244 - val_accuracy: 0.7103\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.1810 - accuracy: 0.9408 - val_loss: 1.3566 - val_accuracy: 0.6947\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.1329 - accuracy: 0.9473 - val_loss: 2.0256 - val_accuracy: 0.7163\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1007 - accuracy: 0.9591 - val_loss: 1.3819 - val_accuracy: 0.6935\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0671 - accuracy: 0.9727 - val_loss: 2.3103 - val_accuracy: 0.6959\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0631 - accuracy: 0.9769 - val_loss: 1.4677 - val_accuracy: 0.6695\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0390 - accuracy: 0.9887 - val_loss: 2.8420 - val_accuracy: 0.7139\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 2.7674 - val_accuracy: 0.6659\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0573 - accuracy: 0.9763 - val_loss: 1.6802 - val_accuracy: 0.6791\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 3.5181 - val_accuracy: 0.6815\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 4.6677 - val_accuracy: 0.6899\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 4.4630e-04 - accuracy: 1.0000 - val_loss: 5.2962 - val_accuracy: 0.7067\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.2843e-04 - accuracy: 1.0000 - val_loss: 5.6085 - val_accuracy: 0.7067\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 7.9359e-05 - accuracy: 1.0000 - val_loss: 5.9201 - val_accuracy: 0.7019\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 4.6081e-05 - accuracy: 1.0000 - val_loss: 6.1854 - val_accuracy: 0.7019\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 1s 23ms/step - loss: 3.2760e-05 - accuracy: 1.0000 - val_loss: 6.4173 - val_accuracy: 0.6995\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 2.4712e-05 - accuracy: 1.0000 - val_loss: 6.6406 - val_accuracy: 0.6947\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.9515e-05 - accuracy: 1.0000 - val_loss: 6.8025 - val_accuracy: 0.6923\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.5989e-05 - accuracy: 1.0000 - val_loss: 7.0504 - val_accuracy: 0.6911\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.3643e-05 - accuracy: 1.0000 - val_loss: 7.1652 - val_accuracy: 0.6911\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.1772e-05 - accuracy: 1.0000 - val_loss: 7.2653 - val_accuracy: 0.6875\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.0254e-05 - accuracy: 1.0000 - val_loss: 7.3535 - val_accuracy: 0.6875\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 9.1004e-06 - accuracy: 1.0000 - val_loss: 7.5256 - val_accuracy: 0.6875\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 1s 30ms/step - loss: 8.0921e-06 - accuracy: 1.0000 - val_loss: 7.6037 - val_accuracy: 0.6875\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 6.3589e-06 - accuracy: 1.0000 - val_loss: 7.7922 - val_accuracy: 0.6863\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 3.7149e-06 - accuracy: 1.0000 - val_loss: 8.1703 - val_accuracy: 0.6827\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.8537e-06 - accuracy: 1.0000 - val_loss: 8.5138 - val_accuracy: 0.6815\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.0442e-06 - accuracy: 1.0000 - val_loss: 8.7787 - val_accuracy: 0.6803\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 6.9332e-07 - accuracy: 1.0000 - val_loss: 8.9799 - val_accuracy: 0.6803\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 4.9963e-07 - accuracy: 1.0000 - val_loss: 9.1556 - val_accuracy: 0.6803\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 3.9483e-07 - accuracy: 1.0000 - val_loss: 9.3050 - val_accuracy: 0.6803\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 3.1970e-07 - accuracy: 1.0000 - val_loss: 9.4234 - val_accuracy: 0.6803\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.6842e-07 - accuracy: 1.0000 - val_loss: 9.5434 - val_accuracy: 0.6791\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.2534e-07 - accuracy: 1.0000 - val_loss: 9.6358 - val_accuracy: 0.6803\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.9410e-07 - accuracy: 1.0000 - val_loss: 9.7376 - val_accuracy: 0.6803\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.6913e-07 - accuracy: 1.0000 - val_loss: 9.8394 - val_accuracy: 0.6803\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 1.4757e-07 - accuracy: 1.0000 - val_loss: 9.9266 - val_accuracy: 0.6803\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.3053e-07 - accuracy: 1.0000 - val_loss: 10.0094 - val_accuracy: 0.6803\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.1509e-07 - accuracy: 1.0000 - val_loss: 10.0832 - val_accuracy: 0.6815\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.0308e-07 - accuracy: 1.0000 - val_loss: 10.1486 - val_accuracy: 0.6815\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 9.3897e-08 - accuracy: 1.0000 - val_loss: 10.2256 - val_accuracy: 0.6827\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 8.4559e-08 - accuracy: 1.0000 - val_loss: 10.2881 - val_accuracy: 0.6827\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 7.7066e-08 - accuracy: 1.0000 - val_loss: 10.3480 - val_accuracy: 0.6827\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 7.0685e-08 - accuracy: 1.0000 - val_loss: 10.4075 - val_accuracy: 0.6827\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 6.4692e-08 - accuracy: 1.0000 - val_loss: 10.4564 - val_accuracy: 0.6827\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 5.9872e-08 - accuracy: 1.0000 - val_loss: 10.5082 - val_accuracy: 0.6827\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 5.5886e-08 - accuracy: 1.0000 - val_loss: 10.5699 - val_accuracy: 0.6827\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 5.1247e-08 - accuracy: 1.0000 - val_loss: 10.6268 - val_accuracy: 0.6827\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 4.7445e-08 - accuracy: 1.0000 - val_loss: 10.6708 - val_accuracy: 0.6827\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 4.4549e-08 - accuracy: 1.0000 - val_loss: 10.7205 - val_accuracy: 0.6827\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 4.1478e-08 - accuracy: 1.0000 - val_loss: 10.7617 - val_accuracy: 0.6827\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 3.9074e-08 - accuracy: 1.0000 - val_loss: 10.8032 - val_accuracy: 0.6827\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 3.6774e-08 - accuracy: 1.0000 - val_loss: 10.8466 - val_accuracy: 0.6827\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 3.4587e-08 - accuracy: 1.0000 - val_loss: 10.8836 - val_accuracy: 0.6827\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 3.2743e-08 - accuracy: 1.0000 - val_loss: 10.9238 - val_accuracy: 0.6827\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 3.0992e-08 - accuracy: 1.0000 - val_loss: 10.9634 - val_accuracy: 0.6827\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.9283e-08 - accuracy: 1.0000 - val_loss: 10.9976 - val_accuracy: 0.6827\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.7912e-08 - accuracy: 1.0000 - val_loss: 11.0372 - val_accuracy: 0.6827\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.6380e-08 - accuracy: 1.0000 - val_loss: 11.0689 - val_accuracy: 0.6827\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.5229e-08 - accuracy: 1.0000 - val_loss: 11.1047 - val_accuracy: 0.6827\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.3920e-08 - accuracy: 1.0000 - val_loss: 11.1304 - val_accuracy: 0.6827\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.3004e-08 - accuracy: 1.0000 - val_loss: 11.1641 - val_accuracy: 0.6827\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 2.1888e-08 - accuracy: 1.0000 - val_loss: 11.1938 - val_accuracy: 0.6827\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.1003e-08 - accuracy: 1.0000 - val_loss: 11.2268 - val_accuracy: 0.6827\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 2.0125e-08 - accuracy: 1.0000 - val_loss: 11.2585 - val_accuracy: 0.6827\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.9233e-08 - accuracy: 1.0000 - val_loss: 11.2826 - val_accuracy: 0.6827\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.8447e-08 - accuracy: 1.0000 - val_loss: 11.3096 - val_accuracy: 0.6827\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.7712e-08 - accuracy: 1.0000 - val_loss: 11.3375 - val_accuracy: 0.6827\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.7054e-08 - accuracy: 1.0000 - val_loss: 11.3671 - val_accuracy: 0.6827\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.6356e-08 - accuracy: 1.0000 - val_loss: 11.3896 - val_accuracy: 0.6827\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.5808e-08 - accuracy: 1.0000 - val_loss: 11.4179 - val_accuracy: 0.6827\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 1.5213e-08 - accuracy: 1.0000 - val_loss: 11.4425 - val_accuracy: 0.6827\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 1.4712e-08 - accuracy: 1.0000 - val_loss: 11.4700 - val_accuracy: 0.6827\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.4126e-08 - accuracy: 1.0000 - val_loss: 11.4899 - val_accuracy: 0.6815\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 1.3703e-08 - accuracy: 1.0000 - val_loss: 11.5154 - val_accuracy: 0.6815\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 1.3224e-08 - accuracy: 1.0000 - val_loss: 11.5380 - val_accuracy: 0.6815\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.2794e-08 - accuracy: 1.0000 - val_loss: 11.5602 - val_accuracy: 0.6815\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.2388e-08 - accuracy: 1.0000 - val_loss: 11.5823 - val_accuracy: 0.6815\n",
      "Validation accuracy: 0.6714285612106323\n",
      "\n",
      "Training with optimizer=SGD, init=glorot_uniform, batch_size=40, epochs=50\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_608 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_609 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_610 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_611 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_612 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_613 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_614 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_615 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_616 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_617 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_618 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_619 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_620 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_621 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_622 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_623 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 5s 25ms/step - loss: 0.6927 - accuracy: 0.5355 - val_loss: 0.6924 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6920 - accuracy: 0.5367 - val_loss: 0.6919 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6915 - accuracy: 0.5367 - val_loss: 0.6915 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6912 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6901 - accuracy: 0.5367 - val_loss: 0.6905 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6899 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6896 - accuracy: 0.5367 - val_loss: 0.6902 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6894 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6893 - accuracy: 0.5367 - val_loss: 0.6899 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6890 - accuracy: 0.5367 - val_loss: 0.6896 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6887 - accuracy: 0.5367 - val_loss: 0.6893 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6885 - accuracy: 0.5367 - val_loss: 0.6891 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6880 - accuracy: 0.5367 - val_loss: 0.6895 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6882 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6873 - accuracy: 0.5367 - val_loss: 0.6878 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6865 - accuracy: 0.5367 - val_loss: 0.6887 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6860 - accuracy: 0.5367 - val_loss: 0.6866 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6851 - accuracy: 0.5367 - val_loss: 0.6893 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6847 - accuracy: 0.5367 - val_loss: 0.6853 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6836 - accuracy: 0.5367 - val_loss: 0.6841 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6821 - accuracy: 0.5367 - val_loss: 0.6834 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6806 - accuracy: 0.5367 - val_loss: 0.6812 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6787 - accuracy: 0.5367 - val_loss: 0.6855 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6772 - accuracy: 0.5367 - val_loss: 0.6781 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6750 - accuracy: 0.5367 - val_loss: 0.6807 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6719 - accuracy: 0.5367 - val_loss: 0.6727 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6691 - accuracy: 0.5367 - val_loss: 0.6695 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6652 - accuracy: 0.5367 - val_loss: 0.6663 - val_accuracy: 0.5325\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6619 - accuracy: 0.6463 - val_loss: 0.6609 - val_accuracy: 0.6779\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6568 - accuracy: 0.6967 - val_loss: 0.6644 - val_accuracy: 0.7236\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6514 - accuracy: 0.7109 - val_loss: 0.6751 - val_accuracy: 0.6731\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6455 - accuracy: 0.7233 - val_loss: 0.6448 - val_accuracy: 0.7175\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6386 - accuracy: 0.7328 - val_loss: 0.6382 - val_accuracy: 0.7115\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6310 - accuracy: 0.7310 - val_loss: 0.6986 - val_accuracy: 0.5325\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6221 - accuracy: 0.7352 - val_loss: 0.6415 - val_accuracy: 0.6394\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6086 - accuracy: 0.7536 - val_loss: 0.6566 - val_accuracy: 0.5913\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6029 - accuracy: 0.7453 - val_loss: 0.6295 - val_accuracy: 0.7067\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5917 - accuracy: 0.7565 - val_loss: 0.5975 - val_accuracy: 0.7272\n",
      "Validation accuracy: 0.7460317611694336\n",
      "\n",
      "Training with optimizer=SGD, init=glorot_uniform, batch_size=40, epochs=100\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_624 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_625 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_626 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_627 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_628 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_629 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_630 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_631 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_632 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_633 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_634 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_635 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_636 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_637 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_638 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_639 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "43/43 [==============================] - 5s 29ms/step - loss: 0.6927 - accuracy: 0.5367 - val_loss: 0.6922 - val_accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6917 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6913 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6901 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6899 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6895 - accuracy: 0.5367 - val_loss: 0.6902 - val_accuracy: 0.5325\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6895 - accuracy: 0.5367 - val_loss: 0.6901 - val_accuracy: 0.5325\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6892 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6891 - accuracy: 0.5367 - val_loss: 0.6899 - val_accuracy: 0.5325\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6888 - accuracy: 0.5367 - val_loss: 0.6897 - val_accuracy: 0.5325\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6887 - accuracy: 0.5367 - val_loss: 0.6893 - val_accuracy: 0.5325\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6882 - accuracy: 0.5367 - val_loss: 0.6891 - val_accuracy: 0.5325\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6880 - accuracy: 0.5367 - val_loss: 0.6887 - val_accuracy: 0.5325\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6876 - accuracy: 0.5367 - val_loss: 0.6884 - val_accuracy: 0.5325\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6872 - accuracy: 0.5367 - val_loss: 0.6887 - val_accuracy: 0.5325\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6870 - accuracy: 0.5367 - val_loss: 0.6878 - val_accuracy: 0.5325\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6865 - accuracy: 0.5367 - val_loss: 0.6873 - val_accuracy: 0.5325\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6858 - accuracy: 0.5367 - val_loss: 0.6866 - val_accuracy: 0.5325\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6854 - accuracy: 0.5367 - val_loss: 0.6861 - val_accuracy: 0.5325\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6844 - accuracy: 0.5367 - val_loss: 0.6852 - val_accuracy: 0.5325\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6837 - accuracy: 0.5367 - val_loss: 0.6856 - val_accuracy: 0.5325\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6829 - accuracy: 0.5367 - val_loss: 0.6857 - val_accuracy: 0.5325\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6815 - accuracy: 0.5367 - val_loss: 0.6822 - val_accuracy: 0.5325\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6802 - accuracy: 0.5367 - val_loss: 0.6815 - val_accuracy: 0.5325\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6789 - accuracy: 0.5367 - val_loss: 0.6827 - val_accuracy: 0.5325\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6770 - accuracy: 0.5367 - val_loss: 0.6773 - val_accuracy: 0.5325\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6749 - accuracy: 0.5367 - val_loss: 0.6819 - val_accuracy: 0.5325\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6722 - accuracy: 0.5367 - val_loss: 0.6725 - val_accuracy: 0.5325\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6688 - accuracy: 0.5367 - val_loss: 0.6776 - val_accuracy: 0.5325\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6657 - accuracy: 0.5367 - val_loss: 0.6819 - val_accuracy: 0.7007\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6616 - accuracy: 0.5960 - val_loss: 0.6688 - val_accuracy: 0.7212\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6570 - accuracy: 0.6736 - val_loss: 0.6568 - val_accuracy: 0.6803\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6520 - accuracy: 0.6973 - val_loss: 0.6550 - val_accuracy: 0.7236\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6460 - accuracy: 0.7073 - val_loss: 0.6457 - val_accuracy: 0.7103\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6374 - accuracy: 0.7287 - val_loss: 0.6549 - val_accuracy: 0.6046\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6294 - accuracy: 0.7293 - val_loss: 0.6329 - val_accuracy: 0.7356\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6182 - accuracy: 0.7423 - val_loss: 0.6362 - val_accuracy: 0.6611\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6129 - accuracy: 0.7441 - val_loss: 0.6127 - val_accuracy: 0.7236\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6007 - accuracy: 0.7476 - val_loss: 0.6802 - val_accuracy: 0.5649\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.5952 - accuracy: 0.7399 - val_loss: 0.6398 - val_accuracy: 0.6430\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.5913 - accuracy: 0.7322 - val_loss: 0.5898 - val_accuracy: 0.7308\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.5676 - accuracy: 0.7565 - val_loss: 0.5791 - val_accuracy: 0.7308\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5634 - accuracy: 0.7512 - val_loss: 0.6442 - val_accuracy: 0.6466\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.5632 - accuracy: 0.7512 - val_loss: 0.5638 - val_accuracy: 0.7284\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.5503 - accuracy: 0.7541 - val_loss: 0.6001 - val_accuracy: 0.7031\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5479 - accuracy: 0.7476 - val_loss: 0.5932 - val_accuracy: 0.6983\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5266 - accuracy: 0.7707 - val_loss: 0.5535 - val_accuracy: 0.7404\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5282 - accuracy: 0.7642 - val_loss: 0.6265 - val_accuracy: 0.6466\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.5308 - accuracy: 0.7654 - val_loss: 0.6161 - val_accuracy: 0.6935\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5296 - accuracy: 0.7624 - val_loss: 0.6138 - val_accuracy: 0.6731\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5229 - accuracy: 0.7690 - val_loss: 0.5484 - val_accuracy: 0.7392\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.5226 - accuracy: 0.7713 - val_loss: 0.5741 - val_accuracy: 0.7344\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5115 - accuracy: 0.7767 - val_loss: 0.5753 - val_accuracy: 0.7188\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5143 - accuracy: 0.7778 - val_loss: 0.5554 - val_accuracy: 0.7344\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5182 - accuracy: 0.7725 - val_loss: 0.5822 - val_accuracy: 0.7320\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5035 - accuracy: 0.7802 - val_loss: 0.5871 - val_accuracy: 0.7103\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5016 - accuracy: 0.7861 - val_loss: 0.6015 - val_accuracy: 0.7163\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.5065 - accuracy: 0.7767 - val_loss: 0.6376 - val_accuracy: 0.6514\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4831 - accuracy: 0.7998 - val_loss: 0.5695 - val_accuracy: 0.7332\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.5150 - accuracy: 0.7719 - val_loss: 0.5516 - val_accuracy: 0.7452\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.4833 - accuracy: 0.8004 - val_loss: 0.7396 - val_accuracy: 0.6262\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.4985 - accuracy: 0.7838 - val_loss: 0.7798 - val_accuracy: 0.5986\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4969 - accuracy: 0.7873 - val_loss: 0.5841 - val_accuracy: 0.7200\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.4844 - accuracy: 0.7962 - val_loss: 0.6055 - val_accuracy: 0.7067\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.4741 - accuracy: 0.8033 - val_loss: 0.9224 - val_accuracy: 0.5589\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.5084 - accuracy: 0.7796 - val_loss: 0.5638 - val_accuracy: 0.7332\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4673 - accuracy: 0.8063 - val_loss: 0.6832 - val_accuracy: 0.6659\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.4762 - accuracy: 0.8015 - val_loss: 0.5571 - val_accuracy: 0.7416\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4464 - accuracy: 0.8276 - val_loss: 0.5631 - val_accuracy: 0.7308\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.5134 - accuracy: 0.7737 - val_loss: 0.5965 - val_accuracy: 0.7404\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4527 - accuracy: 0.8223 - val_loss: 0.5794 - val_accuracy: 0.7344\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4876 - accuracy: 0.7956 - val_loss: 0.5710 - val_accuracy: 0.7332\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.4755 - accuracy: 0.8021 - val_loss: 0.5771 - val_accuracy: 0.7308\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4538 - accuracy: 0.8164 - val_loss: 0.5839 - val_accuracy: 0.7368\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.4426 - accuracy: 0.8241 - val_loss: 0.5706 - val_accuracy: 0.7392\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4951 - accuracy: 0.7820 - val_loss: 0.6793 - val_accuracy: 0.6815\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.4553 - accuracy: 0.8164 - val_loss: 0.5722 - val_accuracy: 0.7320\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4641 - accuracy: 0.8092 - val_loss: 0.5713 - val_accuracy: 0.7356\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.4308 - accuracy: 0.8300 - val_loss: 0.6142 - val_accuracy: 0.7067\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4526 - accuracy: 0.8187 - val_loss: 0.5719 - val_accuracy: 0.7392\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4581 - accuracy: 0.8134 - val_loss: 0.5748 - val_accuracy: 0.7344\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4105 - accuracy: 0.8466 - val_loss: 0.7226 - val_accuracy: 0.6587\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4760 - accuracy: 0.8033 - val_loss: 0.5754 - val_accuracy: 0.7356\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4209 - accuracy: 0.8460 - val_loss: 0.6147 - val_accuracy: 0.7272\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4510 - accuracy: 0.8158 - val_loss: 0.5758 - val_accuracy: 0.7380\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.4472 - accuracy: 0.8175 - val_loss: 0.6644 - val_accuracy: 0.7043\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4515 - accuracy: 0.8128 - val_loss: 0.5735 - val_accuracy: 0.7320\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4647 - accuracy: 0.8098 - val_loss: 0.6936 - val_accuracy: 0.6430\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.4274 - accuracy: 0.8359 - val_loss: 0.6924 - val_accuracy: 0.6430\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.4039 - accuracy: 0.8507 - val_loss: 0.6977 - val_accuracy: 0.6526\n",
      "Validation accuracy: 0.6222222447395325\n",
      "\n",
      "Training with optimizer=SGD, init=glorot_uniform, batch_size=80, epochs=50\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_640 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_641 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_642 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_643 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_644 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_645 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_646 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_647 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_648 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_649 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_650 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_651 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_652 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_653 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_654 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_655 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 5s 35ms/step - loss: 0.6930 - accuracy: 0.5255 - val_loss: 0.6926 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6924 - accuracy: 0.5367 - val_loss: 0.6923 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6920 - accuracy: 0.5367 - val_loss: 0.6921 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6918 - accuracy: 0.5367 - val_loss: 0.6919 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.6916 - accuracy: 0.5367 - val_loss: 0.6918 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6915 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6913 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6901 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6900 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6899 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6899 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6897 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6896 - accuracy: 0.5367 - val_loss: 0.6905 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6895 - accuracy: 0.5367 - val_loss: 0.6902 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6894 - accuracy: 0.5367 - val_loss: 0.6901 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6893 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6892 - accuracy: 0.5367 - val_loss: 0.6901 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6892 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6890 - accuracy: 0.5367 - val_loss: 0.6896 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6889 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6887 - accuracy: 0.5367 - val_loss: 0.6894 - val_accuracy: 0.5325\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6886 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6885 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6883 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6881 - accuracy: 0.5367 - val_loss: 0.6893 - val_accuracy: 0.5325\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6879 - accuracy: 0.5367 - val_loss: 0.6892 - val_accuracy: 0.5325\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6886 - val_accuracy: 0.5325\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6875 - accuracy: 0.5367 - val_loss: 0.6883 - val_accuracy: 0.5325\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6872 - accuracy: 0.5367 - val_loss: 0.6881 - val_accuracy: 0.5325\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6871 - accuracy: 0.5367 - val_loss: 0.6878 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Training with optimizer=SGD, init=glorot_uniform, batch_size=80, epochs=100\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_656 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_657 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_658 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_659 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_660 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_661 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_662 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_663 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_664 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_665 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_666 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_667 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_668 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_669 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_670 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_671 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 4s 32ms/step - loss: 0.6930 - accuracy: 0.5071 - val_loss: 0.6929 - val_accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6927 - accuracy: 0.5367 - val_loss: 0.6925 - val_accuracy: 0.5325\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6923 - accuracy: 0.5367 - val_loss: 0.6923 - val_accuracy: 0.5325\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6921 - accuracy: 0.5367 - val_loss: 0.6921 - val_accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6918 - accuracy: 0.5367 - val_loss: 0.6918 - val_accuracy: 0.5325\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6915 - accuracy: 0.5367 - val_loss: 0.6917 - val_accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6914 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6912 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6909 - val_accuracy: 0.5325\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6901 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6901 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6900 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6900 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6900 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6899 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6898 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6897 - accuracy: 0.5367 - val_loss: 0.6905 - val_accuracy: 0.5325\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6897 - accuracy: 0.5367 - val_loss: 0.6903 - val_accuracy: 0.5325\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6896 - accuracy: 0.5367 - val_loss: 0.6905 - val_accuracy: 0.5325\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6896 - accuracy: 0.5367 - val_loss: 0.6904 - val_accuracy: 0.5325\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6895 - accuracy: 0.5367 - val_loss: 0.6901 - val_accuracy: 0.5325\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6894 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6893 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6891 - accuracy: 0.5367 - val_loss: 0.6905 - val_accuracy: 0.5325\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6891 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6889 - accuracy: 0.5367 - val_loss: 0.6896 - val_accuracy: 0.5325\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6887 - accuracy: 0.5367 - val_loss: 0.6894 - val_accuracy: 0.5325\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6886 - accuracy: 0.5367 - val_loss: 0.6893 - val_accuracy: 0.5325\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6883 - accuracy: 0.5367 - val_loss: 0.6906 - val_accuracy: 0.5325\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6883 - accuracy: 0.5367 - val_loss: 0.6890 - val_accuracy: 0.5325\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6880 - accuracy: 0.5367 - val_loss: 0.6887 - val_accuracy: 0.5325\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6877 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6880 - accuracy: 0.5367 - val_loss: 0.6883 - val_accuracy: 0.5325\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6871 - accuracy: 0.5367 - val_loss: 0.6886 - val_accuracy: 0.5325\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6868 - accuracy: 0.5367 - val_loss: 0.6877 - val_accuracy: 0.5325\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6863 - accuracy: 0.5367 - val_loss: 0.6884 - val_accuracy: 0.5325\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6860 - accuracy: 0.5367 - val_loss: 0.6881 - val_accuracy: 0.5325\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6857 - accuracy: 0.5367 - val_loss: 0.6881 - val_accuracy: 0.5325\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6851 - accuracy: 0.5367 - val_loss: 0.6890 - val_accuracy: 0.5325\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6848 - accuracy: 0.5367 - val_loss: 0.6864 - val_accuracy: 0.5325\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 1s 27ms/step - loss: 0.6839 - accuracy: 0.5367 - val_loss: 0.6860 - val_accuracy: 0.5325\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6836 - accuracy: 0.5367 - val_loss: 0.6846 - val_accuracy: 0.5325\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6830 - accuracy: 0.5367 - val_loss: 0.6869 - val_accuracy: 0.5325\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6825 - accuracy: 0.5367 - val_loss: 0.6874 - val_accuracy: 0.5325\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6822 - accuracy: 0.5367 - val_loss: 0.6844 - val_accuracy: 0.5325\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6815 - accuracy: 0.5367 - val_loss: 0.6827 - val_accuracy: 0.5325\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6808 - accuracy: 0.5367 - val_loss: 0.6853 - val_accuracy: 0.5325\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6799 - accuracy: 0.5367 - val_loss: 0.6885 - val_accuracy: 0.5325\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6795 - accuracy: 0.5367 - val_loss: 0.6800 - val_accuracy: 0.5325\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6784 - accuracy: 0.5367 - val_loss: 0.6824 - val_accuracy: 0.5325\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6776 - accuracy: 0.5367 - val_loss: 0.6797 - val_accuracy: 0.5325\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6756 - accuracy: 0.5367 - val_loss: 0.6803 - val_accuracy: 0.5325\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6749 - accuracy: 0.5367 - val_loss: 0.6769 - val_accuracy: 0.5325\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6731 - accuracy: 0.5367 - val_loss: 0.6747 - val_accuracy: 0.5325\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6718 - accuracy: 0.5367 - val_loss: 0.6737 - val_accuracy: 0.5325\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6706 - accuracy: 0.5367 - val_loss: 0.6738 - val_accuracy: 0.5325\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6688 - accuracy: 0.5367 - val_loss: 0.6704 - val_accuracy: 0.5325\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.6675 - accuracy: 0.5367 - val_loss: 0.6821 - val_accuracy: 0.5325\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6659 - accuracy: 0.5367 - val_loss: 0.6682 - val_accuracy: 0.5325\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6630 - accuracy: 0.5367 - val_loss: 0.6671 - val_accuracy: 0.5325\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6613 - accuracy: 0.5367 - val_loss: 0.6772 - val_accuracy: 0.5325\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6601 - accuracy: 0.5427 - val_loss: 0.6603 - val_accuracy: 0.5361\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6555 - accuracy: 0.6428 - val_loss: 0.6579 - val_accuracy: 0.6478\n",
      "Validation accuracy: 0.6634920835494995\n",
      "\n",
      "Training with optimizer=SGD, init=normal, batch_size=40, epochs=50\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_672 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_673 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_674 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_675 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_676 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_677 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_678 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_679 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_680 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_681 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_682 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_683 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_684 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_685 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_686 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_687 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "43/43 [==============================] - 5s 27ms/step - loss: 0.6929 - accuracy: 0.5332 - val_loss: 0.6926 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6924 - accuracy: 0.5367 - val_loss: 0.6922 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6919 - accuracy: 0.5367 - val_loss: 0.6919 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6915 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6913 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 1s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Training with optimizer=SGD, init=normal, batch_size=40, epochs=100\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_688 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_689 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_690 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_691 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_692 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_693 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_694 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_695 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_696 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_697 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_698 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_699 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_700 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_701 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_702 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_703 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "43/43 [==============================] - 5s 27ms/step - loss: 0.6928 - accuracy: 0.5344 - val_loss: 0.6923 - val_accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6920 - accuracy: 0.5367 - val_loss: 0.6919 - val_accuracy: 0.5325\n",
      "Epoch 3/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6916 - accuracy: 0.5367 - val_loss: 0.6917 - val_accuracy: 0.5325\n",
      "Epoch 4/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6913 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 6/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 8/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 9/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 10/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 11/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 12/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 13/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 14/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 15/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 16/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 18/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 19/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 20/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 21/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 22/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 23/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 24/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 25/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 26/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 27/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 28/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 29/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 30/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 31/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 32/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 33/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 34/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 35/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 36/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 37/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 38/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 39/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 40/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 41/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 42/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 43/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 44/100\n",
      "43/43 [==============================] - 0s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 45/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 46/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 47/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 48/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 49/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 50/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 51/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 52/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 53/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 54/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 55/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 56/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 57/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 58/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 59/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 60/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 61/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 62/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 63/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 64/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 65/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 66/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 67/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 68/100\n",
      "43/43 [==============================] - 1s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 69/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 70/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 71/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 72/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 73/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 74/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 75/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 76/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 77/100\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 78/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 79/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 80/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 81/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 82/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 83/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 84/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 85/100\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 86/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 87/100\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 88/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 89/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 90/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 91/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 92/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 93/100\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 94/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 95/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 96/100\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 97/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 98/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 99/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 100/100\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Training with optimizer=SGD, init=normal, batch_size=80, epochs=50\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_704 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_705 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_706 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_707 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_708 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_709 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_710 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_711 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_712 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_713 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_714 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_715 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_716 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_717 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_718 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_719 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 4s 32ms/step - loss: 0.6929 - accuracy: 0.5367 - val_loss: 0.6927 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6925 - accuracy: 0.5367 - val_loss: 0.6923 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6921 - accuracy: 0.5367 - val_loss: 0.6920 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6917 - accuracy: 0.5367 - val_loss: 0.6918 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6914 - accuracy: 0.5367 - val_loss: 0.6917 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6914 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6912 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Training with optimizer=SGD, init=normal, batch_size=80, epochs=100\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_720 (Dense)           (None, 256)               884992    \n",
      "                                                                 \n",
      " dense_721 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_722 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_723 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_724 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_725 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_726 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_727 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_728 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_729 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_730 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_731 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_732 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_733 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_734 (Dense)           (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_735 (Dense)           (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1132673 (4.32 MB)\n",
      "Trainable params: 1132673 (4.32 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 4s 34ms/step - loss: 0.6929 - accuracy: 0.5332 - val_loss: 0.6927 - val_accuracy: 0.5325\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6924 - accuracy: 0.5367 - val_loss: 0.6924 - val_accuracy: 0.5325\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6922 - accuracy: 0.5367 - val_loss: 0.6921 - val_accuracy: 0.5325\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6918 - accuracy: 0.5367 - val_loss: 0.6918 - val_accuracy: 0.5325\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6915 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6914 - accuracy: 0.5367 - val_loss: 0.6915 - val_accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6911 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6913 - val_accuracy: 0.5325\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6909 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 61/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 0s 14ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 0s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Validation accuracy: 0.5349206328392029\n",
      "\n",
      "Best accuracy: 0.7460317611694336 with parameters: {'optimizer': 'SGD', 'init': 'glorot_uniform', 'batch_size': 40, 'epochs': 50}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "def create_model(optimizer='adam', init='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer=init, input_shape=(3456,)))\n",
    "    for _ in range(14):  # Adding 14 more layers\n",
    "        model.add(Dense(128, activation='relu', kernel_initializer=init))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Hyperparameter grid\n",
    "optimizers = ['adam', 'SGD']\n",
    "initializers = ['glorot_uniform', 'normal']\n",
    "batch_sizes = [40 ,80]\n",
    "epochs_list = [50, 100]\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "# Manual hyperparameter tuning\n",
    "for optimizer in optimizers:\n",
    "    for init in initializers:\n",
    "        for batch_size in batch_sizes:\n",
    "            for epochs in epochs_list:\n",
    "                print(f'Training with optimizer={optimizer}, init={init}, batch_size={batch_size}, epochs={epochs}')\n",
    "                model = create_model(optimizer=optimizer, init=init)\n",
    "                print(model.summary())\n",
    "                history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.33, verbose=1)\n",
    "                loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "                print(f'Validation accuracy: {accuracy}')\n",
    "                print()\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    best_params = {\n",
    "                        'optimizer': optimizer,\n",
    "                        'init': init,\n",
    "                        'batch_size': batch_size,\n",
    "                        'epochs': epochs\n",
    "                    }\n",
    "\n",
    "print(f'Best accuracy: {best_accuracy} with parameters: {best_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46048b54-10f4-411f-9790-ca3734e63cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "43/43 [==============================] - 4s 22ms/step - loss: 0.6928 - accuracy: 0.5355 - val_loss: 0.6925 - val_accuracy: 0.5325\n",
      "Epoch 2/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6922 - accuracy: 0.5367 - val_loss: 0.6922 - val_accuracy: 0.5325\n",
      "Epoch 3/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6918 - accuracy: 0.5367 - val_loss: 0.6918 - val_accuracy: 0.5325\n",
      "Epoch 4/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6915 - accuracy: 0.5367 - val_loss: 0.6916 - val_accuracy: 0.5325\n",
      "Epoch 5/50\n",
      "43/43 [==============================] - 1s 17ms/step - loss: 0.6912 - accuracy: 0.5367 - val_loss: 0.6914 - val_accuracy: 0.5325\n",
      "Epoch 6/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6910 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 7/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6908 - accuracy: 0.5367 - val_loss: 0.6912 - val_accuracy: 0.5325\n",
      "Epoch 8/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 9/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6907 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 10/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 11/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6906 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 12/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6905 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 13/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6910 - val_accuracy: 0.5325\n",
      "Epoch 14/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6904 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 15/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6903 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 16/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6908 - val_accuracy: 0.5325\n",
      "Epoch 17/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6907 - val_accuracy: 0.5325\n",
      "Epoch 18/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6900 - accuracy: 0.5367 - val_loss: 0.6911 - val_accuracy: 0.5325\n",
      "Epoch 19/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6902 - accuracy: 0.5367 - val_loss: 0.6902 - val_accuracy: 0.5325\n",
      "Epoch 20/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6897 - accuracy: 0.5367 - val_loss: 0.6900 - val_accuracy: 0.5325\n",
      "Epoch 21/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6895 - accuracy: 0.5367 - val_loss: 0.6901 - val_accuracy: 0.5325\n",
      "Epoch 22/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6894 - accuracy: 0.5367 - val_loss: 0.6896 - val_accuracy: 0.5325\n",
      "Epoch 23/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6890 - accuracy: 0.5367 - val_loss: 0.6893 - val_accuracy: 0.5325\n",
      "Epoch 24/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6886 - accuracy: 0.5367 - val_loss: 0.6889 - val_accuracy: 0.5325\n",
      "Epoch 25/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6884 - accuracy: 0.5367 - val_loss: 0.6889 - val_accuracy: 0.5325\n",
      "Epoch 26/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6879 - accuracy: 0.5367 - val_loss: 0.6883 - val_accuracy: 0.5325\n",
      "Epoch 27/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6875 - accuracy: 0.5367 - val_loss: 0.6882 - val_accuracy: 0.5325\n",
      "Epoch 28/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6869 - accuracy: 0.5367 - val_loss: 0.6877 - val_accuracy: 0.5325\n",
      "Epoch 29/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6864 - accuracy: 0.5367 - val_loss: 0.6865 - val_accuracy: 0.5325\n",
      "Epoch 30/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6855 - accuracy: 0.5367 - val_loss: 0.6878 - val_accuracy: 0.5325\n",
      "Epoch 31/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6848 - accuracy: 0.5367 - val_loss: 0.6864 - val_accuracy: 0.5325\n",
      "Epoch 32/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6839 - accuracy: 0.5367 - val_loss: 0.6859 - val_accuracy: 0.5325\n",
      "Epoch 33/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6832 - accuracy: 0.5367 - val_loss: 0.6831 - val_accuracy: 0.5325\n",
      "Epoch 34/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6820 - accuracy: 0.5367 - val_loss: 0.6835 - val_accuracy: 0.5325\n",
      "Epoch 35/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6803 - accuracy: 0.5367 - val_loss: 0.6848 - val_accuracy: 0.5325\n",
      "Epoch 36/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6788 - accuracy: 0.5367 - val_loss: 0.6793 - val_accuracy: 0.5325\n",
      "Epoch 37/50\n",
      "43/43 [==============================] - 1s 12ms/step - loss: 0.6775 - accuracy: 0.5367 - val_loss: 0.6787 - val_accuracy: 0.5325\n",
      "Epoch 38/50\n",
      "43/43 [==============================] - 1s 13ms/step - loss: 0.6754 - accuracy: 0.5367 - val_loss: 0.6768 - val_accuracy: 0.5325\n",
      "Epoch 39/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6726 - accuracy: 0.5367 - val_loss: 0.6743 - val_accuracy: 0.5325\n",
      "Epoch 40/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6703 - accuracy: 0.5628 - val_loss: 0.6696 - val_accuracy: 0.6178\n",
      "Epoch 41/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6670 - accuracy: 0.6351 - val_loss: 0.6663 - val_accuracy: 0.6839\n",
      "Epoch 42/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6634 - accuracy: 0.6813 - val_loss: 0.6749 - val_accuracy: 0.5349\n",
      "Epoch 43/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6597 - accuracy: 0.7044 - val_loss: 0.6603 - val_accuracy: 0.7200\n",
      "Epoch 44/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6551 - accuracy: 0.7145 - val_loss: 0.6731 - val_accuracy: 0.5445\n",
      "Epoch 45/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6478 - accuracy: 0.7293 - val_loss: 0.6485 - val_accuracy: 0.7175\n",
      "Epoch 46/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6416 - accuracy: 0.7387 - val_loss: 0.6462 - val_accuracy: 0.7308\n",
      "Epoch 47/50\n",
      "43/43 [==============================] - 1s 16ms/step - loss: 0.6338 - accuracy: 0.7376 - val_loss: 0.6722 - val_accuracy: 0.6034\n",
      "Epoch 48/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6252 - accuracy: 0.7464 - val_loss: 0.6297 - val_accuracy: 0.7043\n",
      "Epoch 49/50\n",
      "43/43 [==============================] - 1s 15ms/step - loss: 0.6162 - accuracy: 0.7464 - val_loss: 0.6888 - val_accuracy: 0.5325\n",
      "Epoch 50/50\n",
      "43/43 [==============================] - 1s 14ms/step - loss: 0.6078 - accuracy: 0.7482 - val_loss: 0.6160 - val_accuracy: 0.7248\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/IAAAGJCAYAAAApGAgTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAACsr0lEQVR4nOzdeXwTdfoH8M8kTdL7vkuh3PclNyheICIieCKiKCLuYlEE3Z+yLHiDt67HyoqiuKIgeKEgCBVEBERAUO6blkJbSu8rSZP5/TGZydGkSXolLZ/369Vf0snMZFL25zfPPM/3+QqiKIogIiIiIiIiomZB5esLICIiIiIiIiLPMZAnIiIiIiIiakYYyBMRERERERE1IwzkiYiIiIiIiJoRBvJEREREREREzQgDeSIiIiIiIqJmhIE8ERERERERUTPCQJ6IiIiIiIioGWEgT0RERERERNSMMJAnasEEQcDTTz/t9XGnT5+GIAj4+OOPG/yaiIiIyP819neIzZs3QxAEbN68uU7XR3SpYyBP1Mg+/vhjCIIAQRCwdevWGq+LoojU1FQIgoAbb7zRB1dIRERE/ojfIYjIFQbyRE0kMDAQn332WY3tP//8M86ePQudTueDqyIiIiJ/x+8QROSIgTxRE7nhhhuwcuVKVFdX223/7LPP0K9fPyQmJvroyi4d5eXlvr4EIiIir/E7BBE5YiBP1EQmTpyIixcvYsOGDco2g8GAVatW4a677nJ6THl5OR577DGkpqZCp9Ohc+fOePXVVyGKot1+er0es2bNQlxcHMLCwnDTTTfh7NmzTs+ZnZ2N+++/HwkJCdDpdOjevTuWLFlSp89UUFCAxx9/HD179kRoaCjCw8MxevRo7Nu3r8a+VVVVePrpp9GpUycEBgYiKSkJt9xyC06cOKHsYzab8e9//xs9e/ZEYGAg4uLicP3112PXrl0Aap935ziX7+mnn4YgCDh48CDuuusuREVF4fLLLwcA/Pnnn7jvvvvQrl07BAYGIjExEffffz8uXrzo9O81depUJCcnQ6fToW3btpg+fToMBgNOnjwJQRDwxhtv1Dhu27ZtEAQBn3/+ubd/ViIiIjst8TuEKytXrkS/fv0QFBSE2NhY3H333cjOzrbbJycnB1OmTEGrVq2g0+mQlJSEcePG4fTp08o+u3btwqhRoxAbG4ugoCC0bdsW999/f4NeK5EvBfj6AoguFWlpaRgyZAg+//xzjB49GgDwww8/oLi4GHfeeSfeeustu/1FUcRNN92ETZs2YerUqejTpw/Wr1+Pf/zjH8jOzrYLHh944AF8+umnuOuuuzB06FD89NNPGDNmTI1ryM3NxeDBgyEIAmbMmIG4uDj88MMPmDp1KkpKSvDoo4969ZlOnjyJb775Brfffjvatm2L3Nxc/Pe//8WVV16JgwcPIjk5GQBgMplw4403IiMjA3feeSdmzpyJ0tJSbNiwAfv370f79u0BAFOnTsXHH3+M0aNH44EHHkB1dTV++eUX7NixA/379/fq2mS33347OnbsiAULFihfXjZs2ICTJ09iypQpSExMxIEDB/D+++/jwIED2LFjBwRBAACcO3cOAwcORFFRER588EF06dIF2dnZWLVqFSoqKtCuXTsMGzYMy5Ytw6xZs+zed9myZQgLC8O4cePqdN1ERESylvgdwpmPP/4YU6ZMwYABA7Bw4ULk5ubi3//+N3799Vf88ccfiIyMBADceuutOHDgAB5++GGkpaUhLy8PGzZsQGZmpvL7ddddh7i4ODz55JOIjIzE6dOn8dVXX9X7Gon8hkhEjeqjjz4SAYi///67+M4774hhYWFiRUWFKIqiePvtt4tXX321KIqi2KZNG3HMmDHKcd98840IQHz++eftznfbbbeJgiCIx48fF0VRFPfu3SsCEB966CG7/e666y4RgPjUU08p26ZOnSomJSWJ+fn5dvveeeedYkREhHJdp06dEgGIH330Ua2fraqqSjSZTHbbTp06Jep0OvHZZ59Vti1ZskQEIL7++us1zmE2m0VRFMWffvpJBCA+8sgjLvep7bocP+tTTz0lAhAnTpxYY1/5c9r6/PPPRQDili1blG2TJ08WVSqV+Pvvv7u8pv/+978iAPHQoUPKawaDQYyNjRXvvffeGscRERF5qiV/h9i0aZMIQNy0aZMoitLYGR8fL/bo0UOsrKxU9vv+++9FAOL8+fNFURTFwsJCEYD4yiuvuDz3119/rfzdiFoqltYTNaE77rgDlZWV+P7771FaWorvv//eZUnc2rVroVar8cgjj9htf+yxxyCKIn744QdlPwA19nO8My6KIr788kuMHTsWoigiPz9f+Rk1ahSKi4uxZ88erz6PTqeDSiX9Z8RkMuHixYsIDQ1F586d7c715ZdfIjY2Fg8//HCNc8jZ7y+//BKCIOCpp55yuU9d/P3vf6+xLSgoSHleVVWF/Px8DB48GACU6zabzfjmm28wduxYp9UA8jXdcccdCAwMxLJly5TX1q9fj/z8fNx99911vm4iIiJbLe07hKNdu3YhLy8PDz30EAIDA5XtY8aMQZcuXbBmzRoA0hiu1WqxefNmFBYWOj2XnLn//vvvYTQa63VdRP6KgTxRE4qLi8OIESPw2Wef4auvvoLJZMJtt93mdN8zZ84gOTkZYWFhdtu7du2qvC4/qlQqpTxd1rlzZ7vfL1y4gKKiIrz//vuIi4uz+5kyZQoAIC8vz6vPYzab8cYbb6Bjx47Q6XSIjY1FXFwc/vzzTxQXFyv7nThxAp07d0ZAgOvZPCdOnEBycjKio6O9ugZ32rZtW2NbQUEBZs6ciYSEBAQFBSEuLk7ZT77uCxcuoKSkBD169Kj1/JGRkRg7dqxdN+Fly5YhJSUF11xzTQN+EiIiupS1tO8Qzq7Z2XsDQJcuXZTXdTodXnrpJfzwww9ISEjA8OHD8fLLLyMnJ0fZ/8orr8Stt96KZ555BrGxsRg3bhw++ugj6PX6el0jkT/hHHmiJnbXXXdh2rRpyMnJwejRo5W7xo3NbDYDAO6++27ce++9Tvfp1auXV+dcsGAB5s2bh/vvvx/PPfccoqOjoVKp8Oijjyrv15BcZeZNJpPLY2yz77I77rgD27Ztwz/+8Q/06dMHoaGhMJvNuP766+t03ZMnT8bKlSuxbds29OzZE6tXr8ZDDz2kVCsQERE1hJb0HaI+Hn30UYwdOxbffPMN1q9fj3nz5mHhwoX46aef0LdvXwiCgFWrVmHHjh347rvvsH79etx///147bXXsGPHDoSGhjbZtRI1FgbyRE3s5ptvxt/+9jfs2LEDK1ascLlfmzZtsHHjRpSWltrdUT98+LDyuvxoNpuVrLfsyJEjdueTu9GaTCaMGDGiQT7LqlWrcPXVV+PDDz+0215UVITY2Fjl9/bt2+O3336D0WiERqNxeq727dtj/fr1KCgocJmVj4qKUs5vS75L74nCwkJkZGTgmWeewfz585Xtx44ds9svLi4O4eHh2L9/v9tzXn/99YiLi8OyZcswaNAgVFRU4J577vH4moiIiDzRkr5DOLtm+b0dK9qOHDmivC5r3749HnvsMTz22GM4duwY+vTpg9deew2ffvqpss/gwYMxePBgvPDCC/jss88wadIkLF++HA888ECjfAaipsR0EVETCw0NxXvvvYenn34aY8eOdbnfDTfcAJPJhHfeecdu+xtvvAFBEJSutfKjY8faN9980+53tVqNW2+9FV9++aXT4PTChQtefxa1Wl1jGZuVK1fWWCbm1ltvRX5+fo3PAkA5/tZbb4UoinjmmWdc7hMeHo7Y2Fhs2bLF7vX//Oc/Xl2z7Tlljn8vlUqF8ePH47vvvlOWv3N2TQAQEBCAiRMn4osvvsDHH3+Mnj17NmlmgoiILg0t6TuEo/79+yM+Ph6LFi2yK4H/4YcfcOjQIaWTfkVFBaqqquyObd++PcLCwpTjCgsLa4zzffr0AQCW11OLwYw8kQ+4KkuzNXbsWFx99dWYO3cuTp8+jd69e+PHH3/Et99+i0cffVSZz9anTx9MnDgR//nPf1BcXIyhQ4ciIyMDx48fr3HOF198EZs2bcKgQYMwbdo0dOvWDQUFBdizZw82btyIgoICrz7HjTfeiGeffRZTpkzB0KFD8ddff2HZsmVo166d3X6TJ0/GJ598gtmzZ2Pnzp244oorUF5ejo0bN+Khhx7CuHHjcPXVV+Oee+7BW2+9hWPHjill7r/88guuvvpqzJgxA4C0TM6LL76IBx54AP3798eWLVtw9OhRj685PDxcmU9nNBqRkpKCH3/8EadOnaqx74IFC/Djjz/iyiuvxIMPPoiuXbvi/PnzWLlyJbZu3WpX0jh58mS89dZb2LRpE1566SWv/o5ERESeainfIRxpNBq89NJLmDJlCq688kpMnDhRWX4uLS1NWeb16NGjuPbaa3HHHXegW7duCAgIwNdff43c3FzceeedAIClS5fiP//5D26++Wa0b98epaWlWLx4McLDw3HDDTfU6zqJ/IZPeuUTXUJsl46pjePSMaIoiqWlpeKsWbPE5ORkUaPRiB07dhRfeeUVZekzWWVlpfjII4+IMTExYkhIiDh27FgxKyurxtIxoiiKubm5Ynp6upiamipqNBoxMTFRvPbaa8X3339f2ceb5ecee+wxMSkpSQwKChKHDRsmbt++XbzyyivFK6+80m7fiooKce7cuWLbtm2V973tttvEEydOKPtUV1eLr7zyitilSxdRq9WKcXFx4ujRo8Xdu3fbnWfq1KliRESEGBYWJt5xxx1iXl6ey+XnLly4UOO6z549K958881iZGSkGBERId5+++3iuXPnnP69zpw5I06ePFmMi4sTdTqd2K5dOzE9PV3U6/U1ztu9e3dRpVKJZ8+erfXvRkRE5ImW/B3Ccfk52YoVK8S+ffuKOp1OjI6OFidNmmQ3rubn54vp6elily5dxJCQEDEiIkIcNGiQ+MUXXyj77NmzR5w4caLYunVrUafTifHx8eKNN94o7tq1q9ZrImpOBFF0qDshIqI66du3L6Kjo5GRkeHrSyEiIiKiFoxz5ImIGsCuXbuwd+9eTJ482deXQkREREQtHDPyRET1sH//fuzevRuvvfYa8vPzcfLkSQQGBvr6soiIiIioBWNGnoioHlatWoUpU6bAaDTi888/ZxBPRERERI2OGXkiIiIiIiKiZoQZeSIiIiIiIqJmhIE8ERERERERUTMS4OsL8Edmsxnnzp1DWFgYBEHw9eUQERFBFEWUlpYiOTkZKhXvw9cXx3oiIvI33oz1DOSdOHfuHFJTU319GURERDVkZWWhVatWvr6MZo9jPRER+StPxnoG8k6EhYUBkP6A4eHhPr4aIiIioKSkBKmpqcoYRfXDsZ6IiPyNN2M9A3kn5BK78PBwDu5ERORXWAbeMDjWExGRv/JkrOckOyIiIiIiIqJmhIE8ERERERERUTPCQJ6IiIiIiIioGeEc+ToSRRHV1dUwmUy+vhRqAGq1GgEBAZx7SkRECo71LQvHeiJqSRjI14HBYMD58+dRUVHh60uhBhQcHIykpCRotVpfXwoREfkYx/qWiWM9EbUUfhHIv/vuu3jllVeQk5OD3r174+2338bAgQOd7nvVVVfh559/rrH9hhtuwJo1awAA9913H5YuXWr3+qhRo7Bu3bp6X6vZbMapU6egVquRnJwMrVbLO7vNnCiKMBgMuHDhAk6dOoWOHTtCpeKsEyKiSxXH+paHYz0RtTQ+D+RXrFiB2bNnY9GiRRg0aBDefPNNjBo1CkeOHEF8fHyN/b/66isYDAbl94sXL6J37964/fbb7fa7/vrr8dFHHym/63S6Brleg8EAs9mM1NRUBAcHN8g5yfeCgoKg0Whw5swZGAwGBAYG+vqSiIjIRzjWt0wc64moJfF5IP/6669j2rRpmDJlCgBg0aJFWLNmDZYsWYInn3yyxv7R0dF2vy9fvhzBwcE1AnmdTofExESPrkGv10Ov1yu/l5SUuD2Gd3FbHv6bEhGRLY4LLQ//TYmopfDpf80MBgN2796NESNGKNtUKhVGjBiB7du3e3SODz/8EHfeeSdCQkLstm/evBnx8fHo3Lkzpk+fjosXL7o8x8KFCxEREaH8pKam1u0DERERERERETUynwby+fn5MJlMSEhIsNuekJCAnJwct8fv3LkT+/fvxwMPPGC3/frrr8cnn3yCjIwMvPTSS/j5558xevRol11n58yZg+LiYuUnKyur7h+KiIgIQGG5Ad/8kY1D591XeVHzVmU0obDCAGO12deXQkRElwifl9bXx4cffoiePXvWaIx35513Ks979uyJXr16oX379ti8eTOuvfbaGufR6XQNNof+UpKWloZHH30Ujz76qK8vhYjI50RRxOGcUvx0OA+bDudhT2YhzCJw/7C2mD+2m68vjxpRWVkZysrLcQEBEAJ0CNZpEKoLQIguAAHq5l/KzfGeiMj/+DSQj42NhVqtRm5urt323Nxct/Pby8vLsXz5cjz77LNu36ddu3aIjY3F8ePHnQbyLZ27TrtPPfUUnn76aa/P+/vvv9eY0kBE1JKIogiz6Pp1fbUJ209cVIL3c8VVdq93SQxDanRQI18l+VqwuRSxqgvSL2bAWKGGsSIAZQiASaWFWqODRhuEAF0gtFpdo3XA53hPRHTp8Gkgr9Vq0a9fP2RkZGD8+PEApCVfMjIyMGPGjFqPXblyJfR6Pe6++26373P27FlcvHgRSUlJDXHZzc758+eV5ytWrMD8+fNx5MgRZVtoaKjyXBRFmEwmBAS4/59GXFxcw14oEZEfOVdUidsXbUd2UaXHxwRqVBjWPhZXd4nH1V3ikRLJIP5SEByoA8zBEKv1EEQTNIIJGpgQDD0glgMGSD9lgElUwShoUK3SQlTrIGgCodYEQqMLglqtrleQz/GeiOjS4fN6r9mzZ2Px4sVYunQpDh06hOnTp6O8vFzpYj958mTMmTOnxnEffvghxo8fj5iYGLvtZWVl+Mc//oEdO3bg9OnTyMjIwLhx49ChQweMGjWqUT6DKIqoMFQ3+Y8o1pImspGYmKj8REREQBAE5ffDhw8jLCwMP/zwA/r16wedToetW7fixIkTGDduHBISEhAaGooBAwZg48aNdudNS0vDm2++qfwuCAI++OAD3HzzzQgODkbHjh2xevXqhvxTExE1CVEU8X+r/vQoiE+JDMI9g9vgo/sGYO/86/DhfQNw9+A2DOJbmFrHek0UKiLaozKmGyqiu6IivAMqQlqjVJuAQkSiwBSEIqMa5QYz9NXVMBsrodIXQ12RB1VxJsT8ozBk70NZ5l4UnvkLBVmHcDH7OApyzqA4/zxMFYWAoRyo1gNmE+Bi/Od4T0R06fD5HPkJEybgwoULmD9/PnJyctCnTx+sW7dOaYCXmZlZY6mQI0eOYOvWrfjxxx9rnE+tVuPPP//E0qVLUVRUhOTkZFx33XV47rnnGm0efKXRhG7z1zfKuWtz8NlRCNY2zD/hk08+iVdffRXt2rVDVFQUsrKycMMNN+CFF16ATqfDJ598grFjx+LIkSNo3bq1y/M888wzePnll/HKK6/g7bffxqRJk3DmzJkaywYSEfmzT3/LxNbj+dAFqPDl9KFoFeU8KBcgIDwooNFKpcl/+GqsB4CD0xMRrLF+FxIhwCyoIaoCIKgCAFUAVOoACCo1oFIDgloK/AHpUVADJiMA4Mknn8CrL7+Mdu07ICo6muM9EVEz5fNAHgBmzJjhspR+8+bNNbZ17tzZZTY6KCgI69f7ZqBtzp599lmMHDlS+T06Ohq9e/dWfn/uuefw9ddfY/Xq1bVOe7jvvvswceJEAMCCBQvw1ltvYefOnbj++usb7+KJiBrQmYvlWLDmEADgieu7oEdKhI+viC51FaIOAaKIAJigEkQIEKEWqwFTNeB8QR6gPB8QTUD+Uen3wtMAgGcfnYKRPRMAlAJVpYiOE9D7piGAIALQ47kZd+LrlcuxetlizHjgbgACYK4Gyi4AF08op79vwnhMHDUEEIAF//i7NN7/tAbXj7xKOgaC9AABEOTfVdbfa2yTz+xwU8zuJpngfLugkm5WqCyPvryxVq0H8o8BeYekv31kKtB1LBAU5btrIqIWyS8C+eYuSKPGwWcbp2zf3fs2lP79+9v9XlZWhqeffhpr1qzB+fPnUV1djcrKSmRmZtZ6nl69einPQ0JCEB4ejry8vAa7TiKixmQyi3h85T5UGk0Y3C4a9w1N8/UlkZ/w1VgviiK0ahWMZhGV1WZUm0wwVRthNhkhmqohmqshmKuhghlqm58qaAAIMIgBUMMMsygFt/172a+gUFZejqdf+y/WZPyC83n5qK42obJKj8ysTMBYYbkIM2DSA3rrUoq9OqYCVYUAgBAVEB4Wirzs09INBJ9TWYJ6S2Bvy2gGSnOB5f8CqizfT1RqwFLZIN0QUFu3qbVAYIT0owu3PA+3/l5VJAXteQeBvMPAxePSDRRb388GOl4H9LwN6Dwa0HDaDRHVHwP5BiAIQoOVuPuKYzfaxx9/HBs2bMCrr76KDh06ICgoCLfddhsMBkOt59FoNHa/C4IAs5nr6hJR87Bk6yn8froQIVo1XrmtN1QqlsyTxNdjvQYAtPJv9oGgKIowmUVUm0VUm8yoNoswBsVBhAp5Qe1RbRZxPuAcACAruAsKzWGWnLmIZ599DNu2bMET/3oabdLSEBQYiIf//gAuGLQ4bU6AABEmqFEohiJLjIMAqSKyJCAK58QYSIX+AAQBheYg5IqRyj6C5TX7RxEqy6P1x0qAaJegr/EoiDb7Wh5FM1QwK+8LmAFX3z1MImAyAPlHgLIsD/7ydaCLAOK7ArEdgHN7gdz9wJE10o82VMrQ97wNaHsVoG7e3x+JyHf4Xw9y6tdff8V9992Hm2++GYCUoT99+rRvL4qIqBEdyy3FKz9KHb7n3dgNqdHBPr4iIs8IgoAAtYAANQBLtV5YoAaCALSy/O/4tKXPQ5ekCERERCiB/19/7MI9996HSZPuglkESktLkZ19FhqtDoFh0RAhAoIKgjYYCIqGyTK10RgQikpNNMwQIYqACBUqhBAUqGKk30URZsDlVEjB5v+KcNjHs16+Ts4pQg0zVBChgtlSpWB/MpPZiHzRgEXGaSg0lEMAoIIZATBBqxIRphUQrhMQphUQqhUQIlRDayqFrroMOlMZgsxlCDSVI9hchiBzOQyqQFwIaoeSsA7QR3eGENcVoXGtERsWiNgwLUxmERVZ+xF4+EvEnlqN4MpzwL7PgX2fo0QdhYLQjtBFJiIirhWCo5KA0HjpJyQeCImVGhuajVKPA3O15dHyu6AGYtoDQZF1+4MRUbPGQJ6c6tixI7766iuMHTsWgiBg3rx5zKwTUYtlNJnx2Mp9MFSbcVXnOEwYkOrrSyJqNLaBf+dOnbD2u29x+y3jlfFeNJsRrA1AYkQgAECtEhAZrLW7uZUYEYT28dbl7FQCkBQZhK5J4XbvZRvIu2oKKYrSzQCzKMJseRTtnjt5hIvtDueqtry/aPk/JtEAk1qHc+F9cF5lgqHajKJKIwzVZsAMoBpAhZd/0FIAeQBOwPLE2ZTC4QCuQD/hKMapt2GMegdiTIUIL94JFAM44+V72gpLBuK7AHFdpUqA+K5AXGdAF1aPkxKRv2MgT069/vrruP/++zF06FDExsbiiSeeQElJifsDiYiaofc2n8CfZ4sREaTBS7f2Yhd6umQ09njvyf8vCYIAQQBUjo3uGkFVVRVQFoj/Te2KwEDpRoUoiqgymlFYYUBRhRFFFQYUVRpRWGFAldEMjVpAgEqFALWgPJcfK40mXCjV40KZHvmWxwuleuSX6ZFfZoBaJSA6WIuoEC2iQzSIDE7GseDr8L9AAe31+1Gedxql+dkQyi8gRihGHIoQK5QgTihCjFAKE1QwQQ2TEACzEACzSgNRFQBRpYFGNCJYnweUnpN+Tvxk91nN4a2gimkvZe2jbR6j0oAArZO/DhE1J4Lo6WLkl5CSkhJERESguLgY4eH2d5arqqpw6tQptG3bVhkAqGXgvy3RpWl/djHGv/srqs0i/n1nH4zrk+LrS3KqtrGJvMex/tLUlP+28ldsT25mlFQZ8WdWMf7ILMTerCL8kVWEgnI9anTxdxCGCnQUzqKT6iw6CWfRUTiLzqqziBeKXF+XoAIiUiHEdADkn1jLY3grqVEgEfmEN2M9M/JERNQiiaKI9M/2YNPhC+icGIYeKeHomRKB7skR6JQQBm2ACvpqEx77Yh+qzSJG90jETb2TfX3ZRNRCeFPZEx6oweUdY3F5x1gA0n+/ckqqUFBuQHGFEYUVRhRVWisGCiuMKK40ospoQoUhBfsM/fGb0YQKQzUqDCbojMVobc5GWyEHaaoc6dHyE4oqoOiM9HMiw/5CAgKlrH1sByA4xqabv8r6XP6JaQ+0GQqEJTbkn43I/1QVSz0pdKHu921CDOSJiKhFWrX7LNb+lQMA2JtVhL1ZRcprGrWAzolhCNYE4EhuKWJCtHh+fA+W1BORXxAEAUkRQUiKqPtSdcUVRpzML8OJC+U4eKEM310ow8m8MpQVnEMr83m0VZ1HOyEH7YRzaCvkoI2QA211FZB3QPrxVFRbKaBvPRhoPVQK8PnfUmopqg3AOwMATTDwyB9+9b9tBvJERNTiXCjV4/k1hwAAD13VHl2SwnEguxj7zxVjf3YJiiuN2J9tnQe84JaeiAnV+epyiYgaXESwBn1bR6Fv6yi77dUmM7IKK3EkpwR/ZRdjWXYJ9mcXo6i8EilCPtoJ59BOyEEYKqAWTFDDDLWls78aZqm7v7oavdSn0ab6FITCU0DhKWDvMukNQuKB1IFARCsgKAoIjJQegyKtz4NjgJCYpv6TEHmv4iJQlis9r9YDGv+ZbsVAnoiIWpxnvz+I4kojuieHY/bITghQq5SyeVEUcbawEvuzi3HgXAlSo4MwqjtLQ4no0hCgVqFtbAjaxobg+h5JAKyl/H+dLcb+cyU4kF2MvFI9yvTVKK2qRpneiCpjzdWLwlCBfqqjGKA6jMu1x9FNPAZNeR5w+Hv3FxKaACT1AZJ6Sz/JfYDwFL/KeBLBaLOMRXUlA3kiIqLGsulwHr7bdw4qAXjxll4IUNs3bhIEAanRwUiNDsbonkk+ukoiIv9hW8p/nYsbm0aTGeWWwL640ogD54rxR2YR9mTG49W8PnilAtDBgJ7CSfRWnUSMUIJWgQYk66oQG1CJSKEMweYyaAxFEKqKpSznsfXSjyw4RgrqUwcB/acCoXHefZDz+4BNCwBNEDDiaalDP1F92AXyet9dhxMM5ImIqMUo11fjX9/sBwBMvbwteraK8PEVERG1DBq1CpHBWkQGa5EKoEdKBCYMaA1A6rq/L6sIe84UYU9mCr482xNFFUagDNKPjQCVgG6xalwfm4/BQWfR0XwCoQX7IeQdlsqYT/wk/Wx7GxiSDgyZAQS6WamjNAfIeM5S3m9ZkOvID8AVjwHDZgIBnDpFdWSsdP7cDzCQJyKiFuPVH48gu6gSraKCMGtkJ19fDhHRJSE8UIMrOsbhio5SBl0URVwo1eNIbimO5pbhWG4pjuSW4lhuGcr01fgzrxp/5kUCiATQA7Ght2NQWghGxFxEP+1ptDq1Cqrze4GfXwJ2LpYC8gEP1CxrNlYC298FfnkdMJZL27rfApRfAE7/Amx6Adi3HLjhFaDDtU33B6GWgxl5IiKixvVHZiE+3nYaALDg5p4I1nKIIyLyBUEQEB8eiPjwQCW4B6QA/3xxFQ6cK8GuMwXYfboQf54tRn6ZAWsOGbAGKgDtEKh5Ag/GHsB9Vf9DdOUZ4Me5wI7/AFc9CfS+C1CpgQNfARueAoqzpJOn9AeuXyg12hNFYP+XwPp/AgUngE9vAbqNA0YtBCJSfPNHoebJNgtfXeW763CC33KIiKjZM5rMmPPVXxBF4Oa+KRjeyct5lURE1OgEQUByZBCSI4MwslsCAKDKaML+7GLsOlOIXacLsftMAQorjHjrfDe8i+dxq3oLZgV8iaSSbGD1wyjb9Dp0YbHQnPtdOml4ijQfvsdtgEolvxHQ8zag40hg00Jg53+Bg98CxzZKNwMGTwfUGt/8Eah5MZRbn/tZIK9yvwuR5KqrrsKjjz6q/J6WloY333yz1mMEQcA333xT7/duqPMQUcv0/paTOJxTiugQLebd2M3Xl0PUbHGsp6YWqFGjf1o0/n5le3xwb3/smTcSG2cPx8JbemJc39bYFn4DrtK/jueMk1AghiK09BQ0536HXgjEoS4Po/zBHUCvO6xBvN3JI4DRLwJ/2yI10DOWAxvmAS+2AT4aA2yYDxz6Dig53/QfnJoHZuTJ18aOHQuj0Yh169bVeO2XX37B8OHDsW/fPvTq1cvjc/7+++8ICQlpyMvE008/jW+++QZ79+61237+/HlERUU5P4iILmknL5Th3xnHAADzbuyK6BCt9cUjPwA5fwHD/8EljajF41hPLYEgCOgQH4YO8WGYOFBqpne+uBI7Tw3EuycmI/XoJxDL87Goeixy90Yj6MA2XNc9AeP7puCKDrE1VioBACT2BKasA/Z9Bmx8BijPA85slX5k4SlAq/5AqwFAn0lAcHQTfWLya3aBPOfIkw9MnToVt956K86ePYtWrVrZvfbRRx+hf//+Xg3sABAX13Slq4mJXOOZiGoSRRH//PovGKrNGN4pDuP72Mx9LL8IrJwirfva8TppjWKiFoxjPbVUSRFBGNcnBeP6pAAYjFP55Sj6Ixvf7s3G6YsV+HbvOXy79xxiQrS4sVcSbuydjMtaR0GtsrmBq1IBfe8Gek8E8o8CZ3cB2bukx7yDQEk2cDBbKsH/41Pg/vVAUKSvPjL5C6NNab2fda1naX1DEEVp/kRT/4iix5d44403Ii4uDh9//LHd9rKyMqxcuRLjx4/HxIkTkZKSguDgYPTs2ROff/55red0LLc7duwYhg8fjsDAQHTr1g0bNmyoccwTTzyBTp06ITg4GO3atcO8efNgNBoBAB9//DGeeeYZ7Nu3D4IgQBAE5Xody+3++usvXHPNNQgKCkJMTAwefPBBlJVZ1ze57777MH78eLz66qtISkpCTEwM0tPTlfciopZh5a6z2HGyAEEaNV4Y3wOCbdb998VSEA8AlYW+uUBqOXw11nsx3nOs51h/qWgbG4JZIzth0+NX4euHhuK+oWmICdHiYrkBS7efwe2LtqPf8xswc/kf+HZvNooqDNaDVWogvitw2T3A2H8D038FnswC7lsjzbUPSwIuHAa+mAyY+L+lSx4z8i2csQJYkNz07/vPc4DWs3K3gIAATJ48GR9//DHmzp2rfNlduXIlTCYT7r77bqxcuRJPPPEEwsPDsWbNGtxzzz1o3749Bg4c6Pb8ZrMZt9xyCxISEvDbb7+huLjYbo6dLCwsDB9//DGSk5Px119/Ydq0aQgLC8P//d//YcKECdi/fz/WrVuHjRs3AgAiImquAV1eXo5Ro0ZhyJAh+P3335GXl4cHHngAM2bMsPvysmnTJiQlJWHTpk04fvw4JkyYgD59+mDatGke/c2IyL+ZzSLe3iSV1M8a2RGp0cHWFw0VwM73bX4vB1G9+GqsBzwe7znWc6y/1AiCgL6to9C3dRTmjumKrcfz8e0f2fjpcB6KKoxKpl4lAJe1jsLVXeJxTZd4dEkMs7/xqwsF0i6XftpfCyy5Hjj1M/D9LOCmtzk161Jmt/ycf82RZ0b+EnL//ffjxIkT+Pnnn5VtH330EW699Va0adMGjz/+OPr06YN27drh4YcfxvXXX48vvvjCo3Nv3LgRhw8fxieffILevXtj+PDhWLBgQY39/vWvf2Ho0KFIS0vD2LFj8fjjjyvvERQUhNDQUAQEBCAxMRGJiYkICgqqcY7PPvsMVVVV+OSTT9CjRw9cc801eOedd/C///0Pubm5yn5RUVF455130KVLF9x4440YM2YMMjIyvP2zEZGf2nLsArIKKhEeGIB7BqfZv7jvM6DiovV3BvJ0ieBYz7H+UqVRq3B153i8eWdf7Jk3El/8bQj+fmV7dE4Ig1kEdp0pxCvrj2D0v3/B1a9uxjs/HUN2kZNS6aRewO0fAYIK+ON/wNY3GuYCs3YCS28C/jtcmvpFzYPBfwN5ZuQbgiZYulvui/f1QpcuXTB06FAsWbIEV111FY4fP45ffvkFzz77LEwmExYsWIAvvvgC2dnZMBgM0Ov1CA727D0OHTqE1NRUJCdbsxVDhgypsd+KFSvw1ltv4cSJEygrK0N1dTXCw8O9+hyHDh1C79697ZrvDBs2DGazGUeOHEFCgrScSffu3aFWq5V9kpKS8Ndff3n1XkTkv5b9lgkAuLVfKwRprf+/DrMJ2PaO9FwVAJirAUOZkzMQecFXY7383h7iWM+xnoAAtQoD20ZjYNtoPDm6C84WVmDTkQvYdDgPvx7Px+mLFXj1x6N4bcNRDGsfi1v7peD67knWsaTTKGD0y8Dax4GMZ4CoNKDHLXW7mPxj0jkOfWfd9vtiaRk88n9+3LWeGfmGIAhSyVtT/9ShzGfq1Kn48ssvUVpaio8++gjt27fHlVdeiVdeeQX//ve/8cQTT2DTpk3Yu3cvRo0aBYPB4P6kHtq+fTsmTZqEG264Ad9//z3++OMPzJ07t0Hfw5ZGY78+qCAIMJvNjfJeRNS0zhVVIuOQlJWbNKiN/YuHvwcKTwFBUUDn0dI2ZuSpvnw11tdhvOdYz7Ge7LWKCsY9g9tgyX0DsGfeSLx6e28MbhcNUQS2Hs/HrBX7MOCFjXhi1Z/4/XQBzGYRGDgNGPyQdIKv/y5l1L1RmgN89yjw7iApiBdUQJvLpdd+/9Dv5luTC7al9UYG8uRDd9xxB1QqFT777DN88sknuP/++yEIAn799VeMGzcOd999N3r37o127drh6NGjHp+3a9euyMrKwvnz1nU4d+zYYbfPtm3b0KZNG8ydOxf9+/dHx44dcebMGbt9tFotTCaT2/fat28fysutX8x//fVXqFQqdO7c2eNrJqLma/nvWTCLwOB20egQH2p9QRSBX9+Sng94AAiJl54zI0+XEI71RK6F6AJwW79WWP7gEPzyf1dj1ohOSI0OQpm+Git2ZeH2Rdsx5MUMzPnqL/zU+mGYOo4GTHrg8zuBglPu36CqBPjpeeCtvsDujwDRBHQaDUzfBkz+BghLlpa/O/B1o39WagDMyJO/CA0NxYQJEzBnzhycP38e9913HwCgY8eO2LBhA7Zt24ZDhw7hb3/7m90cNHdGjBiBTp064d5778W+ffvwyy+/YO7cuXb7dOzYEZmZmVi+fDlOnDiBt956C19/bf8fsbS0NJw6dQp79+5Ffn4+9PqadysnTZqEwMBA3Hvvvdi/fz82bdqEhx9+GPfcc49SakdELZfRZMbynVJZ/d2DHbLxmdul5YTUOmDgg9YGYb7IyG//D/BGT+kLHVET4lhP5JnU6GDMHNERPz9+NVY8OBh39G+FEK0auSV6fL4zE/d/8gcGHL4TZ7QdgYqLqP7fbTVXQdGXAZm/Ab9/IGXg3+oLbHlFyuS2GgBM+QG4a7nUKV+tAQZMlY7b8Z5XK1CRj9g1u/OvKgoG8pegqVOnorCwEKNGjVLmuf3rX//CZZddhlGjRuGqq65CYmIixo8f7/E5VSoVvv76a1RWVmLgwIF44IEH8MILL9jtc9NNN2HWrFmYMWMG+vTpg23btmHevHl2+9x66624/vrrcfXVVyMuLs7psjjBwcFYv349CgoKMGDAANx222249tpr8c4773j/xyCiZifjUC7ySvWIDdXhum4O607L2fg+E4HQeEBrydb7IiNfcREozpSyM0RNjGM9kedUKgGD2sXg5dt6Y8/8kVh6/0DcM7gNkiICUWDU4PaSR5EtxiCg8DgOvD4Gh7+YB3HFZCloX9gKWHIdsOYxKQNfkQ/EdADu+B8wdQPQZqj9m/WbAgQEAuf3Alm/+eTzkhfsAnn/WkdeEEXeCnJUUlKCiIgIFBcX12jOUlVVhVOnTqFt27YIDAz00RVSY+C/LVHzcPcHv2Hr8XykX90e/xjVxfrChSPAuwMBCMCMXUBsB2D7u8D6fwI9bwdu/aBpL3T9XGD7O8DQR4Drnqv36Wobm8h7HOsvTfy3JW+IooiD50uw8WAejv+1AwuK/oEwwUkwF5YEJPQAEnsAyZdJ/VnUmpr7yVY/DOz5BOg2HrhjaaNdPzWA/wwB8g5Kz/tMAsb/p1Hfzpuxnl3riYio2TiVX46tx/MhCMCdA1rbv7jtbemxyxgpiAd8W1ovl+AFMFggImqOBEFA9+QIdE+OAEZ0RMH+JJz98Xn8URyKfdWtcUhsjcBWvfHQmEHo1yba8xMP+rsUyB/6DijKAiJTG+9DUP3Yfn/gHHkiIqK6+ew3qWnWVZ3ikBpts2RWaQ7w5wrp+dBHrNt9WVovD/gBuqZ/byIianDRPUag1ezNGPZ/XwFDZuB3VW9kZJpx63vbMfXj33HwnIdTqRK6A22HS43wfl/cuBdN9WPb7I5d64mIiLxXZTRh5e6zAJw0ufvtv4DJAKQOAloPsm5nRp6IiBpYdIgW/7qxG37+x1WYODAVapWAjMN5GPP2L5i5/A/klXoQ8A2aLj3uXsolUv0Zu9YTERE5UVUMbHwayD3gdtcf9p9HUYURKZFBuKpzvPUFfSmw60PpuW02HrAG8npm5ImIqGElRQRh4S29sGHWcNzYKwmiCHy79xxu+Pcv2HQkr/aDO40CotKAqiJrRRn5F1EEjLal9exa3yKwR2DLw39TIh849B2w9Q1gy6tud/10h7TknJz9UOz5n3RDIKYD0PkG+4P8ISOvCWr696YGwXGh5eG/KTWGdnGheOeuy/D9w5ejS2IY8ssMmPLR73jmuwPQV5ucH6RSAwP/Jj3/7b9cis4fmQyAaLb+7mdd6xnIe0mjkTpQVlRUuNmTmhv531T+NyaiJlBVbP/owqHzJdh9phABKgF3DLBpCmQyAjssHWSHzABUDsOaNkx69OkceZbWNzcc61sujvXUmHqkROCb9GG4b2gaAOCjX09j/LvbcDyv1PkBfe+WxqkLh4GTm5ruQskzRocxwM8y8uxa7yW1Wo3IyEjk5UnlMsHBwRAEwc1R5M9EUURFRQXy8vIQGRkJtVrt60siunTIwa6beWfLLE3uRnVPRHyYTWB84BugOAsIiQN6T6x5oD9k5FlaDwB499138corryAnJwe9e/fG22+/jYEDB7rcv6ioCHPnzsVXX32FgoICtGnTBm+++SZuuEGqujCZTHj66afx6aefIicnB8nJybjvvvvwr3/9q97jMsf6lodjPTWVQI0aT9/UHcM7xeLxlX/i0PkS3Pj2Vsy/sTsmDky1/29JYDjQdxLw2yJgxyKg/TW+u3CqyeAQyBv9KyPPQL4OEhMTAUAZ4KlliIyMVP5tiaiJyB1gaxkcy/TV+HpPNgBg0iCHJecOfCU9DngA0DjJfMuBvNkIVBuAAG19r9hzzMgrVqxYgdmzZ2PRokUYNGgQ3nzzTYwaNQpHjhxBfHx8jf0NBgNGjhyJ+Ph4rFq1CikpKThz5gwiIyOVfV566SW89957WLp0Kbp3745du3ZhypQpiIiIwCOPPFLjnN7iWN8ycaynpnJNlwSsm3kFHlu5D78cy8c/v/4LW45ewIu39kRksM1YNPBBqbT+2Hog/7h1+VTyPcfvJszIN3+CICApKQnx8fEwGo2+vhxqABqNhnfniXxByci7Hhy/3ZuNcoMJ7WJDMKR9jP2Lckl+XGfnB8uBPCCV1wd4sc5vfTEjr3j99dcxbdo0TJkyBQCwaNEirFmzBkuWLMGTTz5ZY/8lS5agoKAA27ZtU0qg09LS7PbZtm0bxo0bhzFjxiivf/7559i5c2eDXDPH+paHYz01tfjwQCydMhAfbj2Fl9cfxroDOfgjqxDzb+yOG3omStn5mPZS47uj64Cd/wVueMXXl02yGqX1/tW1noF8PajVag4IRET1oQTyzjPyoigqTe7uGtS6ZnmzfLfcVdZbrQHUOsCkl8rrg5sykGdGHpCy67t378acOXOUbSqVCiNGjMD27dudHrN69WoMGTIE6enp+PbbbxEXF4e77roLTzzxhDLuDh06FO+//z6OHj2KTp06Yd++fdi6dStef/11p+fU6/XQ6603jEpKPFvvmWM9EdWHSiVg2vB2GNI+Bo98/gdO5pcj/bM9GNYhBs/c1B0d4sOAQX+XAvm9nwHX/AsIjPD1ZRNgk5EXAIh+F8iz2R0REfmOPCganQ+Of2QV4dD5EugCVLitXyvXx9cWLCvz5Ju44R0z8gCA/Px8mEwmJCQk2G1PSEhATk6O02NOnjyJVatWwWQyYe3atZg3bx5ee+01PP/888o+Tz75JO6880506dIFGo0Gffv2xaOPPopJkyY5PefChQsRERGh/KSmpjrdj4ioMfRIicDamVdg5rUdoQ1Q4dfjF3H9m79gwdpDKEu5HIjrKo1Tf3zq60slmbz0XFCU9MhAnoiIyEIOdp1k5EVRxFsZxwAAN/ZKtp9TKJPvlte2xJs2VHps6oZ3zMjXmdlsRnx8PN5//33069cPEyZMwNy5c7Fo0SJlny+++ALLli3DZ599hj179mDp0qV49dVXsXTpUqfnnDNnDoqLi5WfrKyspvo4REQApEZ4s0Z2wsZZV2JE1wRUm0W8v+UkrnntZ+xNuVPaacurwPq5wIlNLm9yUxORv2PI1XzmasBU7bvrccDSeiIi8h15kHTyZWXDwVxsPnIBGrWA9KvbOz/ek2BZJwfyzMj7QmxsLNRqNXJzc+225+bmumw6lpSUVGM+c9euXZGTkwODwQCtVot//OMfSlYeAHr27IkzZ85g4cKFuPfee2ucU6fTQae7tP8tiMg/tI4Jxgf39semw3l4+rsDOHOxAnfuaI2fQlKQXJkNbH9H+gkIAtpeAbS/FugwQppPzxU0mo78HUXOyAPS9w51qG+uxwEz8kRE5DtKRr4KEEVlc6XBhGe+OwgAeHB4O7SLczFoepSR99ESdMzIAwC0Wi369euHjIwMZZvZbEZGRgaGDBni9Jhhw4bh+PHjMJvNyrajR48iKSkJWq1UmVFRUQGVyv5rjFqttjuGiMifXd0lHusfHY5/jOoMaIJwXfkzeNj4MHZFjoYpJEGqVjv2I7DuCeCdfsC/e0kZe7PJ15d+aTA4lNYDftW5noE8ERH5jjLfTARMBmXze5uPI7uoEskRgUi/upaleLyaI9+EgbypGhAtX7Qu8Yw8AMyePRuLFy/G0qVLcejQIUyfPh3l5eVKF/vJkyfbNcObPn06CgoKMHPmTBw9ehRr1qzBggULkJ6eruwzduxYvPDCC1izZg1Onz6Nr7/+Gq+//jpuvvnmJv98RER1FahRI/3qDsh47CoM79kO35mG4Lace9C95E183GsZqq5+Cmg7HFBrgaJM4KfngE9vBcov+vrSWz45WaANlf7+gF/Nk2dpPRER+Y7tgGisBAJ0OJ1fjkU/nwQAzB/bDcFaF0OVaNNB1qM58k1YWm875/8Sz8gDwIQJE3DhwgXMnz8fOTk56NOnD9atW6c0wMvMzLTLrqempmL9+vWYNWsWevXqhZSUFMycORNPPPGEss/bb7+NefPm4aGHHkJeXh6Sk5Pxt7/9DfPnz2/yz0dEVF8pkUH4z6R+2H2mEAvWHsLuM4V4eqeAt0N6YuaIWzDxjmhoDn4F/PAkcHIT8N/hwB2fAK36+frSWy55+TlNsDSWmwwM5ImIiADYD4jVVRBFEU9/dwAGkxnDO8VhVHfnc6il/W3K2/wtI297bWpm5AFgxowZmDFjhtPXNm/eXGPbkCFDsGPHDpfnCwsLw5tvvok333yzga6QiMj3+rWJwqq/D8H6A7l4ad1hnMovx/xvD+CjX0PwxPWjMOqB/hC+mAwUnACWjAJGvwj0n8q5841BCeSDpOo6PfwqkGdpPRER+Y5twGustGtw9/TYbjXXjbc71ibr7ckceX1TZuQtA71aC6g41BIRkecEQcD1PRLx46zheG5cd8SEaHEqvxx//3QP7l1bgdLJG4AuNwJmI7DmMeDrvwOGCl9fdsujlNYHS40HAb9aSYDfLoiIyHeM1mC8qrLcswZ3yrGWwVRQA2qN6/18sY680rGeZfVERFQ3GrUK9wxJw+Z/XIWHr+mAQI0KW45ewB1LDyJ39AfAyGcBQQX8uRz4YARw8YSvL7llsSutt1TXMSNPREQEu4z8yt88bHCnHOtBx3oA0IZJj01aWi834WNZPRER1U9YoAaPXdcZK/82FLGhOhw6X4Jb3tuO4x3vByavBkLigbwDwPtXAUd/9PXlthy2K+NoLDfmGcgTERHBbkBcu+cUADcN7mwZPVzezSdz5Ln0HBERNayerSLw9UND0S42BNlFlbj1ve34XegO/G0LkDoY0JcAKyYBxzf6+lJbBvl7g9zsDmAgT0REBMBuQFSb9e4b3Nkd62lG3ofN7piRJyKiBpQaHYxV04eib+tIFFcaMemD37AuE8B93wNdb5I6qy+fBJz6xdeX2vwpGXkG8kRERFa2y8cBCFEZ3De4s+VxRt4Xy88xI09ERI0jOkSLzx4YjJHdEmCoNmP6sj1Y+ls2cOuHQMdR0hj02QQga6evL7V5sy2tVwJ5vev9mxgDeSIi8g2HwXBM1yj3De7sjpcH2EYsrc/eA5Sc8/44ZuSJiKgRBWnVWHR3P9w9uDVEEXhq9QEs3HAC5tuXAu2uAozlwKe3Aef2+vpSmy+jbWm9ZTy3adLrawzkiYjINxzK00Z1jvTueCUj72lpvZcZ+eKzwAfXSlkNbzEjT0REjUytEvDcuB74x6jOAID//nwSc78/DnHCMqD1UEBfDPzvZiD3oI+vtJmyW36OGXkiIiKJw2CoEw1eHm8Jlt1m5OXSei8z8kWZgGgGirO8Ow5gRp6IiJqEIAhIv7oDXr29N1QC8PnOTCzangvctQJI6QdUFgCfjAPyjzfuhZTmAkV1GC/9me3yc+xa79y7776LtLQ0BAYGYtCgQdi50/V8jquuugqCINT4GTNmjLKPKIqYP38+kpKSEBQUhBEjRuDYsWNN8VGIiMhDZkOF/QZvB0f5Trm7rLeujnPkq4ot71OHQbvaw2oBIiKiBnBbv1Z4amx3AMBL6w5j7bFy4O4vgcSeQHke8MlNQOHpxnlzUzXw4Uhg0TCgsrBx3sMX5O8pdnPkGcgrVqxYgdmzZ+Opp57Cnj170Lt3b4waNQp5eXlO9//qq69w/vx55Wf//v1Qq9W4/fbblX1efvllvPXWW1i0aBF+++03hISEYNSoUaiq8p8/PBHRpe7E+Yv2G7ydd+Zp+Xpd58hXlVjep1JqzOfVtTEjT0RETeveoWm4b2gaAGDWir3Ymy8A93wDxHYGSrKBpTfVre+LO+f2AEVnpBvguQca/vy+wq71tXv99dcxbdo0TJkyBd26dcOiRYsQHByMJUuWON0/OjoaiYmJys+GDRsQHBysBPKiKOLNN9/Ev/71L4wbNw69evXCJ598gnPnzuGbb75pwk9GRES12Xc6135DXTPyni4/ZzIA1V6U7+tLbK7NyzlxnCNPREQ+MO/GbrimSzz01WY8sHQXzhqCgXtXA9HtpGB77T8a/k2PZ1ifXzjc8Of3BVG0L62Xx/O6VOk1Ep8G8gaDAbt378aIESOUbSqVCiNGjMD27ds9OseHH36IO++8EyEh0he1U6dOIScnx+6cERERGDRokMtz6vV6lJSU2P0QEVHjOpjlUHnVWBl5TYjNe3iRla8qsnkvb6+NGXkiImp6apWAtyb2RdekcOSX6XH/x7+jRBMD3Pk5AAE4/H3Dd7I/YRvIH2nYc/tKdRUASzUeS+trys/Ph8lkQkJCgt32hIQE5OTkuD1+586d2L9/Px544AFlm3ycN+dcuHAhIiIilJ/U1FRvPwoREXnBUG3GyXMOpfXeZr09zcgHaAG11vLG3gTyNjd1vb0Dz4w8ERH5SKguAB/e2x/xYToczS3DjM/+QHVMJ6CnZSry5hcb7s0qC4Hs3dbfW0ogb5tcsGt2x671DeLDDz9Ez549MXDgwHqdZ86cOSguLlZ+srJaWMdFIiI/80dmIWByCI69znp7ESzL5fV6Lxre2ZXWMyNPRETNR3JkED68dwCCNGpsOXoBT60+APHK/wMEFXD0ByB7T8O80cnN0govast4V9dAvvC0X5WtK2X1ai2gDrDJyHMdeQBAbGws1Go1cnPt50nm5uYiMTGx1mPLy8uxfPlyTJ061W67fJw359TpdAgPD7f7ISKixvPriYsIhNF+Y12z3u4y8kDdlqCTu9YDzMgTEVGz07NVBP59Zx8IArDst0x8eEgN9Jogvbh5YcO8iTw/vtcd0mNZDlBZ5N05cvYD/+4DfPWA212bjGPVn3xjnhl5iVarRb9+/ZCRYZ1XYTabkZGRgSFDhtR67MqVK6HX63H33XfbbW/bti0SExPtzllSUoLffvvN7TmJiKhpbDueDx0cGs953ezOm4x8HZagq6pPRl6+NmbkiYjId67rnoi5N3QFALyw9hA2J94HCGrg2I/A2V31O7koAid+kp53Hw+EJUvP8496d55TWwCIwPl99buehiTf+NcES4/ycrKcI281e/ZsLF68GEuXLsWhQ4cwffp0lJeXY8qUKQCAyZMnY86cOTWO+/DDDzF+/HjExMTYbRcEAY8++iief/55rF69Gn/99RcmT56M5ORkjB8/vik+EhER1aJcX429WUXQCY4ZeW+DZQ/nyAN1W4JOX5858nJpPTPyRETkW1Mvb4tJg1pDFIGp3xUgM3Wc9MKmBfU78YUj0rJ2AYFAm2FAXGfLdi871+f8JT2W5nq/3KsnDBXel/zbLj0HWG/M+1H5f4CvL2DChAm4cOEC5s+fj5ycHPTp0wfr1q1TmtVlZmZCpbK/33DkyBFs3boVP/74o9Nz/t///R/Ky8vx4IMPoqioCJdffjnWrVuHwEB+oSIi8rWdpwpQbRaRFAZI1fUCALGRM/J1CORtS+uZkSciomZKEAQ8O64HKo0mfLUnG/ccuwKbAldDdSIDyPwNaD2obieWu9W3GSrdVI/rApzc5H3QnGsJ5E16aewNiqzb9biy+mFg/yrggQygVX/PjrFdeg7wy671Pg/kAWDGjBmYMWOG09c2b95cY1vnzp0h1nK3RhAEPPvss3j22Wcb6hKJiKiB/Ho8HwDQKUYD5AAIDJcGbm8HR68y8vUsrWdGnoiImjG1SsCrt/WGLkCFz3cCK4xXYGLAJmDzAmDyt3U7qTw/vv210mNcJ+nRm0C+2gDk2WTwy3IbPpDP3S895h+rQyBv+Y7BrvVERHSp+/WEtOxc+yi1tCEoSnr0Nlhu7Iy8Xdf6uja7Y0aeiIj8g0olYMHNPXHf0DS8axoPo6iWus6f2e79yYyVwJlfpecd5EC+i/ToTSCffwQw20y1K3W/BLnXyixN0PWlnh8jl9ZrHTPy7FpPRESXoItlehw6LwXIrcMtgXxgpPTodfl6XebIe5iRNxmtd+MB7+fve3OTgYiIqIkIgoCnxnbDmCsG4QvTVQCAs1/P8/5EZ7ZJN63Dkq0BvPxYnOn5jfOc/fa/l+U636+uqg3SWvcAoC+ufV9bNUrr2bWeiIguYdtPStn4LolhCFZVSxvlEromych7GMjbltUD9cjIM5AnIiL/IggCnhzdBRWDZsIgqtGq6Hes+nJ5rVOXa5C71Xe4BhAE6XlwNBASJz33tHO93OhO1tAZ+fIL1ufeZOQNDqX1ctd6b2/sNyIG8kRE1GR+PS4F8kPbx1oHw6bIyOvCpEdPMwSOd+297qgvz5FnaT0REfkfQRAwbeyVOJJyCwAgdd8beGXdYc+Decf58bJYuXO9h+X1OX9Kj0HR0mNDZ+TL86zPvSqtZ0aeiIhIse2E1OhuWIcY62AoZ+S9HRwbc458lUMgz4w8ERG1QD0nPAOToMEg1WH88ct3WPLrafcHFWcDFw4BEIB2V9m/FudFIC+K1kZ08jz7hg7ky2wy8o7VdrVxXH5OThpUVzXOEnl1wECeiIiaxNnCCpy5WAG1SsDAttHWYFfOyBsrvRscvQmWvQ7kHQZ7ZuSJiKglikiBesD9AIBZAavw4g8HsT/bzVzyk5ukx5TLpHJ6W940vCvJluavqwKAtldK2xq6tN72xkBdmt0ppfXyeC4CJkODXFp9MZAnIqImsc1SVt+7VQTCAjXWQFzuWu/t4KgMsp4E8l4uP6fnHHkiIrpEXD4LYkAgBqqO4ApxDx75/A+U66td7++qrB6wWYLucM3XHMmN7mI7A5GtpeeNWlrvTUbecuNfTgTYjud+spY8A3kiImoSvypl9bHSBiWQj7Tu5Gnm22yyLlcT4E3X+jqW1jMjT0RELVV4EoSB0wAAL2s/QFH+eTzz3QHn+5pN1ox8B2eBvCUjX3jK/ZQ5udFdYg8gLFF6XtrQpfV1nSPvkJFXawFYmvr5yTx5BvJERNToRFHEthM2je4A60CoDYV1cPTwLrdtYO1RRt4SyOuboGu9KDIjT0REzcvVc4G4LohFEV7WvI8vdmVh9b5zNfc7t1cqh9dFACn9a74emgAERgCiGbh4vPb3lBvdJfaUjgOkZrMN2Rm+rK4ZeYeu9YJgHdP9pHM9A3kiImp0x/LKcKFUj0CNCpe1iZQ22t7t1ni5rIttYO1RRl4urfe0a3095sibjAAsc/2ZkSciouZAEwTc+gGg1mKEeg/uVm/E3K/+QlZBhf1+Jyxl9e2GA+qAmucRBJvO9W7K6+VGdwk9pOBfDpQbcp68bSDvTbM7g0PXesDvOtczkCciokb363GprH5AWjR0AWppo1J+HmgdvD0dHOXAWq0FVB4MZd7OkZdL64NjLNflRUbe7iYDM/JERNRMJPYERjwNAJinWYYEw2k8svwPGE1m6z61zY+XKZ3ra1lLXl8KFJy0vq8gAKHx0u+2wXd9OS4/52lTXceu9YB953o/wECeiIgand368TJ5HfiAQJvB0dOMvHwTwINsPFD3rvVyqZ83GXnbmxHMyBMRUXMyaDrQ/hroYMA7undxIPMC/r3xmPRaVTFw9nfpubP58TKlc30tGflcyxz8sGQgxPLdINQyT76sITPyNnPuRZPn47njOvKATUaegTwREV0Cqk1m/HZSCuSHdYixecES8GpsMvJGDwfHai861gPWQN6kt5S+u6G3ZOTlQL4uGfmAQCnDQERE1FyoVMD494DgGHTBGfxfwHK8u/k4tp+4CJz8WQqGYzpau8w748la8raN7mRhljG3oRreVetrNq/1dJ684xx5wKZ6kIE8ERE1d6IIfJsO/DjP5S5/ZRejVF+N8MAAdE+OsL5gG/B6m5E3etlMTi6tBzzLyjdERp7ZeCIiao7CEoFx/wEAPBDwAy4X/sSsFXuhP7xBer22bDxgDeQvHgdMLpaxUwL5ntZtDZ2RL78gPao0gC5ceu5p53p53NfaZuS9TDo0MgbyRERUd8VngT8+Bba9JS1J44TcrX5wuxioVTYZattgXA56vc7Ie1haH6CVBnLAw0Bezshb5uvVNSNPRETUHHW+HhjwAADgTd1/YSjJQ+nB9dJrtc2PB4DwVoAmRFomtvCU832cBfINnZGXy+pD4qRmekAdMvJOAnlm5ImIqNmzbUhjrHC6i9zoTlk/XmYb8AZ42UDG24w8YDNP3oOGd/JAL69r683dd2bkiYioJRj5HBDbGTFiIT7WvoLY6lyYVBogbVjtx6lUQGxH6bmzefKmaiDvoPQ8oREz8mWWjHxoPKALk5572rne4KS0Xp7Ox671RETU7Nk2kXFSfl5lNGHXmUIADvPjzSbpTj1gKa338i63txl5wDqIexLIO5bWe1ryDzAjT0RELYM2GLjtQ0CtRS/VCQDA7+YuyNOr3R+rNLxzMk++4IQ0VmpCgOi21u3ymFvWwBn50HjvSuvNZpvvGSHW7UpGnuvIExFRc1dee0b+r+xiGKrNiA/ToX2czTx1uyXadDbzzhppjjzgeed6UaxZWu9VRl6+NmbkiYiombNZkg4AfjL2wFPfHnB/XG0N7+Sy+oTugMrmpkBDl9bL31FsM/KeBPK231Hsmt1xHXkiImopbEvrDTUD+dP5UtDcOTEMgm0Hd7sl2gK9X5u1Lhl5TwP56iprtYBc5udVRl4urWdGnoiIWoBB04Fu42DShmOdOAQ/7M/Buv1uyt+VQN5Jab2zjvWAdcwtv+C6SZ435O8oIbaBvAel9bZJBbtAnuvIExFRS2E3R75msJtVKG1rFRVs/4I8CKoCAHWATbM7P8jIK/PnBOvatuZqz79UsLSeiIhaEpUKuH0p1E+extgrBwEA5n27H8UVtSznKpfW5x+TStVtOWt0B0hjrqACIFo7ztdHmU1GPtCL0nqj5XtCQKB9xYC3jXkbGQN5IiKqO7vS+poBclaBlKVvHe0QyMsBuxzsenuXu04ZeUtpv7s58nJZfWC4/fk9zcqz2R0REbU0ggCo1Hj4mo5oFxeCC6V6LFh7yPX+kW0AtU4aO4sz7V9TSusdAnmVWsqeAw3T8K7MWWm9Fxl5x+8Y7FpPREQthruMvCWQT412GAwdy8+9bXbXmBl5eZDXRdif3+Ol8ZiRJyKililQo8aLt/QCAKzYlaWsTFODOgCI6SA9t50nX5prSQIIQEK3msc15Dz5ctvSektG3pOu9c6WngO8/67SyBjIExFR3blZfi7TVUbeMdiVM/LeriNfl0Be70VGXhC871LLjDwREbVgA9tG457BbQAAc776C5UGk/MdnTW8y7Vk42M6WMdlW8oSdA0QyCvLzyV417Xe2dJzADPyRETUgtTS7K7KaEJeqRTUprqaIy8Huxovg2U54Nd4E8h7W1ofYblGuaM+M/JEREQA8H/Xd0ZSRCAyCyrw+gYnnekB50vQuWp0J5NXi6lvIG+sAvTyCjRx3nWtV0rrHb67KF3rGcgTEVFzZqwEDDYDokNG/myh9HuYLgCRwRr7Y+VBUL7bXeeMfF3myHtaWh9uf40eZ+S5/BwREbVsYYEavHCzFIx/uPUU9mUV1dwprpP0mG8byO+XHh0b3SkntmTkS+s5R14uq1drgcBIL+fIuyit9/a7SiNjIE9ERHVjm40HasyRzyqwdKyPDrZfeg6wmeOus3/0do68Vxl5L7vWyx1umZEnIiKq4ZouCRjXJxlmEXjiyz9hqHboTm+bkRdF6bmSke/l/KShljny9c3I2y49Jwhedq13VVrPjDwREbUENQJ5+4y8dX68k6y5EuxaXpMHS0+Xn6vPHHlvS+u9zshzjjwREV0a5t/YDVHBGhzOKcV/fz5h/2J0e0BQS1nw0vPSGH/xmPRagovS+obKyNt2rAesGXlvmt05zuFX5sjr63dtDYSBPBER1U157YG80rHecX48UDPY9XZwlPer0/JzXpbWMyNPRETkVEyoDk/f1B0A8PZPx3Es1ybjHaAFottJzy8cAfIOAqIZCI61BuyOGqrZnXy8Eshbbs57NUfe4TsGu9YTEVGL4DjIGlxk5GOcBfIOGXVvs96O69B7oq6l9czIExERuXRT72Rc3TkOBpMZT3z5J0xm0fqibed6pay+p1Tu7kyYTWm9KDrfxxPllo71IXHSo+0ceXfndVlaz0CeiIhaAnlZF5njHPlC6fdaM/Ly3e26Zr29ysjXsbSeGXkiIiKXBEHACzf3RIhWjT2ZRfjf9tPWF5V58odtGt25KKsHpDntAGAyAJWFdb8opbTecmNADuQhuv8eoCw/56q0noE8ERE1Z3JGXi5XsymtF0XRWlrvuIY84GQd+TouP+dNsCwP4u4G8Hp3rWdGnoiILi3JkUF48oauAICX1x9RvgMoGfn8o+4b3QHSDf7ASOl5fcrrHUvrNUGAKkB67q683lVpvbc39hsZA3kiIqobeY58VBvp0SaQL6owokxfDQBoFeUka+7YtV7jbdbbxSBbG3atJyIiajSTBrbGwLRoVBhM+OfXf0EURWsgn3cIyHWz9JysIRreyaX1ciAvCJ6vJe9y+Tk2uyMiopZALq13EsjL8+Pjw3QI1KhrHuvYtV5+9Hb5uUaZIy+X1kdKjxovqwWYkSciokuQSiXgxVt7Qhugwi/H8vHlnmwgpiMAAagskCri1DrLtlo0xBJ08rFyqT7geed6pWu9QyDPZndERNQiyINkVJr0aDNHPqtQXnrOSVk9UDPY9XZwrFNGPtT6HqZq1/vpLYG80rVeXhqPGXkiIqLatIsLxaMjpED9ue8P4oJeDUS2tu4Q3xVQB9R+kobIyMvJBvmmAGDTud5dIO+mtJ6BPBERNWty2ZocyBtqZuSdzo8HanatV4LlSs+61NYnIw8ARhdZebPZWnKndK33NiPPQJ6IiC5d065oh+7J4SiuNOLp1QesDe+A2hvdyZSMfF7t+7liqAAMlrE8NM66vd6l9ZbkAwN5IiJqtgzl1qZxSkbeGshnFVg61rvLyCtd6+UydFHqVFsbUaxbRl6ttTa6cVVebyiT1rgFbLrWe5uRl6sNGMjbevfdd5GWlobAwEAMGjQIO3furHX/oqIipKenIykpCTqdDp06dcLatWvt9snOzsbdd9+NmJgYBAUFoWfPnti1a1djfgwiInJDo1bhpVt7Qa0SsOav8zgptLK+WFujO5mckS+rY0Ze7uETEGitrgPsl6CrjcHV8nOW300GwGyq27U1IAbyRETkPfkueUCQdf6ZXSBvycg7a3QH1Mxa2w6WRjeZb5PRGmx7EywLgjUrr3fRuV4e3FUam2tjRr6+VqxYgdmzZ+Opp57Cnj170Lt3b4waNQp5ec6zLQaDASNHjsTp06exatUqHDlyBIsXL0ZKSoqyT2FhIYYNGwaNRoMffvgBBw8exGuvvYaoqKim+lhERORCj5QI/G14OwDApydsxkN3je4Aa0a+tI5z5OXvKCHx9uvVy5V2HmfkHZefs+l94wcN79xMUCAiInJC6QYbZw2OvZkj79i1Xq0FIAAQ3Zes2QbU3mTkAUAbJjWzc7UEnW3Hennwr3NGns3uZK+//jqmTZuGKVOmAAAWLVqENWvWYMmSJXjyySdr7L9kyRIUFBRg27Zt0Gg0AIC0tDS7fV566SWkpqbio48+Ura1bdu28T4EERF55ZFrO2Ld/hzsuZgAyENiQnf3Byql9XXMyCtryMfZb/e4tN7NHHlA+q7i2AyviTEjT0RE3lPWZ02wDnSWO9gms4jsQnel9Q5d6wXBZr12NwGzElALlhsAXnDXuV7pWB9h3eZ1Iz5m5G0ZDAbs3r0bI0aMULapVCqMGDEC27dvd3rM6tWrMWTIEKSnpyMhIQE9evTAggULYDKZ7Pbp378/br/9dsTHx6Nv375YvHixy+vQ6/UoKSmx+yEiosYTqFHjpdt64YCYhq2m7shtf4f9+OqK0uyujhl5ubTettEd4H3XesdAXh1gnaLnB/PkGcgTEZH3bMvW5GYw5mqg2oDzxZWoNovQqlVICHcRzFY7ZOQBz9drt22UZ1sy5wl3gbxcWm87p862EZ8nmJG3k5+fD5PJhIQE+y9UCQkJyMlxnm05efIkVq1aBZPJhLVr12LevHl47bXX8Pzzz9vt895776Fjx45Yv349pk+fjkceeQRLly51es6FCxciIiJC+UlNTW24D0lERE4NSIvGnYPb427jXNyRcxcM1Wb3B8kBuKHU/ZKxzijfURwz8nJpvafLz4XUfM2POtczkCciIu/Zlq3ZdnU1ViiN7lKigqBWuQi05QHQ9m63kpF3EzDLgb6mDhlvJZD3oLReuS4vBm1RZEa+AZjNZsTHx+P9999Hv379MGHCBMydOxeLFi2y2+eyyy7DggUL0LdvXzz44IOYNm2a3T625syZg+LiYuUnKyurqT4OEdEl7YnRXRAbqsOZixVYuduD//bqwqzfLeqylnyZq4y8p3Pka2moqwTyvp8jz0CeiIi8Z1u2ptYAglr63VhpbXTnqqwecJ61lp97nJH3cn48YF1L3mVpfZH0aFv6501G3rbjPjPyAIDY2Fio1Wrk5tp/GcvNzUViYqLTY5KSktCpUyeo1WplW9euXZGTkwODwaDs061bN7vjunbtiszMTKfn1Ol0CA8Pt/shIqLGF6oLQPrV7QEAb2ccR5XRTcd3Qahfwztl+l+8/XZPutabzTbJBiffY5TqQQ+r9BqR14F8Wloann32WZcDJRERXQJsy9Zsu8EbK5RGdy471gPWAdA2ax3QlBl5d6X1dZwjb7sPM/IAAK1Wi379+iEjI0PZZjabkZGRgSFDhjg9ZtiwYTh+/DjMZmsJ5tGjR5GUlAStVqvsc+TIEbvjjh49ijZt2jTCpyAiovq4a1BrJEcEIqekCst+8yCOrE/DO6Uhr0Mg70nXepsVeJwH8vJa8s0wI//oo4/iq6++Qrt27TBy5EgsX74cer3vPwgRETUhx7I1m4Z3mQVuOtYDztda13hYrubYKM8bdSmt9yYjr1y7IFUqEABg9uzZWLx4MZYuXYpDhw5h+vTpKC8vV7rYT548GXPmzFH2nz59OgoKCjBz5kwcPXoUa9aswYIFC5Cenq7sM2vWLOzYsQMLFizA8ePH8dlnn+H999+324eIiPyDLkCNh6/tCAB4b/NxlOuraz8grAEy8iEuMvK1NbuzHeud3ZD3tDFvE6hTIL93717s3LkTXbt2xcMPP4ykpCTMmDEDe/bsaYxrJCIif6OU1lsGSfmutaHCw9J6J/PIPQ2Yq+uRkZcHcZeBfD271tt+Lm8b8bVgEyZMwKuvvor58+ejT58+2Lt3L9atW6c0wMvMzMT58+eV/VNTU7F+/Xr8/vvv6NWrFx555BHMnDnTbqm6AQMG4Ouvv8bnn3+OHj164LnnnsObb76JSZMmNfnnIyIi927r1wqto4ORX2bA0u2na9851DL1qi4Z+TIXGXlPlp+TM/IBQYDKSaisZOR9H8jXeR35yy67DJdddhlee+01/Oc//8ETTzyB9957Dz179sQjjzyCKVOmQOCXGCKilsmxI6wcyBsrkFkgzX2rPSPvpGu9pwGzsT5z5Bu5az071rs0Y8YMzJgxw+lrmzdvrrFtyJAh2LFjR63nvPHGG3HjjTc2xOUREVEj06hVeHRER8z+Yh/++/NJ3D24DcIDXVSv1TUjry8DjJYxvkYg70Vpvas14ltC13qj0YgvvvgCN910Ex577DH0798fH3zwAW699Vb885//5B1xIqKWSl9mHegcSuv1leXIL5OC2dQoF4OgbWd3246wnjaQqU9G3u068vXsWs+O9URERC6N65OCDvGhKK404sNfTrneUcnIexnIyxWDAUHWBrcyOZA3lAJmFw33lDXk3QTy7hrzNgGvM/J79uzBRx99hM8//xwqlQqTJ0/GG2+8gS5duij73HzzzRgwYECDXigREfkJeZDUBAM6yyBpuXNdUFQIIBrhgQGICHZxl91cDYiWJmbO1pH3OCPfGMvPOSmtD7CZDyeKtZfMG51UGhAREREAQK0SMHtkJzy0bA8+3HoK9w1NQ1SItuaOckbe20DetqzecbyWS+sB6XuA7Vgvq23pOaB5Z+QHDBiAY8eO4b333kN2djZeffVVuyAeANq2bYs777yzwS6SiIj8SJnD/HhAuXNdWCwFwrXOj3fVSEbjbUa+EZafc1Zab5v5dzdwMyNPRERUq+u7J6JbUjjK9NVYtOWE853kjHypl3PkXS09B0jjudpy08BVeb1Bzsi7CuSbcdf6kydPYt26dbj99tuh0TjPtoSEhOCjjz6q98UREZEfUubH1wzkS0qkQNhlWT1gP/g5XX7OzeDYIBl5L0rrbefiu73JwDnyREREtVGpBDx2XScAwNJtp5FX6uQmuTx1ryIfMBk9P3m5w6o6jtx1rldK60Ocv650rW+G68jn5eXht99+q7H9t99+w65duxrkooiIyI85u9ttCeTLy6SBsXVMbYG8ZfBT6+zL3pS56I2ZkbcMzHovSuvVAYAqwP693V0bM/JEREQuXdMlHn1SI1FlNOM/m5xk5YNjrGOvnEDwhGMzXkfuOte7La1vxhn59PR0ZGVl1dienZ3NtVuJiC4F5U6WdbEMeBXl0sCYGlVLkO1sDXnApjt8Y86Rr2X5OVO1tdOtzmHenLdL4zEjT0RE5JIgCHj8us4AgM9+y8S5IofxVaWyVv55swRdmbuMvJvO9UZ3pfXNeB35gwcP4rLLLquxvW/fvjh48GCDXBQREfkxZ6X1lmZ3hkopQPZoDXnHrvPKXe4myMg7K63X25TZ2ZbWA553rnd1k4KIiIjsDOsQg0Fto2EwmfH2T8dr7qA0vKtDRj7UVUZeDuSLnb+uLD/norRe/q7iB13rvQ7kdTodcnNrdg88f/48AgLqvCw9ERE1F7U0uzNWSQFy7c3uXGStNZ5m5OtRvl5bIC+X1WuCAbVDDxhPqwWYkSciIvKIIAh4fJSUlV+5KwtnLjqMzXVpeFfuJNlgq96l9c24a/11112HOXPmoLjYehejqKgI//znPzFy5EivL+Ddd99FWloaAgMDMWjQIOzcubPW/YuKipCeno6kpCTodDp06tQJa9euVV5/+umnIQiC3Y9jV30iIqqHcmeBvDTgacxVEAQgJbK20no52HXYx9PBsdrNIFsbuWt9dWXNNWSddayXeTx/nxl5IiIiTw1Ii8aVneJQbRbxVoZDVr4uS9C5K60P9LS03kVCQvk+4Ps58l6n0F999VUMHz4cbdq0Qd++fQEAe/fuRUJCAv73v/95da4VK1Zg9uzZWLRoEQYNGoQ333wTo0aNwpEjRxAfX/MuisFgwMiRIxEfH49Vq1YhJSUFZ86cQWRkpN1+3bt3x8aNG60fkpUCREQNRx5QnXStD4IeCWGBCNSoXR/vqrO7xsN5Z/VZq922VM5Qbl9C76xjvUwOzD3OyDOQJyIi8sQj13bAz0cv4If95/H8+B4I0lq+Q8jBuKcZeVH0oLTeTdd6g5tAPsDDG/tNwOsINyUlBX/++SeWLVuGffv2ISgoCFOmTMHEiRNdLkfnyuuvv45p06ZhypQpAIBFixZhzZo1WLJkCZ588ska+y9ZsgQFBQXYtm2b8l5paWk1P1RAABITE739aERE5I4oAmXOmt1ZA/nWtZXVA9bBr0azO0+DZfn4OmTkA3SAoAZEk5NA3knHepmny83INykc5/8TERGRU5e1jkJKZBCyiyrx89ELuL6HJY4L9TIjbyizjtMuS+vr2+zOf7rW1ylVHRISggcffLBeb2wwGLB7927MmTNH2aZSqTBixAhs377d6TGrV6/GkCFDkJ6ejm+//RZxcXG466678MQTT0CttmZ/jh07huTkZAQGBmLIkCFYuHAhWrdu7fJa9Ho99HrrP4a8DjIRETmwHSRDaza7CxIMaBXtJsB2Fex6epdbDvTrEiwLglRery+u2bm+ttJ6ZuSJiIgahSAIGN0jER9sPYV1+89bA/kwL+fIy9l4TQigC3W+j8dz5F1l5P2na32da84PHjyIzMxMGAwGu+033XSTR8fn5+fDZDIhIcF+/kJCQgIOHz7s9JiTJ0/ip59+wqRJk7B27VocP34cDz30EIxGI5566ikAwKBBg/Dxxx+jc+fOOH/+PJ555hlcccUV2L9/P8LCwpyed+HChXjmmWc8um4iokuaPEhqQ+3L1L3KyLsIdjVNkJEHpMHdWSBfW2m9txl5NrsjIiLy2OieSfhg6ylsPJQHfbUJugC1tdmdp13rnTXjdaQE8u661rsK5P2na73XgfzJkydx880346+//oIgCBBFEYB0JwUATCZTbYfXi9lsRnx8PN5//32o1Wr069cP2dnZeOWVV5RAfvTo0cr+vXr1wqBBg9CmTRt88cUXmDp1qtPzzpkzB7Nnz1Z+LykpQWpqaqN9DiKiZktZes5h7pkl0A2CHqlRbgJ5ZR14h2A3oInK1111rq+ttP4SzchnZWVBEAS0atUKALBz50589tln6NatW70r84iIiGR9UyOREK5DbokeW4/l49quCfbN7kRRqqqrjVyCX1sgL4/xdW1215y71s+cORNt27ZFXl4egoODceDAAWzZsgX9+/fH5s2bPT5PbGws1Gp1jaXscnNzXc5vT0pKQqdOnezK6Lt27YqcnJwalQGyyMhIdOrUCcePO1mb0EKn0yE8PNzuh4iInHA1SGqk4DhY0KN1jLuMvJy1dsioe9oJ1ljPjLyrQL7WrvWXZkb+rrvuwqZNmwAAOTk5GDlyJHbu3Im5c+fi2Wef9fHVERFRS6FSCRjdIwkA8MN+Sym9PM/dbAQqCtyfpNzSw8cx2WCrvsvPaZpxIL99+3Y8++yziI2NhUqlgkqlwuWXX46FCxfikUce8fg8Wq0W/fr1Q0ZGhrLNbDYjIyMDQ4YMcXrMsGHDcPz4cZjNZmXb0aNHkZSUBK1W6/SYsrIynDhxAklJSR5fGxERuVDupNEdgGq1FLgGwuA+I+9qrXVlrXZ3wXI95sgD1iXoapTWyxl5zpGX7d+/HwMHDgQAfPHFF+jRowe2bduGZcuW4eOPP/btxRERUYsy2jI3/scDOTBUm4EALRAcI71Y5sE8eSXZ4GLpOcCLrvUtcB15k8mkzDWPjY3FuXPnAABt2rTBkSNHvDrX7NmzsXjxYixduhSHDh3C9OnTUV5ernSxnzx5sl0zvOnTp6OgoAAzZ87E0aNHsWbNGixYsADp6enKPo8//jh+/vlnnD59Gtu2bcPNN98MtVqNiRMnevtRiYjIkVJabx/I51VJlVJB0CM+zE022t0cebfLzzVSRl4J5CNrHnOJZuSNRiN0OumzbNy4UemD06VLF5w/f96Xl0ZERC1M/7RoxIZqUVJVje0nL0obvVmCzqM58p52rQ9x/nqAh9WDTcDrOfI9evTAvn370LZtWwwaNAgvv/wytFot3n//fbRr186rc02YMAEXLlzA/PnzkZOTgz59+mDdunVKA7zMzEyoVNZ7DampqVi/fj1mzZqFXr16ISUlBTNnzsQTTzyh7HP27FlMnDgRFy9eRFxcHC6//HLs2LEDcXG1lFgQEZFnXNztPlsmIBlS13qVmylsLjPqSta7sva5cPXOyNehtP4Szch3794dixYtwpgxY7BhwwY899xzAIBz584hJibGx1dHREQtiVolYFT3RCz7LRPr9p/HlZ3ipO8beQc9W4LORdWgHaW03kVG3l1pve13FR/zOpD/17/+hfJy6cvPs88+ixtvvBFXXHEFYmJisGLFCq8vYMaMGZgxY4bT15zNuR8yZAh27Njh8nzLly/3+hqIiMhDyiBpf3M0s1TEQAAqiFIw62oABKzBsKt15CECJoPzrLYo2gTLdc3IW0rr9exa785LL72Em2++Ga+88gruvfde9O7dG4C0HKxcck9ERNRQbuiZhGW/ZWL9gVw8N86MAHkJOk8CeXkfV2vIA9ab9cYKwFQNqB3CYbfN7prxOvKjRo1Snnfo0AGHDx9GQUEBoqKilM71RETUQrkYJE8VW3uXwFBReyDvao687THGSufBsG3ZfUPPkZfvzrNrveKqq65Cfn4+SkpKEBUVpWx/8MEHERzsphcCERGRlwa1jUZUsAYF5QbsPFWAoUppvSeBvJxs8GCOPAAYSoGgKPvX3S0/p9zYb2Zz5I1GIwICArB//3677dHR0QziiYguBS4GycwiA/SiRvpFHgRdcdW1Xq0FINjv48i2lK2x5siza72isrISer1eCeLPnDmDN998E0eOHEF8fC0ZDyIiojoIUKtwXTcpC//D/hxAyci7mSMvijbT/2qZUh2gtd5sd2x4Z6qWKgIB9xl50QSYjLVfUyPzKpDXaDRo3bp1o64VT0REfqqWQTKzoAKVsKwe4rbrvIt15AXBfcAs3wFXBdQsh/OUy0C+ltL6SzQjP27cOHzyyScAgKKiIgwaNAivvfYaxo8fj/fee8/HV0dERC3R9T2l4H3dgRyYQzzMyOtLAJPlZnptpfWA6yXobL97uJsjD/g8K+911/q5c+fin//8JwoKPFjLj4iIWo5aBsmzBRWogCUwNzoEyI7krLWzQVIO7l0FzPXtWA/YBPI2pfXGKutnc1Zaf4lm5Pfs2YMrrrgCALBq1SokJCTgzJkz+OSTT/DWW2/5+OqIiKglGtY+FmGBAbhQqseRMsv46y4jL3es14a5LouXuepcLy89B8H1DXm7QN638+S9Tme88847OH78OJKTk9GmTRuEhNi35t+zZ0+DXRwREfkRuazeYZAs11fjYrkBlVqdVBnv6TrwzgbJgCAAhe4z8nWdHw/YzJG3ueGgdK8VpM9X47ouzYx8RUWFsuTsjz/+iFtuuQUqlQqDBw/GmTNnfHx1RETUEmkDVBjZLQFf7cnGxrMCugLuM/LK0nMerFTmqnO9baM7V9PGBQFQ66Sb/z7uXO91ID9+/PhGuAwiIvJ7Lsrqswqlgc+gkoNdN3Pkla71TrLWGjcBs3JsQ2TkbQJ5uaxeFwaonBSrKWvce5qRbxmBfIcOHfDNN9/g5ptvVpZ/BYC8vDyEhzuZgkBERNQARvdIwld7svH1sWo8DEjVfvoyQBfq/IByOZCvpdGdzFVpvbul52QBgVIg39wy8k899VRjXAcREfk7F4Nk5kUpcBcDgoBq2JSmuVDb8nHyNlfzzuRAuj4ZeZ2TrvV6S6M7Z2X1ttflNiPvYv5/MzV//nzcddddmDVrFq655hoMGTIEgJSd79u3r4+vjoiIWqorOsYiRKvGyRITTKEhUFeXSwkFV4G8nJEP8SAjL4/1tWXka6MJlL43uLu538i8niNPRESXKBeDZFahNJAJWjnYrcc8ciXz7S4jX49A2dnyc7V1rPfkugDAbLZ2u20hGfnbbrsNmZmZ2LVrF9avX69sv/baa/HGG2/48MqIiKglC9SocW1XKXFQpI6WNpbWMk++rA4Zeceu9e6WnpP5yVryXmfkVSpVrUvNsaM9EVEL5WKQzCqQBj6VLhSogAfN7uSstas58nB9M6C6IZvdOSmtd9ax3pPrAqzN8oAWk5EHgMTERCQmJuLs2bMAgFatWmHgwIE+vioiImrpRvdIxOp955BpDEMMUHvDO2X6nwdLozZEaT3g8671XgfyX3/9td3vRqMRf/zxB5YuXYpnnnmmwS6MiIj8jFJabz9IyoG8NtASIHuakXdWHq/c5XaTka9XszsngbxcXueqtN6TjLztay0kI282m/H888/jtddeQ1mZVMEQFhaGxx57DHPnzoXKWT8BIiKiBnBV53gEadQ4awxHXzVqb3hXbmnI61Eg76Jrvael9Z42wG1kXgfy48aNq7HttttuQ/fu3bFixQpMnTq1QS6MiIj8jIvS+kxLIB8YbClZd9fsrrbO7hp3GfmGaHZnc51mE6BSuy+t9yQjL9+gEFTSOvctwNy5c/Hhhx/ixRdfxLBhwwAAW7duxdNPP42qqiq88MILPr5CIiJqqYK0alzVOQ4XDkdKGzzJyLtbQx5w3bXe4GUg39wy8q4MHjwYDz74YEOdjoiI/I2T0npRFJWu9cGhliDYXbO72ua5uxscG2T5OZtlU40V0oDurrRefj/RBJiMgFrj+toCAl0vW9PMLF26FB988AFuuukmZVuvXr2QkpKChx56iIE8ERE1qtE9k/DnoRgAgHjoOwiDHwLCEmvuKC+RW6+u9XIg7yZZ4EmVXhNokJq4yspKvPXWW0hJSWmI0xERkT8qq1laf7HcgCqjGYIAhIRYBsbastaiaJ1L7iyr7rbZXQPMkQ8IlLLmgLW83l1pve37uawWqKWJXzNVUFCALl261NjepUsXFBQU+OCKiIjoUnJNl3j8IFyOs2IshIKTwMdjgJJz9juJos30v/p0rZfnyDePjLzXgXxUVBSio6OVn6ioKISFhWHJkiV45ZVXGuMaiYjI1+wGSZtAvkzq0h4ZpEGAMke+loy83TxyZxl5N8u8NURGXhAArXw33tK53m1pvQ6AYH8Nrq6tPjcZ/Ezv3r3xzjvv1Nj+zjvvoFevXj64IiIiupSE6gLQtWMn3Gn4F4p1ScDF41IwX5xt3amqyLpqjDel9fXuWt/MSuvfeOMNu671KpUKcXFxGDRoEKKiohr04oiIyE9UFTsdJC+WSVno6BCt9Q62x4F8bc3uXGS9GyIjD0jl9fpi6xJ07krrBUG63urKSyoj//LLL2PMmDHYuHGjsob89u3bkZWVhbVr1/r46oiI6FJwY68kbDyUi6l4Gisjnrdm5u/7HohoZS2r10V4dqPfbWm9u0De8h2kuS0/d9999zXCZRARkV+Ty+p14XaD5MVyKbiPCdW5b1QH2DeEczbPXNMEGXmgZud6d6X18ntWV3qQkW8ZHesB4Morr8TRo0fx7rvv4vDhwwCAW265BQ8++CCef/55XHHFFT6+QiIiaulGdU9EmC4Au4rDsHv0MvTffDdQeEoK5u/93mbpOQ/K6gGbrvWuSuvdLT+ns9/fR7wurf/oo4+wcuXKGttXrlyJpUuXNshFERGRn3Gx9JyckY8J0QIaJ8u6OVIy6i4awinzzpogIw9Yr7WqSHrU1RLIu13jvuVl5AEgOTkZL7zwAr788kt8+eWXeP7551FYWIgPP/zQ15dGRESXgCCtGjf1SQYAfHLIDNy3FohKAwpPS8F89m5pR08a3QGuM/Jed633bUbe60B+4cKFiI2NrbE9Pj4eCxYsaJCLIiIiP+NiWZcCJSOv9S4j7ypr3WQZecsSdJ6W1tu+5yWUkSciIvIHEwakAgDWHchBsTYRuG8NENUWKDoDbHxK2inEw4y8PNZXVwHVBut2T0vrNW6SDk3E60A+MzMTbdu2rbG9TZs2yMzMbJCLIiIiP6Ms6+KQkbcE8tEhOu/myLsKdt11gm2sjLwnpfWXaEaeiIjI13qmRKBLYhgM1WZ8uy9bmht/3xogup11J08z8nLDW8A+K+9xaX0zzcjHx8fjzz//rLF93759iImJaZCLIiIiP+OytF4K5GNDtdYurx4F8i6C3aZYRx6wD+RF0ZqRd9W13vY9mZEnIiJqUoIg4I7+Ulb+i11Z0saIFEsw3176PbpmstkpdYA1+WA7T95oubnvtrS+mXatnzhxIh555BGEhYVh+PDhAICff/4ZM2fOxJ133tngF0hERH7ATWm91LXek9J6ORB3cbdbDpZdldY3VEZeJ5fWl1qCeZP0e22l9W4z8m5uUjQjt9xyS62vFxUVNc2FEBERWdzcNwUv/nAY+7NLcOBcMbonRwDhycDUDcDxjUDXsZ6fTBcuJR6cZeTdLj/nZhpgE/E6kH/uuedw+vRpXHvttQgIkA43m82YPHky58gTEbVULkrr88ttl5/zZB15N+XnypIuboLlBpsjX269G68KqP0uvNuMvJv5/81IREQtUwwsr0+ePLmJroaIiAiICtFiZLcErPnrPL74PQvPjLOMVSExQO8J3p1MFwaU5Thk5L3sWt/cMvJarRYrVqzA888/j7179yIoKAg9e/ZEmzZtGuP6iIjIHyhLuzjPyMfaLj9nqCWQt+1a70xTZeRtS+uriqXnunDnnfRl8jVfAhn5jz76yNeXQEREVMMdA1Kx5q/z+GbvOcy5oSsCNeq6nchZ53qPm90103XkZR07dkTHjh0b8lqIiMhfldfMyFebzCiqMAKwZOQFy8Bn0gNmE6ByMri6y1orGXl389DrGSzbBfIedKwHbAbulp+RJyIi8keXd4hFckQgzhVX4ceDubipd3LdTiSP+baBvNfLzzWzrvW33norXnrppRrbX375Zdx+++0NclFERORHRBEoszS7s5kjX1AhZeMFAYgK1trPKXNVXl/tYUbeZdd6N3PsPWW7/JwnHeuBSyojT0RE5I/UKgG39WsFAFgpN72rCzkjL1flATal9Z42u2tmXeu3bNmCG264ocb20aNHY8uWLQ1yUURE5EcqCwGzlHm3XaNVLquPCtZCrRIsga6lNL2uS7S5DZbd3AjwlKvS+towI09ERORzt1u61289no+zhbVM56uNzklGXimtdzdH3s33gSbidSBfVlYGrVZbY7tGo0FJSYmTI4iIqFmTy+oDI+yazMlLz0WHWMYEQXC/lry7rvVul5/T1368p5wF8g2WkWcgT0RE1FhSo4MxrEMMRBFYuets3U7iGMibjNakhduu9ZZkhI+71nsdyPfs2RMrVqyosX358uXo1q1bg1wUERH5ERdLz120ZORjQmxu7rpreGd0U35uu4SdKDo5vqEy8pbSer0XpfUeZ+RZWk9ERNSY5DXlV+0+C7PZyfcFd5Rmd5bvALYJCI/nyDezrvXz5s3DLbfcghMnTuCaa64BAGRkZOCzzz7DqlWrGvwCiYjIx+T58aEJdpsLyqTANSbUNpCXM/J1zFor20XAZLAPis0m693yhpwj72lpvZKRd9eIjxl5IiKixjSqeyLCAwOQXVSJX0/k44qOce4PsuXYtV7+3iKoAHXN6nM77vr5NBGvM/Jjx47FN998g+PHj+Ohhx7CY489huzsbPz000/o0KFDY1wjERH5ktKx3n6QtGbkbYJtrbvSejfzyG0DdMcB0vbmQIPOkfe2a72LmxTuqg2IiIioQQRq1BjfNwUA8EVdyusdu9YbyqVHTUjtS9ECfpOR9zqQB4AxY8bg119/RXl5OU6ePIk77rgDjz/+OHr37t3Q10dERL7mprQ+2llpfV271qu1sDbMcxggbQfMhgzkve5az4w8ERGRr8nl9esP5KDIspKOx5Su9XJpvdyx3oOKv+batV62ZcsW3HvvvUhOTsZrr72Ga665Bjt27GjIayMiIn9QVnMNeQC4WGtpvZuMvMZFsCsIcLk+qzzIqnWAqs7Dl0QurTeWA5VF0nOPu9bXsSM/ERERNZgeKRHolhQOQ7UZ3/yR7d3BSrO7ugTyNj1znPXzaSJefRPKycnBiy++iI4dO+L2229HeHg49Ho9vvnmG7z44osYMGBAY10nERH5ipyRdwjkC5yV1suBvKtmd55krTUuMt9Kx/sGyHjLGXkAKM2RHt2V1jMjT0RE5FcmDJCy8l6X1zt2rVeWnnPT6A6wv2Hvw6y8x4H82LFj0blzZ/z555948803ce7cObz99tuNeW1EROQPyp03u6tTab0n88gDXGS+lY719Wx0B0jXKViGwNJz0qPHXeuZkSciIvIH4/okQxugwsHzJdifXez5ga661rtbeg6ovZ9PE/I4kP/hhx8wdepUPPPMMxgzZgzUanVjXhcREfkLubQ+xKHZnWUd+Vjb0no50+22a30twbjSDdbhLndDZuQFwVpeX3FRemTXeiIiomYlMliL67pJiQavyutrdK33IiOvCrAmA5pDIL9161aUlpaiX79+GDRoEN555x3k5+c35rUREZGviaI10A2OUTYbTWYUV0pLwXnX7M6DrHWAzVrythoyIw/Yl9cDzMgTERE1Q2N6JgEANh7KhejpnHV5Op3JIN2g92aOvF0/n2YQyA8ePBiLFy/G+fPn8be//Q3Lly9HcnIyzGYzNmzYgNLS0sa8TiIi8gVjBWCyBKjB0crmQktZvUqQ7oYr3Da7c9O1HrDpBtuIc+QB7wN5TzPy9V3jvoV69913kZaWhsDAQAwaNAg7d+6sdf+ioiKkp6cjKSkJOp0OnTp1wtq1a53u++KLL0IQBDz66KONcOVEROTPrugUB61ahdMXK3DiQrlnB8lVeYCUlTd4kZEHbAL5ZjBHXhYSEoL7778fW7duxV9//YXHHnsML774IuLj43HTTTc1xjUSEZGvVBZKjyqN3aAnz4+PCtZCrbJZb1UJ5N1krWsLxjU+ysiza32jWbFiBWbPno2nnnoKe/bsQe/evTFq1Cjk5eU53d9gMGDkyJE4ffo0Vq1ahSNHjmDx4sVISUmpse/vv/+O//73v+jVq1djfwwiIvJDoboADGkvVQ1uOJjr2UEqtfV7jb7Eu9J6wObmvovvBE2gXuv3dO7cGS+//DLOnj2Lzz//vKGuiYiI/IUcyAdFSaVkFgXOGt0B1mDXVdd6oycZeRflag2ekbe5Gx8QCARoXe9re12cI++1119/HdOmTcOUKVPQrVs3LFq0CMHBwViyZInT/ZcsWYKCggJ88803GDZsGNLS0nDllVeid+/edvuVlZVh0qRJWLx4MaKioprioxARkR8aYZknv/GQh4E8YL8EnTel9YBfrCVfz4V4JWq1GuPHj8fq1asb4nREROQvbAN5G/nO1pAHPF9HvrastS8y8u7K6m2vy6QHzGb718wmwGy0XB8DeVsGgwG7d+/GiBEjlG0qlQojRozA9u3bnR6zevVqDBkyBOnp6UhISECPHj2wYMECmEwmu/3S09MxZswYu3O7otfrUVJSYvdDREQtw4iu0hK5ezILle8obtk2vDNaSvI96VoPuK/SawINEsgTEVEL5SKQd7qGPGAdAF0G8h50rXc170w5toFK120DeXdl9bbXZXstyu8218rSejv5+fkwmUxISLBfvjAhIQE5OTlOjzl58iRWrVoFk8mEtWvXYt68eXjttdfw/PPPK/ssX74ce/bswcKFCz26joULFyIiIkL5SU1NrfuHIiIiv5IUEYSeKREQReCnw86nbdVgF8jLGXlPS+tbSEaeiIhaKBeBvLz0nPcZeU/WkZcDeRcZ+YZqJmdbWh/oQSBf27qxtr+rGcjXl9lsRnx8PN5//33069cPEyZMwNy5c7Fo0SIAQFZWFmbOnIlly5YhMNCzCog5c+aguLhY+cnKymrMj0BERE1sRFfphrHH8+Tlsb+qLqX1vu9aH+CzdyYiIv/nKpB3OUfeXbM7D+aRa1zMRVfK8hthjrwnpfUqtdT0z2ys+fnka1MFAGoOrbZiY2OhVquRm2v/xSo3NxeJiYlOj0lKSoJGo4FarVa2de3aFTk5OUqpfl5eHi677DLldZPJhC1btuCdd96BXq+3OxYAdDoddDreZCEiaqlGdIvHGxuP4pdjF1BlNCFQo679ANuMvMFSWq8Jcb2/LXd9c5oAM/JERORaRYH0WKO03jJH3ptmd6ZqwFxtv58zrjLy1Q2dkfeytN72vV1l5Dk/vgatVot+/fohIyND2WY2m5GRkYEhQ4Y4PWbYsGE4fvw4zDa9CI4ePYqkpCRotVpce+21+Ouvv7B3717lp3///pg0aRL27t1bI4gnIqKWr1tSOFIig1BlNOPX4/nuD1AC+eaZkWcgT0RErskZ+WBXpfUOGc7aSuttBzuPmt05DI7GBg6W7ZrdeRjIu1puhkvP1Wr27NlYvHgxli5dikOHDmH69OkoLy/HlClTAACTJ0/GnDlzlP2nT5+OgoICzJw5E0ePHsWaNWuwYMECpKenAwDCwsLQo0cPu5+QkBDExMSgR48ePvmMRETkW4IgKE3vPOper7NU49ktP+dhIK9x0c+nCbH+j4iIXHPT7K5GaX1tze7sGsJ5svycq4y8j0rrbd+bGXmvTJgwARcuXMD8+fORk5ODPn36YN26dUoDvMzMTKhU1txCamoq1q9fj1mzZqFXr15ISUnBzJkz8cQTT/jqIxARUTMwolsClm4/g42H8vCCWYRKJbjeuV7N7lx8V2lCDOSJiMi1yiLp0cXyc7Eum905GdjkYFelkeabu6KUrzvc5VYy8o1RWu9hIC+/NzPyXpsxYwZmzJjh9LXNmzfX2DZkyBDs2LHD4/M7OwcREV1aBrWNQaguABdK9dh3tgh9W0e53lmuxtOXWhMQni4/x671RETk15xk5I0mM0qqpLnu0Y7Lz3lSWu8uay0PjjWC5QbOyOu87Fpv+97MyBMREfkdbYAKV3aOA+BBeb2cka+yLa33NJB30TOnCTGQJyIi15wE8oWWsnqVAEQGaez3l7Pp5mqg2mD/mjzYuQvEXQ2OjZqR93SOPDPyRERE/mykZRm6jQfdrCfvtLTe02Z3ctKBgTwREfkjJ4F8fpl1fnyNuWe2d7Ids/KeZq1dLj/n4Y0AT3GOPBERUYtzded4qFUCjuSWIvOikwpBmc6mtN7gbUaeXeuJiMhfGSut5ew2gbzLRncAEKCV1lKXj7c7nxzsuslaKxl5x+Mr7V+vrzp1rWdGnoiIyJ9FBGswMC0agJvyeiWQL/a+tN7Vjf0mxECeiIick7Pxgtqu9Pyisoa8i6DV1Tz5ag9L410t6dLgGfm6rCPPjDwREZG/G9FNKq/fcLC2QN5SWl9RAIgm6TnXkSciombPtqxesJbQy2vIRzt2rJcp68C7CuTdZeRdrNXe4Bn5OpTWu8zIe/jZiIiIqNHJ68nvPF2A4gqj853kajxDmXWb7U3+2rBrPRER+S03a8jHOiutB6wZeUMd58i7usvdmHPkgyI9O4YZeSIiIr/XJiYEnRJCYTKL2HzURdM7OSMvUwUAao3zfR25urHfhHweyL/77rtIS0tDYGAgBg0ahJ07d9a6f1FREdLT05GUlASdTodOnTph7dq19TonERE54SKQl0vrayw9J3NZWm+5a+0uENe4GBwbPCMfDFw7H7h6LjPyRERELcxId+X1mhAANk17PZ0fDzAjv2LFCsyePRtPPfUU9uzZg969e2PUqFHIy3N+18RgMGDkyJE4ffo0Vq1ahSNHjmDx4sVISUmp8zmJiMgFV4G8u9J6rRzIuwrE/SQjDwBXPAZc+X+e7+8yIy83u2NGnoiIyB+MsCxD9/ORCzBUm2vuoFLZ98jxKpC/xOfIv/7665g2bRqmTJmCbt26YdGiRQgODsaSJUuc7r9kyRIUFBTgm2++wbBhw5CWloYrr7wSvXv3rvM5iYjIBZcZeXel9a7myHvY2d02Iy+K0nNRbPiMfF0wI09ERNQs9G4VidhQHUr11fjt1EXnO9mW13va6A64tLvWGwwG7N69GyNGjLBejEqFESNGYPv27U6PWb16NYYMGYL09HQkJCSgR48eWLBgAUwmU53PCQB6vR4lJSV2P0RElzw3c+SdLj8H1L9rvZLVFgGTpUGNySD9Dvg2WGZGnoiIqFlQqQSl6d1GV+X1doE8M/Ieyc/Ph8lkQkJCgt32hIQE5OTkOD3m5MmTWLVqFUwmE9auXYt58+bhtddew/PPP1/ncwLAwoULERERofykpqbW89MREbUAFQXSY43Sesvyc6Hu5sjXMWttGwzLa8nbDpTe3DFvaK466rPZHRERkd+Ry+s3HsqDKFf52Qq0La334vuFEshfonPkvWU2mxEfH4/3338f/fr1w4QJEzB37lwsWrSoXuedM2cOiouLlZ+srKwGumIiomZMzsgHRyubDNVmlFRVAwBi3HatL7ffrsxxd5eR10FpPmOssn+EAKhdvG9TkK+dGXkiIiK/d3nHWARqVMguqsTB806qrm0z8to6ZOR92LU+wFdvHBsbC7Vajdxc+zKH3NxcJCYmOj0mKSkJGo0GarVa2da1a1fk5OTAYDDU6ZwAoNPpoNNxXiMRkR0npfWFFVJZvVolICLIxRItrprdeTpHXhCkAbK60iYjb3nUBNmtad/k3GbkOZYQERH5i0CNGpd3iMXGQ3n4+egFdE92WKWmzqX1l3DXeq1Wi379+iEjI0PZZjabkZGRgSFDhjg9ZtiwYTh+/DjMZmvXwaNHjyIpKQlarbZO5yQiIhcqi6RHmzXW8y1l9VHBWqhULgJqV83uPO1aD1jnojtm5H2d8WZGnoiIqFkZ3ikOALDl6IWaL+rqWFqvfB+4RNeRnz17NhYvXoylS5fi0KFDmD59OsrLyzFlyhQAwOTJkzFnzhxl/+nTp6OgoAAzZ87E0aNHsWbNGixYsADp6eken5OIiDzkJCMvN7pzWVYPuF9H3pNgN8AhYLbNyPtSgMMNBhnnyBMREfml4R2lQH73mUKU66vtX7TLyId4flJ5vDdXA6bq2vdtJD4rrQeACRMm4MKFC5g/fz5ycnLQp08frFu3TmlWl5mZCZXKeq8hNTUV69evx6xZs9CrVy+kpKRg5syZeOKJJzw+JxEReai2QN7VGvJALc3uvMjIKyVr/pqRr+O0ASIiImpSbWKCkRodhKyCSuw4eRHXdrWJC+uakbcd7016QN30YbVPA3kAmDFjBmbMmOH0tc2bN9fYNmTIEOzYsaPO5yQiIg9U6wGjpVmdTSCfX+Zm6TnAOhDWaHbnRbBru5Y8wIw8ERER1YkgCBjeMQ7LfsvEL8fy7QP5+natB6TvBFovsvkNpFl1rScioiYiZ+MhADprY5iCcikYj3W19Bzgfvk5TwZKx/VZmZEnIiKiOrqio4t58nZd670IxlVqQGVp+uujteQZyBMRUU1KWX0kYDPF6aInGXlXXeuNXnR2Z0aeiIiIGsjQDjFQqwSczC9HVoFNDx+7OfJefsdwTDo0MQbyRERUkxLIR9ttvljuSWm9HMi7WEfeoznyzSAjL4rW7czIExER+a3wQA36pkYCAH45lm99wW6OvBfLzwHWFXYYyBMRkd9w0ugOsDa7i61TsztvutY7DI5KRt7Hgbx8XaIZMBmt25mRJyIi8mtyef0vx2zK6+sTyDMjT0REfsdFIH/Rso58dIgHc+QNjsvPNcQ68j4urbctu7OdJ8+MPBERkV8b3ikWAPDr8XxUm8zSxgYprdc3wNV5j4E8ERHV5CqQ92j5OXl+u6t15D0IdgMcmsr5S0ZerQUgSM9t58kzI09EROTXerWKRESQBiVV1dh3tljaGNgAGXnHCsQmwkCeiIhqchLI66tNKK2qBgDE1KXZnTdd6/01Iy8INTvXm6oBs/R3YUaeiIjIP6lVAi7vIGXllfL6emXkLWM+M/JEROQ3KgqkR5tAvrBcmhMeoBIQHqhxfawyR77CviGcN13rlXI1OSMv3wTwg4y34x14k77ma0REROR3rugoBfLKMnSaYEBQS8+9XQve1ZK0TYSBPBER1eQkI3/RsoZ8VIgWKpXg+ljljrZoDcBFm+eeZNWV8nw5I+/F/PrGVmNpPNtAnhl5IiIif3VFJ6nh3d6sIhRXGqVKu6BI6UVtqHcnY0aeiIj8jhzIB1uXn5PXkK+1rB6wn2OmZK2NACzZeY8y8vLgKHet96LjfWOr0VHf8qjSACq1b66JiIiI3EqJDEL7uBCYRWDbccsydCOfA4bMAOI6e3cydq0nIiK/4yQjX+BJoztACmbVlkDcYFlL3rbszKPl5+RyNcfl53w8R972GuSbFP5ULUBERES1Gm7Jym+R15PvOwkY9YKUnfdGgEM/nybGQJ6IiGqqLJIebQL5fE+WnpM5Nrzztvxc4zAP3ehHXeFrZOS59BwREVFzMdyynvyWoxcg2vby8RYz8kRE5Hdqy8i7K60HbBreWTLytllrT+54N6eMPJeeIyIiajYGtYuGVq1CdlElTuWX1/1EnCNPRER+xWQEDKXS8zoH8i4awnmatXa5/JwfBMvMyBMRETVbwdoA9E+Tvt/8IpfX1wW71hMRkV+Ry+oBIDBCeZpvaXYX7W6OPGC/BB3gXcd6wEmw7E8ZeYeyf2bkiYiImpUrbMrr64wZeSIi8iuVljXkAyPsurAXWJafi/FkjrwcyBscA3kPs9aOgbxfZeQdy/4tA7g/rHFPREREbg3vJK0nv/3kRRiqzXU7ieP3gSbGQJ6IiOw5mR8PABc97VoPOGl252UgXqM03x8z8g7Lz/nDTQYiIiJyq2tiOGJDtagwmLD7TGHdTiInJ9i1noiI/IISyEfbbS7wdB15wElpvZdZ62aRka/j/H8iIiLyKZVKUMrrfzlWx/J6dq0nIiK/4iQjr682oVRfDcDT0no5o24J5L1da50ZeSIiImpEV3SUyuu31DWQ1zCQJyIif1LL0nMBKgHhQQHuz+EqI9+i5sgzI09ERNRcXW4J5Pdnl+BiWR0a1jEjT0REfsVJIH9R7lgfooXgyTrwLpvd1aFrvdkMmOTSfGbkiYiIqP7iwwLRNSkcALD1eB2WoWPXeiIi8ivOAvlyayDvkRql8V52rZeDZdFsXdMe8I9gmRl5IiKiFmG4XF5/tC6BvMN3nSbGQJ6IiOxVWJafsyutl4LV2FAPg1Wti3XkPc2o22buK226yTIjT0RERA1keCdrwztRFL07mBl5IiLyK25K6z1SY468t+vI6wAI9tej0tita+8zNTLyXn42IiIi8gv92kQhUKNCXqkeR3JL3R9gi3PkiYjIr8iBc7B1+Tmv1pAHbAJ5S7DrbbM6QbDuK1+PP2TjAScZebm0nhl5IiKi5iRQo8bAtjEAgF+PX/TuYHatJyIiv+Ksa703a8gDNs3uyqXHupSfyxnuyiL7332NGXkiIqIW4/IOciDv5Tx5ZuSJiMivyIGzXbM7Kesc7cka8kAtze68COTlc1RZrsfTjveNjRl5IiKiFmNoe6nh3W8nL8JoMnt+oBLIc448ERH5mqka0BdLz510rfe4tN5VsztvstY1Suv9JFBmRp6IiKjF6JYUjqhgDcoNJuzLKvL8QPl7irES8LZRXgNgIE9ERFZVxdbngZHK04LyOpbWK4F8HdaBl/eVA3l/yXgzI09ERNRiqFQChnaQsvJerSev3MAXAZOx4S/MDQbyRERkVWlZek4XDqgDlM1y1/oYT5efcyytlx/rlZH3k9J6JSPP5eeIiIhagmGW8vpt3jS8s/1eUt30a8kzkCciIiul0V2ksqnKaEKZvhqAN8vPhUiPjhn5usyRV5rd+UmgrLEppQNsPhtL64mIiJqjyy0Z+T2ZhSi3fOdxS23zncgH8+QZyBMRkZWzjvWWsnqNWkB4YICzo2qSg3CD4xz5unSt99OMvNkImE3MyBMRETVzrWOC0SoqCNVmETtPF3h2kO1SuT7oXM9AnoiIrJRA3rqGvBzIR4doIQiCZ+fRWjLyJn3dg90AP8/IA1JWnhl5IiKiZk/Oyv96zJt58g59c5oQA3kiIrJykpG/qATyXgSqttlzY0Udl5/z8znygPS5mJEnIiJq9obVqeEdM/JEROQPnAXyZVLGOdbTpecAy8Bmyd7bZq29WUIuwE+71qtU1nlxxkouP0dERNQCDG0fAwA4nFOK/DIP57zLYz/nyBMRkU/VMkfe40Z3gDRvzHYJOmM9MvJyJ1h/ycgD9p3rmZEnIiJq9mJCdeiaFA4A2HbCw+718ncTdq0nIiKfchLI58tLz3lTWg/YN7yr0xz5wNp/9yXbzvWcI09ERNQiDLNk5bd5Wl7PjDwREfmFCkunVruMvDQ4xXhTWg8AWjkjX9kwgbxfZeRt5sQpn82Prs8Pvfvuu0hLS0NgYCAGDRqEnTt31rp/UVER0tPTkZSUBJ1Oh06dOmHt2rXK6wsXLsSAAQMQFhaG+Ph4jB8/HkeOHGnsj0FERC3YsI7SPPlfjuVDFEX3B9hW6DUxBvJERGTVUKX1gH1pfV3mkTvOp/erjLxl4NaXAKJZes6MvEsrVqzA7Nmz8dRTT2HPnj3o3bs3Ro0ahby8PKf7GwwGjBw5EqdPn8aqVatw5MgRLF68GCkpKco+P//8M9LT07Fjxw5s2LABRqMR1113HcrLy5vqYxERUQszMC0aGrWA7KJKZBZUuD9AHvt90LXewwWBiYjokiAH8sHW5eespfXeBvJyaX0ZYDLYb/OEY4bbHzPy8tJ4ttuohtdffx3Tpk3DlClTAACLFi3CmjVrsGTJEjz55JM19l+yZAkKCgqwbds2aDQaAEBaWprdPuvWrbP7/eOPP0Z8fDx2796N4cOHN84HISKiFi1EF4C+qVHYeboAvx6/iDYxIbUfwK71RETkF2rJyHtdWq8JsT8n0PIy8lVF1m3MyDtlMBiwe/dujBgxQtmmUqkwYsQIbN++3ekxq1evxpAhQ5Ceno6EhAT06NEDCxYsgMlkcvk+xcXFAIDo6Ginr+v1epSUlNj9EBEROZKXofvVk3nyGgbyRETka2YTUCUFQ3Igr682IadYGpziw7wMpOVgV553D7S8OfJyRl6tkzr1Uw35+fkwmUxISEiw256QkICcnBynx5w8eRKrVq2CyWTC2rVrMW/ePLz22mt4/vnnne5vNpvx6KOPYtiwYejRo4fTfRYuXIiIiAjlJzU1tX4fjIiIWqTLO1oa3p3Ih9nsZp48M/JERORzVcUALANWYCQAYH92MQwmM2JDtWgV5WUgrXFYB15QA2qN58fX6FrvRxlvx4y8P1ULtABmsxnx8fF4//330a9fP0yYMAFz587FokWLnO6fnp6O/fv3Y/ny5S7POWfOHBQXFys/WVlZjXX5RETUjPVqFYkQrRqFFUYcPO+mekvpWs858kRE5CtywK0NBQKkMvpdp6Vt/dpEQfA246yVS+stGXlvg13HDLw/dYV3zMj7000GPxMbGwu1Wo3c3Fy77bm5uUhMTHR6TFJSEjQaDdRqtbKta9euyMnJgcFggFZrneYxY8YMfP/999iyZQtatWrl8jp0Oh10Ov47ERFR7TRqFQa3i0HG4Tz8ejwfPVIiXO8sfzfxQbM7ZuSJiEjiZH7875ZAvn8b5/OOa+VYWu9tsFujtN6Pst7MyHtMq9WiX79+yMjIULaZzWZkZGRgyJAhTo8ZNmwYjh8/DrPZrGw7evQokpKSlCBeFEXMmDEDX3/9NX766Se0bdu2cT8IERFdMobK8+RPXKx9Rx9m5BnIExGRRAnkIwFIgdKeTEsgnxbl4qBayMvPyef1do47M/ItxuzZs7F48WIsXboUhw4dwvTp01FeXq50sZ88eTLmzJmj7D99+nQUFBRg5syZOHr0KNasWYMFCxYgPT1d2Sc9PR2ffvopPvvsM4SFhSEnJwc5OTmorKxs8s9HREQty+WWQH7nqYvQV7tutGqdI69vgqtyeOsmf0ciIvr/9u4+OKr63uP4ZzfJbh4gDxDIA4SnglFQggZNV2pFyRWo04LFEWdoBTvKgImD0vaqtxWwHQfa3oGpHQbtA2CnHbB4i1o7gjSadEQQCCBgMYoiD0ISIhKSSBLM/u4fmyxZkkCy2eTsyb5fMztJzjl78j1fw3z97u+c3y88XTYi/2lVnc7WNcod7dS4zCvcVtYRfyPPiHykmz17ts6cOaMlS5aovLxcEyZM0JYtW/wT4B0/flxO56WxhaysLG3dulWPP/64xo8fryFDhmjRokV64okn/MesWbNGkjR58uSA37Vu3TrNmzevx68JANB3XZPWT6n93KqqbdC+4+f0zVED2z/QP2t973+ITCMPAPDxN/K+2+j3fOZrwHOykuWKDuIGLv+t9c3n7Wqz22ayuzBqlv0j8s2z/DMif1WFhYUqLCxsd19xcXGbbR6PRzt37uzwfMZcZSZhAACC5HA4NGn0QL26/5S2H6nquJG3cESeW+sBAD6Xjci3THR3czC31UutJrsLspG/fAQ+nJafa4mNEXkAAPqklvXk37nSevI8Iw8AsNzljfyxbkx0J11qvC/W+b52eUT+8mfkw6hZbomtoXlZGkbkAQDoU1oa+QMnq3W+/mL7BzFrPQDAcq0a+araBh2t8jXgNw0LckS+5Rl5/899cES+RTh9yAAAALptSHKcRqYmqMlr9N6nZ9s/iBF5AIDlWjXypc2j8dlp/ZUUHxPc+S5v5PviiLz/Z0bkAQDoayaN9j0bv72j2+tbBhl4Rh4AYJmW9d7jUvwT3eUG+3y81M7ycV2dtb7V8dGxksMRfCyhxog8AAB93qRvNK8n31Ej7x+R7/1Z62nkAQA+rUbkLz0f341GvmWyuxZdXQfe4bjUIIdbo8yIPAAAfZ7nGwPldEgfV9bq1Ll2mvVIn7V+9erVGjFihGJjY5WXl6ddu3Z1eOz69evlcDgCXrGxgf+DN2/evDbHTJs2racvAwDsrbmRb3Al6dDnvmXVbh4R5ER3UvdH5KVLBTKcno+XGJEHACACJMe7NCErWZJU8tGZtge4EqT4VCk2uVfjksKgkX/ppZe0ePFiLV26VHv37lVOTo6mTp2qysrKDt+TmJio06dP+1/Hjh1rc8y0adMCjtmwYUNPXgYA2JvX619K7YOzUbrYZDS4v1tDU7rRQHf3GXnpUgMfbo0yI/IAAESEydmDJUnFZe30p5k3Sv/9ifTQtl6OKgwa+ZUrV+rhhx/Wgw8+qLFjx+r5559XfHy81q5d2+F7HA6H0tPT/a+0tLQ2x7jd7oBjUlK6cXsoAPR1Decl45Uk7arwfZ04IkWO7jyX3t1Z6yVG5AEAgKUmZw+SJG0/8oUav/ZaHM0lljbyjY2NKi0tVX5+vn+b0+lUfn6+duzY0eH7amtrNXz4cGVlZWnGjBn64IMP2hxTXFyswYMHKzs7WwsXLtQXX3zR4fkaGhp0/vz5gBcARJSW5+Nj4vXecd+yc0GvH9+iza31jMgDAAB7uT4zSan9XKpt+Nq/qk84sLSRr6qqUlNTU5sR9bS0NJWXl7f7nuzsbK1du1avvvqq/vKXv8jr9erWW2/VyZMn/cdMmzZNf/7zn1VUVKRf/epXKikp0fTp09XU1NTuOZcvX66kpCT/KysrK3QXCQB20NzIm1ZLz03szoz1UjuT3QXzjHzze8KtkWdEHgCAiOB0OvTta3yj8u3eXm8Ry2+t7yqPx6MHHnhAEyZM0O23366///3vGjRokF544QX/Mffff7++973v6YYbbtDMmTP1+uuva/fu3SouLm73nE899ZSqq6v9rxMnTvTS1QBAmLjgW26uMSZR5+u/VlxMlK7LSOzeOaNiJGf0pZ+7Omt96/cEc1t+T2JEHgCAiHHpOfl2JryziKWNfGpqqqKiolRRURGwvaKiQunp6Z06R0xMjG688UYdOXKkw2NGjRql1NTUDo9xu91KTEwMeAFARLlwTpL0peknSbpxWLJiokJQIlo/Jx9Ms9vSwAfzIUBPuvyDhXB7hh8AAITMt8ekyumQyipq2l+GzgKWNvIul0u5ubkqKiryb/N6vSoqKpLH4+nUOZqamnTw4EFlZGR0eMzJkyf1xRdfXPEYAIhozbfWV1z0Nd7dWj++tYBGPpjJ7hiRBwAA1rrqMnQWsPzW+sWLF+sPf/iDXnzxRR0+fFgLFy5UXV2dHnzwQUnSAw88oKeeesp//C9+8Qu9+eab+vTTT7V371794Ac/0LFjx/TQQw9J8k2E99Of/lQ7d+7UZ599pqKiIs2YMUOjR4/W1KlTLblGAAh7zY38sa9ckqTc7qwf31rrkepgmvFwHZGPipEcrUooz8gDANCntdxe//aH4fGcfPTVD+lZs2fP1pkzZ7RkyRKVl5drwoQJ2rJli38CvOPHj8vpvPQ/S19++aUefvhhlZeXKyUlRbm5uXr33Xc1duxYSVJUVJQOHDigF198UefOnVNmZqbuuusu/fKXv5TbzYgJALSruZH/vCFWTod007Dk0Jy32yPyLcvPhVmj7HD4Ply46JvhnxF5AAD6tjuyB2vlto+0/UiVGr/2yhVt7Zi45Y28JBUWFqqwsLDdfZdPULdq1SqtWrWqw3PFxcVp69atoQwPAPq+5kb+nOmn7PRE9Y+NCc15XSFq5MNxxDsmtlUjH4bxAQCAkBmXmajUfi5V1TZqz7GzuvUbqZbGY/mt9QCAMNDSyKtf6J6PlwJvrQ+m2c3ICfwaTqK7eW0AAMA2Wi9DVxIGs9fTyAMApK98y8+dMwndXz++tZhWa8kHc/t57lzpv49K4+8LXUyh0vp2f26tBwCgz/M/Jx8G68nTyAMA5G1u5KvVTxNDNdGddNlkd0FOWBcfwnhCiRF5AAAiSssydB9V1Fq+DB2NPABATXW+Rj46YYCGJIdwhvjuriMfzhiRBwAgoiTHu3TjMN+di8UW315PIw8Akc4YRTVUS5JGZGWF9tzdnewunLW+nr52bQAAoF2Tm5+TL7b49noaeQCIdA01cqpJknTtyGGhPXd3J7sLZwHXxog8AACRoOU5+ZZl6KxCIw8AEa6p+fn4ehOjCaMyQnvygMnu+lgjz4g8AAARp2UZurrGJu357KxlcdDIA0CEO3byc0m+ie6uTe8f2pNHyoh8lMu6OAAAQK9xOh26/RrfqHzxR9Y9J08jDwAR7pPjJyRJja4kRUeFuCy0NLtRLsnZx0pOywcT0bGSw2FtLAAAoNdMzrb+Ofloy35zhGr82quPKmr0walqHfr8vA6dqtaZmgarwwIQwb751WH9l0NyxIVw/fgWruZb6/vaaLx06UMKno8HACCi3NZqGbrPz10I7Yo/nUQj38PeP3FOBz+v9jfuZeU1amyyblIEALhcbFS1FCP1Txkc+pP35Wa39Yg8AACIGC3L0JUe+1LFZZWakze812Ogke9hj27Yp+NnvwrYlhQXo+uHJOr6zCSNG5KkrJQ4ObgtE4BF0vbvlUqlpAE90cg3Lz8X3fufVPe4vvwhBQAAuKI7sgc1N/JnaOT7otvGpOrklxf8jfv1Q5I0lMYdQDj5T73va0/cWt9yztjE0J/baozIAwAQsSZnD9b/vvmR3j1SpYavm+SOjurV308j38OevecGq0MAgCu78KXva0808kMmSrc/IQ37ZujPbTVG5AEAiFhjMxKV2s+tqtoG7fnsS00andqrv59Gvqd9WiLVV1sdBQB07MyHvq890cg7ndId/xP684YDRuQBAIhYvmXoBun/9p5UcVkljXyfs+1p6fT7VkcBAFcXP8DqCOylZUb+lnkAAABARJmc3dLIn9HP7u7d300j39PSb+ibkzwB6FsSM6RvTLE6CnsZPUW6/l4p536rIwEAABb49phBGpIcp5uGpehik1cxUc5e+9008j1txmqrIwAA9IS4FOneP1kdBQAAsEhSfIzeeeIOSyYy772PDAAAAAAA6EOsWo2MRh4AAAAAABuhkQcAAAAAwEZo5AEAAAAAsBEaeQAAAAAAbIRGHgAAAAAAG6GRBwAAAADARmjkAQAAAACwERp5AAAAAABshEYeAAAAAAAboZEHAAAAAMBGoq0OIBwZYyRJ58+ftzgSAAB8WmpSS41C91DrAQDhpiu1nka+HTU1NZKkrKwsiyMBACBQTU2NkpKSrA7D9qj1AIBw1Zla7zB8tN+G1+vVqVOn1L9/fzkcjm6d6/z588rKytKJEyeUmJgYoggjA7kLDnkLHrkLDnkLTlfzZoxRTU2NMjMz5XTyZFx3hbLWS/w7CBZ5Cw55Cx65Cw55C15XcteVWs+IfDucTqeGDh0a0nMmJibyRx8kchcc8hY8chcc8hacruSNkfjQ6YlaL/HvIFjkLTjkLXjkLjjkLXidzV1naz0f6QMAAAAAYCM08gAAAAAA2AiNfA9zu91aunSp3G631aHYDrkLDnkLHrkLDnkLDnnrW/jvGRzyFhzyFjxyFxzyFryeyh2T3QEAAAAAYCOMyAMAAAAAYCM08gAAAAAA2AiNPAAAAAAANkIjDwAAAACAjdDI97DVq1drxIgRio2NVV5ennbt2mV1SGHl3//+t7773e8qMzNTDodDr7zySsB+Y4yWLFmijIwMxcXFKT8/Xx9//LE1wYaR5cuX6+abb1b//v01ePBgzZw5U2VlZQHH1NfXq6CgQAMHDlS/fv00a9YsVVRUWBRx+FizZo3Gjx+vxMREJSYmyuPx6I033vDvJ2+ds2LFCjkcDj322GP+beSufcuWLZPD4Qh4XXvttf795M3+qPVXR70PDvU+ONT60KDWd54VtZ5Gvge99NJLWrx4sZYuXaq9e/cqJydHU6dOVWVlpdWhhY26ujrl5ORo9erV7e7/9a9/reeee07PP/+83nvvPSUkJGjq1Kmqr6/v5UjDS0lJiQoKCrRz505t27ZNFy9e1F133aW6ujr/MY8//rj+8Y9/aNOmTSopKdGpU6f0/e9/38Kow8PQoUO1YsUKlZaWas+ePbrzzjs1Y8YMffDBB5LIW2fs3r1bL7zwgsaPHx+wndx1bNy4cTp9+rT/9c477/j3kTd7o9Z3DvU+ONT74FDru49a33W9XusNeswtt9xiCgoK/D83NTWZzMxMs3z5cgujCl+SzObNm/0/e71ek56ebn7zm9/4t507d8643W6zYcMGCyIMX5WVlUaSKSkpMcb48hQTE2M2bdrkP+bw4cNGktmxY4dVYYatlJQU88c//pG8dUJNTY0ZM2aM2bZtm7n99tvNokWLjDH8zV3J0qVLTU5OTrv7yJv9Ueu7jnofPOp98Kj1nUet7zoraj0j8j2ksbFRpaWlys/P929zOp3Kz8/Xjh07LIzMPo4ePary8vKAHCYlJSkvL48cXqa6ulqSNGDAAElSaWmpLl68GJC7a6+9VsOGDSN3rTQ1NWnjxo2qq6uTx+Mhb51QUFCgu+++OyBHEn9zV/Pxxx8rMzNTo0aN0pw5c3T8+HFJ5M3uqPWhQb3vPOp911Hru45aH5zervXR3Y4Y7aqqqlJTU5PS0tICtqelpenDDz+0KCp7KS8vl6R2c9iyD5LX69Vjjz2mSZMm6frrr5fky53L5VJycnLAseTO5+DBg/J4PKqvr1e/fv20efNmjR07Vvv37ydvV7Bx40bt3btXu3fvbrOPv7mO5eXlaf369crOztbp06f1zDPP6LbbbtOhQ4fIm81R60ODet851PuuodYHh1ofHCtqPY08YHMFBQU6dOhQwHM4uLLs7Gzt379f1dXVevnllzV37lyVlJRYHVZYO3HihBYtWqRt27YpNjbW6nBsZfr06f7vx48fr7y8PA0fPlx/+9vfFBcXZ2FkAOyEet811Pquo9YHz4paz631PSQ1NVVRUVFtZiOsqKhQenq6RVHZS0ueyGHHCgsL9frrr+vtt9/W0KFD/dvT09PV2Nioc+fOBRxP7nxcLpdGjx6t3NxcLV++XDk5Ofrtb39L3q6gtLRUlZWVuummmxQdHa3o6GiVlJToueeeU3R0tNLS0shdJyUnJ+uaa67RkSNH+JuzOWp9aFDvr45633XU+q6j1odOb9R6Gvke4nK5lJubq6KiIv82r9eroqIieTweCyOzj5EjRyo9PT0gh+fPn9d7770X8Tk0xqiwsFCbN2/WW2+9pZEjRwbsz83NVUxMTEDuysrKdPz48YjPXXu8Xq8aGhrI2xVMmTJFBw8e1P79+/2viRMnas6cOf7vyV3n1NbW6pNPPlFGRgZ/czZHrQ8N6n3HqPehQ62/Omp96PRKrQ96mjxc1caNG43b7Tbr1683//nPf8z8+fNNcnKyKS8vtzq0sFFTU2P27dtn9u3bZySZlStXmn379pljx44ZY4xZsWKFSU5ONq+++qo5cOCAmTFjhhk5cqS5cOGCxZFba+HChSYpKckUFxeb06dP+19fffWV/5gFCxaYYcOGmbfeesvs2bPHeDwe4/F4LIw6PDz55JOmpKTEHD161Bw4cMA8+eSTxuFwmDfffNMYQ966ovVMtsaQu478+Mc/NsXFxebo0aNm+/btJj8/36SmpprKykpjDHmzO2p951Dvg0O9Dw61PnSo9Z1jRa2nke9hv/vd78ywYcOMy+Uyt9xyi9m5c6fVIYWVt99+20hq85o7d64xxrckzdNPP23S0tKM2+02U6ZMMWVlZdYGHQbay5kks27dOv8xFy5cMI888ohJSUkx8fHx5p577jGnT5+2Lugw8aMf/cgMHz7cuFwuM2jQIDNlyhR/YTeGvHXF5cWd3LVv9uzZJiMjw7hcLjNkyBAze/Zsc+TIEf9+8mZ/1Pqro94Hh3ofHGp96FDrO8eKWu8wxpjgx/MBAAAAAEBv4hl5AAAAAABshEYeAAAAAAAboZEHAAAAAMBGaOQBAAAAALARGnkAAAAAAGyERh4AAAAAABuhkQcAAAAAwEZo5AEAAAAAsBEaeQBhyeFw6JVXXrE6DAAA0EOo9UDwaOQBtDFv3jw5HI42r2nTplkdGgAACAFqPWBv0VYHACA8TZs2TevWrQvY5na7LYoGAACEGrUesC9G5AG0y+12Kz09PeCVkpIiyXcr3Jo1azR9+nTFxcVp1KhRevnllwPef/DgQd15552Ki4vTwIEDNX/+fNXW1gYcs3btWo0bN05ut1sZGRkqLCwM2F9VVaV77rlH8fHxGjNmjF577bWevWgAACIItR6wLxp5AEF5+umnNWvWLL3//vuaM2eO7r//fh0+fFiSVFdXp6lTpyolJUW7d+/Wpk2b9K9//SugeK9Zs0YFBQWaP3++Dh48qNdee02jR48O+B3PPPOM7rvvPh04cEDf+c53NGfOHJ09e7ZXrxMAgEhFrQfCmAGAy8ydO9dERUWZhISEgNezzz5rjDFGklmwYEHAe/Ly8szChQuNMcb8/ve/NykpKaa2tta//5///KdxOp2mvLzcGGNMZmam+dnPftZhDJLMz3/+c//PtbW1RpJ54403QnadAABEKmo9YG88Iw+gXXfccYfWrFkTsG3AgAH+7z0eT8A+j8ej/fv3S5IOHz6snJwcJSQk+PdPmjRJXq9XZWVlcjgcOnXqlKZMmXLFGMaPH+//PiEhQYmJiaqsrAz2kgAAQCvUesC+aOQBtCshIaHN7W+hEhcX16njYmJiAn52OBzyer09ERIAABGHWg/YF8/IAwjKzp072/x83XXXSZKuu+46vf/++6qrq/Pv3759u5xOp7Kzs9W/f3+NGDFCRUVFvRozAADoPGo9EL4YkQfQroaGBpWXlwdsi46OVmpqqiRp06ZNmjhxor71rW/pr3/9q3bt2qU//elPkqQ5c+Zo6dKlmjt3rpYtW6YzZ87o0Ucf1Q9/+EOlpaVJkpYtW6YFCxZo8ODBmj59umpqarR9+3Y9+uijvXuhAABEKGo9YF808gDatWXLFmVkZARsy87O1ocffijJN8vsxo0b9cgjjygjI0MbNmzQ2LFjJUnx8fHaunWrFi1apJtvvlnx8fGaNWuWVq5c6T/X3LlzVV9fr1WrVuknP/mJUlNTde+99/beBQIAEOGo9YB9OYwxxuogANiLw+HQ5s2bNXPmTKtDAQAAPYBaD4Q3npEHAAAAAMBGaOQBAAAAALARbq0HAAAAAMBGGJEHAAAAAMBGaOQBAAAAALARGnkAAAAAAGyERh4AAAAAABuhkQcAAAAAwEZo5AEAAAAAsBEaeQAAAAAAbIRGHgAAAAAAG/l/ww9XxaUPZeAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 5ms/step - loss: 0.6149 - accuracy: 0.7206\n",
      "Best Model - Loss: 0.6149426698684692, Accuracy: 0.720634937286377\n"
     ]
    }
   ],
   "source": [
    "# Re-train the best model\n",
    "best_model = create_model(optimizer=best_params['optimizer'], init=best_params['init'])\n",
    "history = best_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], validation_split=0.33)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f'Best Model - Loss: {loss}, Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d3e7b67-cfe6-45d6-8885-bdc89b8a227d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 3ms/step\n",
      "Precision: 0.6568364611260054, Recall: 0.8361774744027304, F1-Score: 0.7357357357357357\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision, recall, and f1-score\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "print(f'Precision: {precision}, Recall: {recall}, F1-Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "99701e1f-5290-4e3a-9ae1-8a9f99226bad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630, 1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d53c11-e368-40d9-82c5-a9e9986ec4fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predicting on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9f77f3a-0f00-4ce7-8b8f-47d8f16759cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.374</th>\n",
       "      <th>GIST.375</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.791880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.43900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.024576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.177500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42297</td>\n",
       "      <td>2.025100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.039678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057236</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014737</td>\n",
       "      <td>0.013860</td>\n",
       "      <td>0.058389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.319860</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.026954</td>\n",
       "      <td>0.050490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021365</td>\n",
       "      <td>0.027606</td>\n",
       "      <td>0.031131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.881920</td>\n",
       "      <td>1.093600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.022733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79380</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.44172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.32580</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>1.544700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027531</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021376</td>\n",
       "      <td>0.016964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.72694</td>\n",
       "      <td>0.399110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.008311</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.4189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.46587</td>\n",
       "      <td>0.872840</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036006</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>0.051569</td>\n",
       "      <td>0.087066</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.34373</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.144660</td>\n",
       "      <td>0.30262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91524</td>\n",
       "      <td>0.76578</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.030069</td>\n",
       "      <td>0.010835</td>\n",
       "      <td>0.015817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.035659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.060526</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CNNs  CNNs.1    CNNs.2   CNNs.3    CNNs.4    CNNs.5  CNNs.6   CNNs.7  \\\n",
       "0        NaN  0.2334  0.000000      NaN  0.791880  0.000000     NaN  0.00000   \n",
       "1    0.45386  0.0000       NaN      NaN  0.000000  1.177500     0.0  0.00000   \n",
       "2    0.00000  0.0000  0.000000  0.00000  0.000000  0.000000     0.0  0.00000   \n",
       "3    0.22014     NaN  0.000000      NaN  0.881920  1.093600     NaN  0.00000   \n",
       "4        NaN  0.0000       NaN      NaN  0.000000  0.000000     0.0  1.79380   \n",
       "..       ...     ...       ...      ...       ...       ...     ...      ...   \n",
       "995  0.44172  0.0000  0.000000  1.32580  0.185200  1.544700     0.0  0.00000   \n",
       "996  0.00000     NaN       NaN      NaN  0.017221       NaN     0.0      NaN   \n",
       "997  0.00000  1.4189       NaN      NaN  0.000000  0.433280     0.0  0.00000   \n",
       "998  0.34373  0.0000  0.144660  0.30262       NaN  0.018162     0.0  0.91524   \n",
       "999  0.00000  0.0000  0.060526  0.00000  0.165390  0.000000     0.0  0.00000   \n",
       "\n",
       "      CNNs.8    CNNs.9  ...  GIST.374  GIST.375  GIST.376  GIST.377  GIST.378  \\\n",
       "0    0.43900  0.000000  ...  0.009773       NaN  0.011548       NaN  0.017014   \n",
       "1    0.42297  2.025100  ...  0.009820  0.026096  0.039678       NaN  0.057236   \n",
       "2    0.00000  0.319860  ...       NaN       NaN  0.026954  0.050490       NaN   \n",
       "3    0.00000  0.000000  ...  0.007899  0.023398       NaN  0.022786       NaN   \n",
       "4    0.00000  0.000000  ...  0.012921       NaN  0.019792  0.019010  0.003771   \n",
       "..       ...       ...  ...       ...       ...       ...       ...       ...   \n",
       "995      NaN  0.000000  ...  0.027531  0.028246  0.004278       NaN  0.012703   \n",
       "996  0.72694  0.399110  ...  0.005851  0.018785  0.028535  0.037508  0.021416   \n",
       "997  0.46587  0.872840  ...       NaN       NaN  0.036006  0.045336  0.051569   \n",
       "998  0.76578  0.021152  ...       NaN  0.030069  0.010835  0.015817       NaN   \n",
       "999      NaN  0.000000  ...  0.018771  0.038194       NaN  0.051699  0.017156   \n",
       "\n",
       "     GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  \n",
       "0         NaN  0.020395       NaN  0.007909  0.024576  \n",
       "1    0.023440       NaN  0.014737  0.013860  0.058389  \n",
       "2         NaN       NaN  0.021365  0.027606  0.031131  \n",
       "3    0.007288  0.043885       NaN  0.011621  0.022733  \n",
       "4    0.003214  0.001543       NaN  0.003199       NaN  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "995       NaN  0.017077       NaN  0.021376  0.016964  \n",
       "996  0.000547  0.020206  0.005268  0.008311       NaN  \n",
       "997  0.087066  0.027203  0.020635       NaN       NaN  \n",
       "998  0.008095       NaN  0.017243  0.002980  0.035659  \n",
       "999  0.024348  0.030551       NaN  0.017531  0.019828  \n",
       "\n",
       "[1000 rows x 3456 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_csv('test.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b56f61d-d167-47af-a1b3-17d91399a7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNs        215\n",
       "CNNs.1      186\n",
       "CNNs.2      185\n",
       "CNNs.3      225\n",
       "CNNs.4      214\n",
       "           ... \n",
       "GIST.379    201\n",
       "GIST.380    212\n",
       "GIST.381    217\n",
       "GIST.382    201\n",
       "GIST.383    199\n",
       "Length: 3456, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4b9d4b-6edb-4f84-9a24-c3c0ee4496c4",
   "metadata": {},
   "source": [
    "## Applying same things as on done our training set to remove the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e1dd5b5-a359-4f1e-9b38-63c5c6771ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_cnn=df_test.iloc[:,:3072]\n",
    "df_test_gist=df_test.iloc[:,3072:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "786eb17d-2165-4305-91fb-fecff633cfa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CNNs', 'CNNs.1', 'CNNs.2', 'CNNs.3', 'CNNs.4', 'CNNs.5', 'CNNs.6',\n",
       "       'CNNs.7', 'CNNs.8', 'CNNs.9',\n",
       "       ...\n",
       "       'CNNs.3062', 'CNNs.3063', 'CNNs.3064', 'CNNs.3065', 'CNNs.3066',\n",
       "       'CNNs.3067', 'CNNs.3068', 'CNNs.3069', 'CNNs.3070', 'CNNs.3071'],\n",
       "      dtype='object', length=3072)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_cnn.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e8bb640-b428-46c2-892f-8c3c6fc5392a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GIST', 'GIST.1', 'GIST.2', 'GIST.3', 'GIST.4', 'GIST.5', 'GIST.6',\n",
       "       'GIST.7', 'GIST.8', 'GIST.9',\n",
       "       ...\n",
       "       'GIST.374', 'GIST.375', 'GIST.376', 'GIST.377', 'GIST.378', 'GIST.379',\n",
       "       'GIST.380', 'GIST.381', 'GIST.382', 'GIST.383'],\n",
       "      dtype='object', length=384)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_gist.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb9822-f6d2-4342-a3f3-5d9e5a7ad933",
   "metadata": {},
   "source": [
    "## Impute NaN values with most frequent values. For both the CNN and Gist features their respective  most frequent values will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "46b5a1c8-7999-482c-808b-081f8a34fc23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>CNNs.3062</th>\n",
       "      <th>CNNs.3063</th>\n",
       "      <th>CNNs.3064</th>\n",
       "      <th>CNNs.3065</th>\n",
       "      <th>CNNs.3066</th>\n",
       "      <th>CNNs.3067</th>\n",
       "      <th>CNNs.3068</th>\n",
       "      <th>CNNs.3069</th>\n",
       "      <th>CNNs.3070</th>\n",
       "      <th>CNNs.3071</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.791880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.43900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.66924</td>\n",
       "      <td>0.45612</td>\n",
       "      <td>0.22030</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.177500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42297</td>\n",
       "      <td>2.025100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.17880</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.899000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.319860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.22220</td>\n",
       "      <td>4.77850</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.881920</td>\n",
       "      <td>1.093600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.95832</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.41876</td>\n",
       "      <td>0.444400</td>\n",
       "      <td>0.033549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79380</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91257</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.70167</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.080808</td>\n",
       "      <td>0.337520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.44172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.32580</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>1.544700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.78843</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.62514</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.017221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72694</td>\n",
       "      <td>0.399110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.26456</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.33241</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.16293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.4189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.46587</td>\n",
       "      <td>0.872840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.06060</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.34373</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.144660</td>\n",
       "      <td>0.30262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91524</td>\n",
       "      <td>0.76578</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.31877</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.20055</td>\n",
       "      <td>0.69974</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.14584</td>\n",
       "      <td>0.45222</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.060526</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30925</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.39880</td>\n",
       "      <td>0.19527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3072 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CNNs  CNNs.1    CNNs.2   CNNs.3    CNNs.4    CNNs.5  CNNs.6   CNNs.7  \\\n",
       "0    0.00000  0.2334  0.000000  0.00000  0.791880  0.000000     0.0  0.00000   \n",
       "1    0.45386  0.0000  0.000000  0.00000  0.000000  1.177500     0.0  0.00000   \n",
       "2    0.00000  0.0000  0.000000  0.00000  0.000000  0.000000     0.0  0.00000   \n",
       "3    0.22014  0.0000  0.000000  0.00000  0.881920  1.093600     0.0  0.00000   \n",
       "4    0.00000  0.0000  0.000000  0.00000  0.000000  0.000000     0.0  1.79380   \n",
       "..       ...     ...       ...      ...       ...       ...     ...      ...   \n",
       "995  0.44172  0.0000  0.000000  1.32580  0.185200  1.544700     0.0  0.00000   \n",
       "996  0.00000  0.0000  0.000000  0.00000  0.017221  0.000000     0.0  0.00000   \n",
       "997  0.00000  1.4189  0.000000  0.00000  0.000000  0.433280     0.0  0.00000   \n",
       "998  0.34373  0.0000  0.144660  0.30262  0.000000  0.018162     0.0  0.91524   \n",
       "999  0.00000  0.0000  0.060526  0.00000  0.165390  0.000000     0.0  0.00000   \n",
       "\n",
       "      CNNs.8    CNNs.9  ...  CNNs.3062  CNNs.3063  CNNs.3064  CNNs.3065  \\\n",
       "0    0.43900  0.000000  ...    0.00000    0.00000    0.00000    0.00000   \n",
       "1    0.42297  2.025100  ...    0.00000    1.17880    0.00000    0.00000   \n",
       "2    0.00000  0.319860  ...    0.00000    0.00000    0.00000    1.22220   \n",
       "3    0.00000  0.000000  ...    0.00000    0.95832    0.00000    0.00000   \n",
       "4    0.00000  0.000000  ...    0.91257    0.00000    0.70167    0.00000   \n",
       "..       ...       ...  ...        ...        ...        ...        ...   \n",
       "995  0.00000  0.000000  ...    0.00000    0.00000    0.00000    0.00000   \n",
       "996  0.72694  0.399110  ...    0.00000    0.00000    0.00000    0.26456   \n",
       "997  0.46587  0.872840  ...    0.00000    0.00000    1.06060    0.00000   \n",
       "998  0.76578  0.021152  ...    0.31877    0.00000    0.20055    0.69974   \n",
       "999  0.00000  0.000000  ...    0.30925    0.00000    0.00000    0.00000   \n",
       "\n",
       "     CNNs.3066  CNNs.3067  CNNs.3068  CNNs.3069  CNNs.3070  CNNs.3071  \n",
       "0      0.66924    0.45612    0.22030    0.00000   0.000000   0.000000  \n",
       "1      0.00000    0.00000    0.00000    0.00000   0.000000   1.899000  \n",
       "2      4.77850    0.00000    0.00632    0.00000   0.000000   0.000000  \n",
       "3      0.00000    0.00000    0.00000    0.41876   0.444400   0.033549  \n",
       "4      0.00000    0.00000    0.00000    0.00000   0.080808   0.337520  \n",
       "..         ...        ...        ...        ...        ...        ...  \n",
       "995    0.78843    0.00000    0.62514    0.00000   0.000000   0.000000  \n",
       "996    0.00000    0.33241    0.00000    0.16293   0.000000   0.000000  \n",
       "997    0.00000    0.00000    0.00000    0.00000   0.000000   0.000000  \n",
       "998    0.00000    0.14584    0.45222    0.00000   0.000000   0.000000  \n",
       "999    0.00000    0.00000    1.39880    0.19527   0.000000   0.297210  \n",
       "\n",
       "[1000 rows x 3072 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For CNN Features\n",
    "final_df_test_cnn=[]\n",
    "for row in range(len(df_test_cnn)):\n",
    "    nparray=np.array(df_test_cnn.iloc[row,:])\n",
    "    modeofrow=mode(df_test_cnn.iloc[row,:])\n",
    "    nparray=np.where(np.isnan(nparray),modeofrow,nparray)\n",
    "    final_df_test_cnn.append(nparray)\n",
    "    \n",
    "    \n",
    "    nan_mask = np.isnan(nparray)\n",
    "\n",
    "    # Check if any NaN value is present in the entire array\n",
    "    is_nan_present = np.any(nan_mask)\n",
    "    \n",
    "final_df_test_cnn=pd.DataFrame(final_df_test_cnn,columns=df_test_cnn.columns)\n",
    "final_df_test_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "44c57f1c-85da-452f-9624-bedc80a6439e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNs         0\n",
       "CNNs.1       0\n",
       "CNNs.2       0\n",
       "CNNs.3       0\n",
       "CNNs.4       0\n",
       "            ..\n",
       "CNNs.3067    0\n",
       "CNNs.3068    0\n",
       "CNNs.3069    0\n",
       "CNNs.3070    0\n",
       "CNNs.3071    0\n",
       "Length: 3072, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_test_cnn.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67cd918d-05bf-4fa9-bfee-255e5a0d9cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40/372518851.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_test_gist[col].fillna(modeofrow,inplace=True)\n",
      "/tmp/ipykernel_40/372518851.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_gist[col].fillna(modeofrow,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## For GIST Features\n",
    "for col in df_test_gist.columns:\n",
    "    modeofrow=mode(df_test_gist[col])\n",
    "    df_test_gist[col].fillna(modeofrow,inplace=True)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0aecd36-7971-4942-a9f4-3c1e9767b533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNs         0\n",
       "CNNs.1       0\n",
       "CNNs.2       0\n",
       "CNNs.3       0\n",
       "CNNs.4       0\n",
       "            ..\n",
       "CNNs.3067    0\n",
       "CNNs.3068    0\n",
       "CNNs.3069    0\n",
       "CNNs.3070    0\n",
       "CNNs.3071    0\n",
       "Length: 3072, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_test_cnn.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "956cc88d-258c-4d22-877c-4f3f0d0eb4ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GIST        0\n",
       "GIST.1      0\n",
       "GIST.2      0\n",
       "GIST.3      0\n",
       "GIST.4      0\n",
       "           ..\n",
       "GIST.379    0\n",
       "GIST.380    0\n",
       "GIST.381    0\n",
       "GIST.382    0\n",
       "GIST.383    0\n",
       "Length: 384, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_gist.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "50ce8ba3-1b0d-4737-96c6-7641faaf3169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNNs</th>\n",
       "      <th>CNNs.1</th>\n",
       "      <th>CNNs.2</th>\n",
       "      <th>CNNs.3</th>\n",
       "      <th>CNNs.4</th>\n",
       "      <th>CNNs.5</th>\n",
       "      <th>CNNs.6</th>\n",
       "      <th>CNNs.7</th>\n",
       "      <th>CNNs.8</th>\n",
       "      <th>CNNs.9</th>\n",
       "      <th>...</th>\n",
       "      <th>GIST.374</th>\n",
       "      <th>GIST.375</th>\n",
       "      <th>GIST.376</th>\n",
       "      <th>GIST.377</th>\n",
       "      <th>GIST.378</th>\n",
       "      <th>GIST.379</th>\n",
       "      <th>GIST.380</th>\n",
       "      <th>GIST.381</th>\n",
       "      <th>GIST.382</th>\n",
       "      <th>GIST.383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.791880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.43900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009773</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.011548</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.017546</td>\n",
       "      <td>0.020395</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.007909</td>\n",
       "      <td>0.024576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.45386</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.177500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.42297</td>\n",
       "      <td>2.025100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.026096</td>\n",
       "      <td>0.039678</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.057236</td>\n",
       "      <td>0.023440</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.014737</td>\n",
       "      <td>0.013860</td>\n",
       "      <td>0.058389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.319860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.026954</td>\n",
       "      <td>0.050490</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>0.017546</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.021365</td>\n",
       "      <td>0.027606</td>\n",
       "      <td>0.031131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.881920</td>\n",
       "      <td>1.093600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>0.020035</td>\n",
       "      <td>0.022786</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.043885</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.011621</td>\n",
       "      <td>0.022733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.79380</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.019792</td>\n",
       "      <td>0.019010</td>\n",
       "      <td>0.003771</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.029553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.44172</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.32580</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>1.544700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027531</td>\n",
       "      <td>0.028246</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.021680</td>\n",
       "      <td>0.012703</td>\n",
       "      <td>0.017546</td>\n",
       "      <td>0.017077</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.021376</td>\n",
       "      <td>0.016964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.017221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.72694</td>\n",
       "      <td>0.399110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>0.037508</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.020206</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.008311</td>\n",
       "      <td>0.029553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.4189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.433280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.46587</td>\n",
       "      <td>0.872840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>0.036006</td>\n",
       "      <td>0.045336</td>\n",
       "      <td>0.051569</td>\n",
       "      <td>0.087066</td>\n",
       "      <td>0.027203</td>\n",
       "      <td>0.020635</td>\n",
       "      <td>0.012905</td>\n",
       "      <td>0.029553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.34373</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.144660</td>\n",
       "      <td>0.30262</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.91524</td>\n",
       "      <td>0.76578</td>\n",
       "      <td>0.021152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019197</td>\n",
       "      <td>0.030069</td>\n",
       "      <td>0.010835</td>\n",
       "      <td>0.015817</td>\n",
       "      <td>0.030574</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.002980</td>\n",
       "      <td>0.035659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.060526</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.165390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.038194</td>\n",
       "      <td>0.020035</td>\n",
       "      <td>0.051699</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.024348</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>0.019828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CNNs  CNNs.1    CNNs.2   CNNs.3    CNNs.4    CNNs.5  CNNs.6   CNNs.7  \\\n",
       "0    0.00000  0.2334  0.000000  0.00000  0.791880  0.000000     0.0  0.00000   \n",
       "1    0.45386  0.0000  0.000000  0.00000  0.000000  1.177500     0.0  0.00000   \n",
       "2    0.00000  0.0000  0.000000  0.00000  0.000000  0.000000     0.0  0.00000   \n",
       "3    0.22014  0.0000  0.000000  0.00000  0.881920  1.093600     0.0  0.00000   \n",
       "4    0.00000  0.0000  0.000000  0.00000  0.000000  0.000000     0.0  1.79380   \n",
       "..       ...     ...       ...      ...       ...       ...     ...      ...   \n",
       "995  0.44172  0.0000  0.000000  1.32580  0.185200  1.544700     0.0  0.00000   \n",
       "996  0.00000  0.0000  0.000000  0.00000  0.017221  0.000000     0.0  0.00000   \n",
       "997  0.00000  1.4189  0.000000  0.00000  0.000000  0.433280     0.0  0.00000   \n",
       "998  0.34373  0.0000  0.144660  0.30262  0.000000  0.018162     0.0  0.91524   \n",
       "999  0.00000  0.0000  0.060526  0.00000  0.165390  0.000000     0.0  0.00000   \n",
       "\n",
       "      CNNs.8    CNNs.9  ...  GIST.374  GIST.375  GIST.376  GIST.377  GIST.378  \\\n",
       "0    0.43900  0.000000  ...  0.009773  0.033122  0.011548  0.021680  0.017014   \n",
       "1    0.42297  2.025100  ...  0.009820  0.026096  0.039678  0.021680  0.057236   \n",
       "2    0.00000  0.319860  ...  0.019197  0.033122  0.026954  0.050490  0.030574   \n",
       "3    0.00000  0.000000  ...  0.007899  0.023398  0.020035  0.022786  0.030574   \n",
       "4    0.00000  0.000000  ...  0.012921  0.033122  0.019792  0.019010  0.003771   \n",
       "..       ...       ...  ...       ...       ...       ...       ...       ...   \n",
       "995  0.00000  0.000000  ...  0.027531  0.028246  0.004278  0.021680  0.012703   \n",
       "996  0.72694  0.399110  ...  0.005851  0.018785  0.028535  0.037508  0.021416   \n",
       "997  0.46587  0.872840  ...  0.019197  0.033122  0.036006  0.045336  0.051569   \n",
       "998  0.76578  0.021152  ...  0.019197  0.030069  0.010835  0.015817  0.030574   \n",
       "999  0.00000  0.000000  ...  0.018771  0.038194  0.020035  0.051699  0.017156   \n",
       "\n",
       "     GIST.379  GIST.380  GIST.381  GIST.382  GIST.383  \n",
       "0    0.017546  0.020395  0.014334  0.007909  0.024576  \n",
       "1    0.023440  0.016528  0.014737  0.013860  0.058389  \n",
       "2    0.017546  0.016528  0.021365  0.027606  0.031131  \n",
       "3    0.007288  0.043885  0.014334  0.011621  0.022733  \n",
       "4    0.003214  0.001543  0.014334  0.003199  0.029553  \n",
       "..        ...       ...       ...       ...       ...  \n",
       "995  0.017546  0.017077  0.014334  0.021376  0.016964  \n",
       "996  0.000547  0.020206  0.005268  0.008311  0.029553  \n",
       "997  0.087066  0.027203  0.020635  0.012905  0.029553  \n",
       "998  0.008095  0.016528  0.017243  0.002980  0.035659  \n",
       "999  0.024348  0.030551  0.014334  0.017531  0.019828  \n",
       "\n",
       "[1000 rows x 3456 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_new=pd.concat([final_df_test_cnn,df_test_gist],axis=1)\n",
    "df_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "320780a5-3655-4466-9bec-a9253c4e12d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CNNs', 'CNNs.1', 'CNNs.2', 'CNNs.3', 'CNNs.4', 'CNNs.5', 'CNNs.6',\n",
       "       'CNNs.7', 'CNNs.8', 'CNNs.9',\n",
       "       ...\n",
       "       'GIST.374', 'GIST.375', 'GIST.376', 'GIST.377', 'GIST.378', 'GIST.379',\n",
       "       'GIST.380', 'GIST.381', 'GIST.382', 'GIST.383'],\n",
       "      dtype='object', length=3456)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "82d59385-d814-42d5-baee-20f48479954f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3456)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df_test_new).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cd7fe011-9cdc-49e4-b934-37505793d284",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]], dtype=int32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_on_test=y_pred = (best_model.predict(df_test_new) > 0.5).astype(\"int32\")\n",
    "predicted_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4cf8bea7-bfc1-42b0-a594-5e536dd61b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_on_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "92571c9c-3a5c-4470-a2c1-58db069f53f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_test=[]\n",
    "for val in predicted_on_test:\n",
    "    predicted_test.append(val[0])\n",
    "predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "27c4c84a-4fde-46de-a27e-8edc3aee30fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prediction\n",
       "0             0\n",
       "1             0\n",
       "2             1\n",
       "3             0\n",
       "4             1\n",
       "..          ...\n",
       "995           0\n",
       "996           0\n",
       "997           1\n",
       "998           1\n",
       "999           0\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df=pd.DataFrame({'prediction':predicted_test})\n",
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3cbcd1f-29e2-4b37-a9f4-72da92b087f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_df.to_csv(\"predicted_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd0294-2b09-45ea-b3ac-60d2206960d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
